distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.21:44920'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.34:45347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.33:36462'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:43479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.211:40114'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:46218'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.35:42340'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.36:35463'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:39450'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.21:37850'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.34:38621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.33:39833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.211:42136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:34986'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.35:45071'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.36:36057'
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:41536
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.21:44359
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:45421
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.35:36913
distributed.worker - INFO -          Listening to:    tcp://172.21.9.21:44359
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:41536
distributed.worker - INFO -          Listening to:    tcp://172.21.9.35:36913
distributed.worker - INFO -          dashboard at:          172.21.9.21:42431
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:45421
distributed.worker - INFO -          dashboard at:          172.21.9.35:38902
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.21:35227
distributed.worker - INFO -          dashboard at:          172.21.9.20:32973
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.9.20:41978
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.35:44314
distributed.worker - INFO -          Listening to:    tcp://172.21.9.21:35227
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.35:44314
distributed.worker - INFO -          dashboard at:          172.21.9.21:41755
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.9.35:36218
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lw7_bv9v
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g3m2f97q
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kp6llc75
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y5fl6df_
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-me2_g3dx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-58tzried
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:37939
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:37939
distributed.worker - INFO -          dashboard at:         172.21.8.242:33046
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:38784
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1ng0rj4g
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:38784
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.8.242:46590
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dh9bjvw9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.34:36689
distributed.worker - INFO -          Listening to:    tcp://172.21.9.34:36689
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.33:44473
distributed.worker - INFO -          dashboard at:          172.21.9.34:33976
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.33:44473
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.33:44698
distributed.worker - INFO -          Listening to:    tcp://172.21.9.33:44698
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.9.33:44648
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.9.33:45669
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f02dgemk
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.34:34176
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.9.34:34176
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.36:38145
distributed.worker - INFO -          Listening to:    tcp://172.21.9.36:38145
distributed.worker - INFO -          dashboard at:          172.21.9.36:38534
distributed.worker - INFO -          dashboard at:          172.21.9.34:46249
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.211:42878
distributed.worker - INFO -          Listening to:   tcp://172.21.8.211:42878
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.211:33712
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zj325r5n
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tup8s5xs
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aysz4f61
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ojp41re
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.36:41698
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.211:36078
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4lcu3bc_
distributed.worker - INFO -          Listening to:    tcp://172.21.9.36:41698
distributed.worker - INFO -          Listening to:   tcp://172.21.8.211:36078
distributed.worker - INFO -          dashboard at:          172.21.9.36:37749
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -          dashboard at:         172.21.8.211:40802
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.177:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-su_93lb2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zkb0xmxx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.177:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene2155: error: *** STEP 8467985.1 ON irene2155 CANCELLED AT 2023-02-22T09:02:14 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.36:35463'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.34:38621'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.33:36462'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:43479'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.35:45071'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.211:40114'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.21:44920'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:39450'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.36:36057'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.34:45347'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.33:39833'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:34986'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.35:42340'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.211:42136'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.21:37850'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:46218'
distributed.worker - INFO - Stopping worker at tcp://172.21.9.34:34176
distributed.worker - INFO - Stopping worker at tcp://172.21.9.35:44314
distributed.worker - INFO - Stopping worker at tcp://172.21.8.211:36078
distributed.worker - INFO - Stopping worker at tcp://172.21.9.34:36689
distributed.worker - INFO - Stopping worker at tcp://172.21.9.35:36913
distributed.worker - INFO - Stopping worker at tcp://172.21.8.211:42878
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:37939
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:38784
distributed.worker - INFO - Stopping worker at tcp://172.21.9.21:44359
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:45421
distributed.worker - INFO - Stopping worker at tcp://172.21.9.21:35227
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:41536
distributed.worker - INFO - Stopping worker at tcp://172.21.9.33:44698
distributed.worker - INFO - Stopping worker at tcp://172.21.9.33:44473
distributed.worker - INFO - Stopping worker at tcp://172.21.9.36:38145
distributed.worker - INFO - Stopping worker at tcp://172.21.9.36:41698
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77233 parent=77128 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77232 parent=77129 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97928 parent=97824 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13335 parent=13232 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68531 parent=68420 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68530 parent=68421 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44772 parent=44667 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44771 parent=44668 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35171 parent=35065 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13336 parent=13233 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84173 parent=84068 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84172 parent=84069 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97929 parent=97825 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25759 parent=25653 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35170 parent=35066 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25758 parent=25654 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x24094c0)

Current thread 0x00002ad70bd9bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23ab4c0)

Current thread 0x00002b8f66c43b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc9d4c0)

Current thread 0x00002b66b03a2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x95b4c0)

Current thread 0x00002b216e281b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x95f4c0)

Current thread 0x00002b8882b66b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18424c0)

Current thread 0x00002b1cd3a3bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xde54c0)

Current thread 0x00002b20202e9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19f84c0)

Current thread 0x00002b6ff4cb5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ad44c0)

Current thread 0x00002ad6df6a4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18eb4c0)

Current thread 0x00002ac565ce0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14ac4c0)

Current thread 0x00002abae98a3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11864c0)

Current thread 0x00002ba8b5cb6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19584c0)

Current thread 0x00002b6ec84e1b80 (most recent call first):
<no Python frame>
srun: error: irene2218: tasks 6-7: Aborted (core dumped)
srun: error: irene2232: tasks 12-13: Aborted (core dumped)
srun: error: irene2231: tasks 10-11: Aborted (core dumped)
srun: error: irene2186: tasks 2-3: Aborted (core dumped)
srun: error: irene2233: tasks 14-15: Aborted (core dumped)
srun: error: irene2230: tasks 8-9: Aborted (core dumped)
srun: error: irene2155: task 1: Aborted (core dumped)
