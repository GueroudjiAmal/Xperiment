distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.61:35727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.196:42817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.61:35953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.196:37366'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.62:38276'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.60:44895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.62:36994'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.73:34036'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.73:34757'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.90:36108'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.85:44164'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.89:40565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.90:35338'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.65:41846'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.95:45775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.85:46498'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.76:42404'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.108:46256'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.109:43516'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.108:40969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.109:42926'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.89:42679'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.95:39643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.76:33870'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.103:46727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.103:42078'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.167:44761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.102:39622'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.115:45310'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.115:44547'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.167:37862'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.107:44720'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.107:33029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.169:45292'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.104:43452'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.104:37690'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.177:40672'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.177:39650'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.197:43918'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.163:43074'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.197:35297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.163:41408'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.168:33892'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.94:39672'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.168:46382'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.106:46779'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.195:46416'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.106:38601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.195:36313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.86:34769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.74:45719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.64:41962'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.87:46540'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.87:33597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.94:39810'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.74:44893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.98:33507'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.98:39441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.64:46584'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.77:43984'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.78:33010'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.78:43883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.9:38348'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.63:34516'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.63:34884'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.164:35284'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.176:38508'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.75:41435'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.194:46352'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.75:39348'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.99:42450'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.99:33998'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.93:44279'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.105:38275'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.116:41305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.68:39838'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.105:46405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.210:46161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.60:37855'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.68:43588'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.79:40957'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.102:41125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.179:34194'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.172:40674'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.179:42028'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.172:41546'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.164:35453'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.181:41754'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.181:43027'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.171:36450'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.171:32983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.175:46293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.162:44975'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.162:37011'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.88:37831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.92:40035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.80:40798'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.88:43117'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.80:43851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.93:35539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.180:42994'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.180:45692'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.210:40992'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.9:35022'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.166:35843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.176:41976'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.178:40902'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.77:43705'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.86:40689'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.178:38007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.65:33754'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.116:38186'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.166:36161'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.194:36977'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.92:45924'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.79:43585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.169:33640'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.174:36955'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.174:35900'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.170:43491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.170:36177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.165:40604'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.165:45909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.173:42834'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.91:36032'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.91:35948'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.175:42632'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.173:39918'
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.164:38389
distributed.worker - INFO -          Listening to:  tcp://172.21.10.164:38389
distributed.worker - INFO -          dashboard at:        172.21.10.164:33819
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-34eu6gzp
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.164:32776
distributed.worker - INFO -          Listening to:  tcp://172.21.10.164:32776
distributed.worker - INFO -          dashboard at:        172.21.10.164:43426
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.181:42015
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.165:44947
distributed.worker - INFO -          Listening to:  tcp://172.21.10.165:44947
distributed.worker - INFO -          dashboard at:        172.21.10.165:33357
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.172:36678
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.210:41224
distributed.worker - INFO -          Listening to:   tcp://172.21.0.210:41224
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.181:42015
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.176:45757
distributed.worker - INFO -          Listening to:  tcp://172.21.10.176:45757
distributed.worker - INFO -          dashboard at:        172.21.10.176:45446
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.165:36807
distributed.worker - INFO -          Listening to:  tcp://172.21.10.165:36807
distributed.worker - INFO -          dashboard at:        172.21.10.165:46598
distributed.worker - INFO -          dashboard at:        172.21.10.181:33535
distributed.worker - INFO -          Listening to:  tcp://172.21.10.172:36678
distributed.worker - INFO -          dashboard at:         172.21.0.210:34448
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.115:44248
distributed.worker - INFO -          Listening to:   tcp://172.21.1.115:44248
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.179:36780
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:        172.21.10.172:42649
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.168:45320
distributed.worker - INFO -          Listening to:  tcp://172.21.10.168:45320
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.195:36506
distributed.worker - INFO -          Listening to:   tcp://172.21.0.195:36506
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.116:32855
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.76:33160
distributed.worker - INFO -          Listening to:    tcp://172.21.1.76:33160
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.109:39436
distributed.worker - INFO -          Listening to:   tcp://172.21.1.109:39436
distributed.worker - INFO -          dashboard at:         172.21.1.109:39265
distributed.worker - INFO -       Start worker at:     tcp://172.21.1.9:35836
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.60:43260
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.108:38846
distributed.worker - INFO -          Listening to:   tcp://172.21.1.108:38846
distributed.worker - INFO -          dashboard at:         172.21.1.108:37532
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.104:41480
distributed.worker - INFO -          Listening to:   tcp://172.21.1.104:41480
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.167:32829
distributed.worker - INFO -          Listening to:  tcp://172.21.10.167:32829
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.93:44450
distributed.worker - INFO -          Listening to:    tcp://172.21.1.93:44450
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.89:44652
distributed.worker - INFO -          Listening to:    tcp://172.21.1.89:44652
distributed.worker - INFO -          dashboard at:          172.21.1.89:33558
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.99:33622
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.90:41317
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6v7thyc_
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.196:36246
distributed.worker - INFO -          Listening to:   tcp://172.21.0.196:36246
distributed.worker - INFO -          dashboard at:         172.21.0.196:38545
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.107:40176
distributed.worker - INFO -          Listening to:   tcp://172.21.1.107:40176
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.68:41092
distributed.worker - INFO -          Listening to:    tcp://172.21.1.68:41092
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.98:35642
distributed.worker - INFO -          Listening to:    tcp://172.21.1.98:35642
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.95:35221
distributed.worker - INFO -          Listening to:    tcp://172.21.1.95:35221
distributed.worker - INFO -          dashboard at:          172.21.1.95:42228
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.194:42762
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.171:44698
distributed.worker - INFO -          Listening to:  tcp://172.21.10.171:44698
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.74:32880
distributed.worker - INFO -          Listening to:    tcp://172.21.1.74:32880
distributed.worker - INFO -          dashboard at:          172.21.1.74:36425
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.162:34193
distributed.worker - INFO -          Listening to:  tcp://172.21.10.162:34193
distributed.worker - INFO -          dashboard at:        172.21.10.162:37600
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.166:36627
distributed.worker - INFO -          Listening to:  tcp://172.21.10.166:36627
distributed.worker - INFO -          dashboard at:        172.21.10.166:46348
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.197:39911
distributed.worker - INFO -          Listening to:   tcp://172.21.0.197:39911
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.78:33465
distributed.worker - INFO -          Listening to:    tcp://172.21.1.78:33465
distributed.worker - INFO -          dashboard at:          172.21.1.78:39822
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.180:36247
distributed.worker - INFO -          Listening to:  tcp://172.21.10.180:36247
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:40999
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:40999
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.105:39325
distributed.worker - INFO -          Listening to:   tcp://172.21.1.105:39325
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.169:34519
distributed.worker - INFO -          Listening to:  tcp://172.21.10.169:34519
distributed.worker - INFO -          dashboard at:        172.21.10.169:35608
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.77:39067
distributed.worker - INFO -          Listening to:    tcp://172.21.1.77:39067
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.64:46466
distributed.worker - INFO -          Listening to:    tcp://172.21.1.64:46466
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.103:33418
distributed.worker - INFO -          Listening to:   tcp://172.21.1.103:33418
distributed.worker - INFO -          dashboard at:         172.21.1.115:33479
distributed.worker - INFO -          Listening to:  tcp://172.21.10.179:36780
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.87:33247
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:41469
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:41469
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.94:41825
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.174:46146
distributed.worker - INFO -          Listening to:  tcp://172.21.10.174:46146
distributed.worker - INFO -          dashboard at:        172.21.10.168:45984
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.85:35047
distributed.worker - INFO -          Listening to:    tcp://172.21.1.85:35047
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.170:37307
distributed.worker - INFO -          Listening to:  tcp://172.21.10.170:37307
distributed.worker - INFO -          dashboard at:        172.21.10.170:43232
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.65:33919
distributed.worker - INFO -          Listening to:    tcp://172.21.1.65:33919
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.177:42135
distributed.worker - INFO -          Listening to:  tcp://172.21.10.177:42135
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.80:41600
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.175:33471
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.173:41080
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.79:40798
distributed.worker - INFO -          Listening to:    tcp://172.21.1.79:40798
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.195:39647
distributed.worker - INFO -          Listening to:   tcp://172.21.0.195:39647
distributed.worker - INFO -          dashboard at:         172.21.0.195:35475
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.76:44926
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.109:39019
distributed.worker - INFO -       Start worker at:     tcp://172.21.1.9:42066
distributed.worker - INFO -          Listening to:     tcp://172.21.1.9:42066
distributed.worker - INFO -          dashboard at:           172.21.1.9:42200
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.1.104:39740
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.75:44738
distributed.worker - INFO -          dashboard at:        172.21.10.167:42099
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.106:45552
distributed.worker - INFO -          dashboard at:          172.21.1.93:43212
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.99:33622
distributed.worker - INFO -          Listening to:    tcp://172.21.1.90:41317
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:         172.21.1.107:36550
distributed.worker - INFO -          dashboard at:          172.21.1.68:32921
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.98:37820
distributed.worker - INFO -          Listening to:    tcp://172.21.1.98:37820
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.62:46616
distributed.worker - INFO -          Listening to:    tcp://172.21.1.62:46616
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.194:42762
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.171:38615
distributed.worker - INFO -          Listening to:  tcp://172.21.10.171:38615
distributed.worker - INFO -          dashboard at:        172.21.10.171:36423
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:         172.21.0.197:43879
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.180:37965
distributed.worker - INFO -          Listening to:  tcp://172.21.10.180:37965
distributed.worker - INFO -          dashboard at:          172.21.1.92:45015
distributed.worker - INFO -          dashboard at:         172.21.1.105:34632
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.77:45505
distributed.worker - INFO -          dashboard at:          172.21.1.64:36178
distributed.worker - INFO -          dashboard at:         172.21.1.103:41232
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.179:37769
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.102:34690
distributed.worker - INFO -          Listening to:   tcp://172.21.1.102:34690
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.63:37188
distributed.worker - INFO -          Listening to:    tcp://172.21.1.63:37188
distributed.worker - INFO -          Listening to:    tcp://172.21.1.87:33247
distributed.worker - INFO -          dashboard at:          172.21.1.88:46035
distributed.worker - INFO -          Listening to:    tcp://172.21.1.94:41825
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lzza9y0j
distributed.worker - INFO -          dashboard at:        172.21.10.174:41853
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.168:39982
distributed.worker - INFO -          Listening to:  tcp://172.21.10.168:39982
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.85:33578
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.163:44822
distributed.worker - INFO -          dashboard at:          172.21.1.65:39490
distributed.worker - INFO -          dashboard at:        172.21.10.177:33714
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.80:45052
distributed.worker - INFO -          Listening to:    tcp://172.21.1.80:45052
distributed.worker - INFO -          Listening to:  tcp://172.21.10.175:33471
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.173:35118
distributed.worker - INFO -          Listening to:  tcp://172.21.10.173:35118
distributed.worker - INFO -          dashboard at:        172.21.10.173:44037
distributed.worker - INFO -          dashboard at:         172.21.0.195:33151
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.116:32855
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:     tcp://172.21.1.9:35836
distributed.worker - INFO -          dashboard at:           172.21.1.9:36302
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.60:40894
distributed.worker - INFO -          Listening to:    tcp://172.21.1.60:40894
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.106:37759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.99:40455
distributed.worker - INFO -          dashboard at:          172.21.1.90:36520
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.196:39354
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.91:40601
distributed.worker - INFO -          Listening to:    tcp://172.21.1.91:40601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.61:34766
distributed.worker - INFO -          Listening to:    tcp://172.21.1.61:34766
distributed.worker - INFO -          dashboard at:          172.21.1.61:34561
distributed.worker - INFO -          dashboard at:          172.21.1.98:43245
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.62:38940
distributed.worker - INFO -          Listening to:    tcp://172.21.1.62:38940
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.194:41091
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.78:42715
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.73:41447
distributed.worker - INFO -          dashboard at:        172.21.10.180:42404
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.179:36514
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.87:33747
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.94:37789
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.172:39855
distributed.worker - INFO -          Listening to:  tcp://172.21.10.172:39855
distributed.worker - INFO -          dashboard at:        172.21.10.172:43573
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.210:38541
distributed.worker - INFO -          Listening to:   tcp://172.21.0.210:38541
distributed.worker - INFO -          dashboard at:         172.21.0.210:32839
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.174:42696
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.85:41188
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.86:41226
distributed.worker - INFO -          Listening to:    tcp://172.21.1.86:41226
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.170:43797
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.80:41600
distributed.worker - INFO -          dashboard at:        172.21.10.175:32926
distributed.worker - INFO -          Listening to:  tcp://172.21.10.173:41080
distributed.worker - INFO -          dashboard at:          172.21.1.79:34374
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.116:36655
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.1.109:39019
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.60:43260
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r5ihrrgo
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.104:40972
distributed.worker - INFO -          Listening to:    tcp://172.21.1.75:44738
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rf7k_gph
distributed.worker - INFO -          Listening to:   tcp://172.21.1.106:45552
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.98:37325
distributed.worker - INFO -          dashboard at:          172.21.1.62:41380
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.95:46114
distributed.worker - INFO -          Listening to:    tcp://172.21.1.95:46114
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.162:41907
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.166:40554
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.73:41447
distributed.worker - INFO -          dashboard at:        172.21.10.180:39481
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.178:35786
distributed.worker - INFO -          Listening to:  tcp://172.21.10.178:35786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.115:42460
distributed.worker - INFO -          Listening to:   tcp://172.21.1.115:42460
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:         172.21.1.102:32832
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-abx50d05
distributed.worker - INFO -          dashboard at:          172.21.1.63:42996
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.94:44821
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y55lgyqb
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.168:43469
distributed.worker - INFO -          Listening to:    tcp://172.21.1.85:33578
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.10.163:44822
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.80:45888
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.60:46325
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.1.104:40972
distributed.worker - INFO -          dashboard at:          172.21.1.75:43443
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.106:43566
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.99:39696
distributed.worker - INFO -          Listening to:    tcp://172.21.1.99:39696
distributed.worker - INFO -          dashboard at:          172.21.1.99:39085
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.196:39354
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.91:45520
distributed.worker - INFO -          Listening to:    tcp://172.21.1.91:45520
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.61:46650
distributed.worker - INFO -          Listening to:    tcp://172.21.1.61:46650
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.1.78:42715
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.73:46303
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:43572
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:43572
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mul80b49
distributed.worker - INFO -          Listening to:  tcp://172.21.10.179:37769
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:33606
distributed.worker - INFO -          Listening to:    tcp://172.21.1.94:37789
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.86:34885
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          Listening to:  tcp://172.21.10.170:43797
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.163:36872
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.177:37477
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.175:37274
distributed.worker - INFO -          dashboard at:        172.21.10.173:40005
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f70fw0t_
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.60:45219
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.108:37453
distributed.worker - INFO -          Listening to:   tcp://172.21.1.108:37453
distributed.worker - INFO -          dashboard at:         172.21.1.108:37728
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n3hc4nnd
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.176:42094
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-my03i8g0
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.196:44052
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rmuptyl1
distributed.worker - INFO -          dashboard at:          172.21.1.91:35108
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-klo48jch
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.62:33651
distributed.worker - INFO -          dashboard at:          172.21.1.95:33865
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:        172.21.10.171:45885
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ghr5e4be
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lwrus_6q
distributed.worker - INFO -          Listening to:  tcp://172.21.10.166:40554
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1m9jfnb2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mza5uan4
distributed.worker - INFO -          Listening to:    tcp://172.21.1.73:46303
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zidvvbeq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9efudgux
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eolnkp1p
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w0k_hmt9
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.178:43475
distributed.worker - INFO -          Listening to:  tcp://172.21.10.178:43475
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n4nr7voc
distributed.worker - INFO -          dashboard at:         172.21.1.115:37885
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.181:37037
distributed.worker - INFO -          Listening to:  tcp://172.21.10.181:37037
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.174:42696
distributed.worker - INFO -          dashboard at:        172.21.10.174:39713
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.85:44978
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:        172.21.10.170:35082
distributed.worker - INFO -          dashboard at:        172.21.10.163:36947
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jrcdsco4
distributed.worker - INFO -          dashboard at:          172.21.1.80:33289
distributed.worker - INFO -          Listening to:  tcp://172.21.10.175:37274
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hy_7yb8p
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.109:35677
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c5btr7w0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.176:42094
distributed.worker - INFO -          Listening to:   tcp://172.21.1.106:37759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.89:44334
distributed.worker - INFO -          Listening to:    tcp://172.21.1.89:44334
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p3xnd1kh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.68:36173
distributed.worker - INFO -          dashboard at:          172.21.1.61:38007
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u1kq0l1k
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jjl2t8e4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.162:41907
distributed.worker - INFO -          dashboard at:        172.21.10.162:44303
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.78:35401
distributed.worker - INFO -          dashboard at:          172.21.1.73:38931
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.92:38489
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z2lsbg5y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nggnw4dc
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.77:37450
distributed.worker - INFO -          Listening to:    tcp://172.21.1.77:37450
distributed.worker - INFO -          dashboard at:          172.21.1.77:40282
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.178:38566
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.179:45884
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.181:35053
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.63:33081
distributed.worker - INFO -          Listening to:    tcp://172.21.1.63:33081
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:33606
distributed.worker - INFO -          dashboard at:          172.21.1.94:40383
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zd_vxzdl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b9ksxvg6
distributed.worker - INFO -          Listening to:  tcp://172.21.10.177:37477
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.175:39291
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.76:44704
distributed.worker - INFO -          Listening to:    tcp://172.21.1.76:44704
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ddnai67c
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.104:40797
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.75:43380
distributed.worker - INFO -          Listening to:    tcp://172.21.1.75:43380
distributed.worker - INFO -          dashboard at:          172.21.1.75:41617
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.167:39872
distributed.worker - INFO -          dashboard at:        172.21.10.176:36760
distributed.worker - INFO -          dashboard at:         172.21.1.106:45664
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.93:34530
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-171_b4_h
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mj65y7kh
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.107:38755
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7p2h7zqd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.74:38088
distributed.worker - INFO -          Listening to:    tcp://172.21.1.74:38088
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.166:35153
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.197:35913
distributed.worker - INFO -          Listening to:   tcp://172.21.0.197:35913
distributed.worker - INFO -          dashboard at:         172.21.0.197:39771
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.73:44833
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w67pq6h4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.169:46215
distributed.worker - INFO -          Listening to:  tcp://172.21.10.169:46215
distributed.worker - INFO -          dashboard at:        172.21.10.169:33658
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pe1a7b7b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.64:37644
distributed.worker - INFO -          Listening to:    tcp://172.21.1.64:37644
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.103:42193
distributed.worker - INFO -          Listening to:   tcp://172.21.1.103:42193
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-owfkxxvh
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dmv7evy2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1oipj6dp
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5txyidyo
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-60szld7c
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:  tcp://172.21.10.163:36872
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5tpr6sk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.79:39460
distributed.worker - INFO -          Listening to:    tcp://172.21.1.79:39460
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2mgukxt0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kq2u6sxs
distributed.worker - INFO -          dashboard at:          172.21.1.76:38613
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gx5_dgqw
distributed.worker - INFO -          Listening to:  tcp://172.21.10.167:39872
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.1.93:34530
distributed.worker - INFO -          dashboard at:          172.21.1.89:40097
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.1.107:38755
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.1.68:36173
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.74:41642
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-paz1klq3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.105:39782
distributed.worker - INFO -          Listening to:   tcp://172.21.1.105:39782
distributed.worker - INFO -          dashboard at:         172.21.1.105:33511
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.64:41710
distributed.worker - INFO -          dashboard at:        172.21.10.178:36804
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:         172.21.1.103:43233
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.63:34981
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-03am6qf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pk0mist5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.163:41770
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.65:45467
distributed.worker - INFO -          dashboard at:        172.21.10.177:43639
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-boulkdv1
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.79:33114
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:        172.21.10.167:42840
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.93:43961
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.90:40229
distributed.worker - INFO -          Listening to:    tcp://172.21.1.90:40229
distributed.worker - INFO -          dashboard at:          172.21.1.90:37246
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fh5xlc9w
distributed.worker - INFO -          dashboard at:         172.21.1.107:44043
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v61rob0x
distributed.worker - INFO -          dashboard at:          172.21.1.68:45737
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n1_jnka2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cwhhspoy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l3tq0wc_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.194:42973
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.102:33887
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.87:45859
distributed.worker - INFO -          dashboard at:          172.21.1.88:39759
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-okkq94ol
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a0ywsu7x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.86:38144
distributed.worker - INFO -          Listening to:    tcp://172.21.1.86:38144
distributed.worker - INFO -          dashboard at:          172.21.1.86:44705
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ohug8gaa
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.65:45467
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n6brv670
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.116:34662
distributed.worker - INFO -          Listening to:   tcp://172.21.1.116:34662
distributed.worker - INFO -          dashboard at:         172.21.1.116:44302
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-49fty7vf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-34u6ra17
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o931dbtl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cmvk5344
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q28b0lto
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.91:37769
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cnid6nwb
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.0.194:42973
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-77gxsktn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jjlwdo41
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z6ldvipl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-onr5oyw1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.1.102:33887
distributed.worker - INFO -          dashboard at:         172.21.1.102:34531
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-abhuu8xo
distributed.worker - INFO -          Listening to:    tcp://172.21.1.87:45859
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gictcozp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.65:40952
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kwqtto7g
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0ap8dv9p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5adz86o4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r5t9d1u2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zvm_vj_6
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-81tu0yk6
distributed.worker - INFO -          dashboard at:         172.21.0.194:45286
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bj1j2b1e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-90fax4yx
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yd6lg6_a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uk7sr_rh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.87:37489
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qwqbwjrf
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-audlc20e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o28neh79
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i_hl176u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p5xvuv4o
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4mfpralc
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1gztk48d
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xdn4nr57
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ppn93rb7
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2vgv41r1
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8sno6sb6
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z41xg1yn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pdxm1xp3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k9i6nv_8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wj_248r7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sfz6s44t
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xuit7srv
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i8sgavzr
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-twt0uekd
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0sm2zyqa
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8n2dox0b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5x7fmilo
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5am4i3or
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qxy38wk5
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wrq7onm0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kwu5_x1x
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yuhrw3x8
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nbh2mkjk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hvawb2wk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hb86zks2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gk70dcsw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cerc14qq
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-egw69tke
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qctayx58
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xh99etaq
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1od__pc3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0gyu_pef
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mkrw8mf3
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-arr0nh98
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ou5ljkst
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3uk78619
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wnp4tp3g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oigrkyzx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dbds4wmi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ycn5lfjs
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ryy9d4sc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.82s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 0.96 GiB from 315 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:37040 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:37752 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:35970 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:57016 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:53924 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:41486 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:51172 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:51766 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:45964 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.163:48578 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 5.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 21866 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 21862 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 51645 was killed by signal 9
distributed.nanny - INFO - Worker process 98031 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 98030 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 12904 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 2454 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 16900 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 71889 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 71888 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 29355 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 2455 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 29354 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.core - INFO - Event loop was unresponsive in Worker for 3.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 88068 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 25998 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 88067 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 58904 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 58905 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 23088 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 66809 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.comm.tcp - INFO - Connection from tcp://172.21.1.104:46712 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.1.104:39554 closed before handshake completed
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.196:40641
distributed.worker - INFO -          Listening to:   tcp://172.21.0.196:40641
distributed.worker - INFO -          dashboard at:         172.21.0.196:40671
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:35875
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.177:38982
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:41420
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:41420
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:35875
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.88:33463
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a39qk0iv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.92:38607
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.94:41910
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kh77h_dy
distributed.worker - INFO -          Listening to:    tcp://172.21.1.94:41910
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.177:34196
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.195:33085
distributed.worker - INFO -          Listening to:   tcp://172.21.0.195:33085
distributed.worker - INFO -          dashboard at:         172.21.0.195:45325
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.169:45883
distributed.worker - INFO -          dashboard at:          172.21.1.94:33194
distributed.worker - INFO -          Listening to:  tcp://172.21.10.177:38982
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:        172.21.10.177:41416
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pkm8ws1x
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.89:38508
distributed.worker - INFO -          Listening to:    tcp://172.21.1.89:38508
distributed.worker - INFO -          dashboard at:          172.21.1.89:37686
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.196:33621
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.177:34196
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.196:33621
distributed.worker - INFO -          Listening to:  tcp://172.21.10.169:45883
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s95pg2so
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.196:39840
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.169:45492
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.177:39984
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:  tcp://172.21.10.169:45492
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sd7ltn0e
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-07xykgdz
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.195:44414
distributed.worker - INFO -          Listening to:   tcp://172.21.0.195:44414
distributed.worker - INFO -          dashboard at:         172.21.0.195:34923
distributed.worker - INFO -          dashboard at:        172.21.10.169:34374
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sdrqmvew
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:40891
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:40891
distributed.worker - INFO -          dashboard at:        172.21.10.169:42062
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.76:38120
distributed.worker - INFO -          Listening to:    tcp://172.21.1.76:38120
distributed.worker - INFO -          dashboard at:          172.21.1.76:35350
distributed.worker - INFO -          dashboard at:          172.21.1.92:37228
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pf2_8pem
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-byjuz21z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q9opnuuj
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0xbfxran
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-20oa9m3j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.63:33572
distributed.worker - INFO -          Listening to:    tcp://172.21.1.63:33572
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c2p1q_kk
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.63:40842
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j3greqdh
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:36120
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:36120
distributed.worker - INFO -          dashboard at:          172.21.1.88:33378
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.63:37762
distributed.worker - INFO -          Listening to:    tcp://172.21.1.63:37762
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.95:40506
distributed.worker - INFO -          Listening to:    tcp://172.21.1.95:40506
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o73o1t9g
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.63:34896
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO -          dashboard at:          172.21.1.95:34584
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bbod59d9
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.90:44637
distributed.worker - INFO -          Listening to:    tcp://172.21.1.90:44637
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aro537eg
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.167:33795
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0vuf1rtk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.90:44324
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jzw6tqbg
distributed.worker - INFO -          Listening to:  tcp://172.21.10.167:33795
distributed.worker - INFO -          dashboard at:        172.21.10.167:38716
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9y0j756z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.192:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1034: error: *** STEP 7728254.1 ON irene1034 CANCELLED AT 2022-12-29T19:29:25 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.61:35953'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.61:35727'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.115:45310'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.115:44547'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.102:39622'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.102:41125'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.80:43851'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.80:40798'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.74:45719'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.9:38348'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.74:44893'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.77:43705'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.166:35843'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.166:36161'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.9:35022'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.62:36994'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.78:33010'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.77:43984'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.78:43883'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.64:41962'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.173:42834'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.62:38276'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.197:35297'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.64:46584'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.197:43918'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.168:46382'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.194:36977'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.210:46161'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.168:33892'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.194:46352'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.171:36450'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.73:34036'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.210:40992'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.173:39918'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.106:46779'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.171:32983'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.73:34757'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.105:38275'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.181:41754'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.86:40689'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.106:38601'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.91:35948'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.98:33507'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.105:46405'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.181:43027'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.172:41546'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.174:36955'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.86:34769'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.79:40957'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.93:44279'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.98:39441'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.174:35900'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.79:43585'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.104:43452'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.93:35539'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.165:40604'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.104:37690'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.176:38508'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.107:44720'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.165:45909'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.107:33029'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.116:41305'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.60:44895'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.176:41976'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.68:39838'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.172:40674'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.68:43588'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.116:38186'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.60:37855'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.91:36032'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.116:32855
distributed.worker - INFO - Stopping worker at tcp://172.21.1.116:34662
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.177:40672'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.177:39650'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.179:42028'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.179:34194'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.75:39348'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.75:41435'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.175:46293'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.175:42632'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.196:37366'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.94:39810'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.164:35284'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.89:42679'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.89:40565'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.178:40902'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.164:35453'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.178:38007'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.169:45292'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.180:45692'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.167:37862'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.180:42994'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.167:44761'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.169:33640'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.94:39672'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.65:33754'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.196:42817'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.95:45775'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.65:41846'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.95:39643'
distributed.worker - INFO - Stopping worker at tcp://172.21.10.164:32776
distributed.worker - INFO - Stopping worker at tcp://172.21.10.164:38389
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.163:41408'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.163:43074'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.63:34516'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.63:34884'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.99:33998'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.99:42450'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.109:42926'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.109:43516'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.85:44164'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.85:46498'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.108:46256'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.108:40969'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.195:46416'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.195:36313'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.162:44975'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.76:42404'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.76:33870'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.162:37011'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.92:45924'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.92:40035'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.90:36108'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.90:35338'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.88:43117'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.88:37831'
distributed.worker - INFO - Stopping worker at tcp://172.21.10.176:42094
distributed.worker - INFO - Stopping worker at tcp://172.21.10.176:45757
distributed.worker - INFO - Stopping worker at tcp://172.21.10.179:37769
distributed.worker - INFO - Stopping worker at tcp://172.21.10.179:36780
distributed.worker - INFO - Stopping worker at tcp://172.21.10.172:36678
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.10.172:39855
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.103:46727'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.103:42078'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.115:44248
distributed.worker - INFO - Stopping worker at tcp://172.21.10.168:39982
distributed.worker - INFO - Stopping worker at tcp://172.21.10.168:45320
distributed.worker - INFO - Stopping worker at tcp://172.21.1.73:46303
distributed.worker - INFO - Stopping worker at tcp://172.21.1.115:42460
distributed.worker - INFO - Stopping worker at tcp://172.21.1.73:41447
distributed.worker - INFO - Stopping worker at tcp://172.21.1.64:37644
distributed.worker - INFO - Stopping worker at tcp://172.21.0.195:44414
distributed.worker - INFO - Stopping worker at tcp://172.21.0.195:33085
distributed.worker - INFO - Stopping worker at tcp://172.21.1.64:46466
distributed.worker - INFO - Stopping worker at tcp://172.21.0.197:39911
distributed.worker - INFO - Stopping worker at tcp://172.21.0.197:35913
distributed.worker - INFO - Stopping worker at tcp://172.21.1.107:38755
distributed.worker - INFO - Stopping worker at tcp://172.21.1.107:40176
distributed.worker - INFO - Stopping worker at tcp://172.21.10.167:33795
distributed.worker - INFO - Stopping worker at tcp://172.21.10.167:39872
distributed.worker - INFO - Stopping worker at tcp://172.21.1.62:38940
distributed.worker - INFO - Stopping worker at tcp://172.21.1.68:41092
distributed.worker - INFO - Stopping worker at tcp://172.21.1.68:36173
distributed.worker - INFO - Stopping worker at tcp://172.21.1.62:46616
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.170:43491'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.170:36177'
distributed.worker - INFO - Stopping worker at tcp://172.21.10.177:38982
distributed.worker - INFO - Stopping worker at tcp://172.21.0.194:42762
distributed.worker - INFO - Stopping worker at tcp://172.21.10.177:34196
distributed.worker - INFO - Stopping worker at tcp://172.21.10.173:35118
distributed.worker - INFO - Stopping worker at tcp://172.21.0.194:42973
distributed.worker - INFO - Stopping worker at tcp://172.21.10.165:44947
distributed.worker - INFO - Stopping worker at tcp://172.21.10.173:41080
distributed.worker - INFO - Stopping worker at tcp://172.21.1.95:35221
distributed.worker - INFO - Stopping worker at tcp://172.21.10.165:36807
distributed.worker - INFO - Stopping worker at tcp://172.21.1.95:40506
distributed.worker - INFO - Stopping worker at tcp://172.21.10.166:40554
distributed.worker - INFO - Stopping worker at tcp://172.21.10.166:36627
distributed.worker - INFO - Stopping worker at tcp://172.21.1.91:45520
distributed.worker - INFO - Stopping worker at tcp://172.21.1.91:40601
distributed.worker - INFO - Stopping worker at tcp://172.21.1.9:42066
distributed.worker - INFO - Stopping worker at tcp://172.21.1.102:33887
distributed.worker - INFO - Stopping worker at tcp://172.21.1.9:35836
distributed.worker - INFO - Stopping worker at tcp://172.21.1.102:34690
distributed.worker - INFO - Stopping worker at tcp://172.21.1.94:41825
distributed.worker - INFO - Stopping worker at tcp://172.21.1.79:40798
distributed.worker - INFO - Stopping worker at tcp://172.21.1.79:39460
distributed.worker - INFO - Stopping worker at tcp://172.21.10.163:44822
distributed.worker - INFO - Stopping worker at tcp://172.21.1.103:42193
distributed.worker - INFO - Stopping worker at tcp://172.21.1.103:33418
distributed.worker - INFO - Stopping worker at tcp://172.21.10.163:36872
distributed.worker - INFO - Stopping worker at tcp://172.21.1.94:41910
distributed.worker - INFO - Stopping worker at tcp://172.21.1.61:34766
distributed.worker - INFO - Stopping worker at tcp://172.21.1.61:46650
distributed.worker - INFO - Stopping worker at tcp://172.21.1.78:42715
distributed.worker - INFO - Stopping worker at tcp://172.21.1.78:33465
distributed.worker - INFO - Stopping worker at tcp://172.21.1.77:37450
distributed.worker - INFO - Stopping worker at tcp://172.21.10.175:33471
distributed.worker - INFO - Stopping worker at tcp://172.21.1.74:32880
distributed.worker - INFO - Stopping worker at tcp://172.21.1.74:38088
distributed.worker - INFO - Stopping worker at tcp://172.21.10.175:37274
distributed.worker - INFO - Stopping worker at tcp://172.21.1.77:39067
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.210:38541
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.210:41224
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.87:33597'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.98:37820
distributed.worker - INFO - Stopping worker at tcp://172.21.1.98:35642
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.87:46540'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.108:37453
distributed.worker - INFO - Stopping worker at tcp://172.21.1.80:41600
distributed.worker - INFO - Stopping worker at tcp://172.21.1.106:45552
distributed.worker - INFO - Stopping worker at tcp://172.21.1.93:44450
distributed.worker - INFO - Stopping worker at tcp://172.21.1.80:45052
distributed.worker - INFO - Stopping worker at tcp://172.21.1.108:38846
distributed.worker - INFO - Stopping worker at tcp://172.21.1.86:41226
distributed.worker - INFO - Stopping worker at tcp://172.21.1.86:38144
distributed.worker - INFO - Stopping worker at tcp://172.21.1.106:37759
distributed.worker - INFO - Stopping worker at tcp://172.21.10.181:42015
distributed.worker - INFO - Stopping worker at tcp://172.21.1.104:40972
distributed.worker - INFO - Stopping worker at tcp://172.21.1.93:34530
distributed.worker - INFO - Stopping worker at tcp://172.21.10.181:37037
distributed.worker - INFO - Stopping worker at tcp://172.21.1.104:41480
distributed.worker - INFO - Stopping worker at tcp://172.21.1.90:41317
distributed.worker - INFO - Stopping worker at tcp://172.21.10.171:44698
distributed.worker - INFO - Stopping worker at tcp://172.21.10.162:34193
distributed.worker - INFO - Stopping worker at tcp://172.21.1.90:44637
distributed.worker - INFO - Stopping worker at tcp://172.21.1.76:44704
distributed.worker - INFO - Stopping worker at tcp://172.21.10.162:41907
distributed.worker - INFO - Stopping worker at tcp://172.21.10.171:38615
distributed.worker - INFO - Stopping worker at tcp://172.21.1.105:39325
distributed.worker - INFO - Stopping worker at tcp://172.21.1.60:43260
distributed.worker - INFO - Stopping worker at tcp://172.21.1.105:39782
distributed.worker - INFO - Stopping worker at tcp://172.21.1.89:38508
distributed.worker - INFO - Stopping worker at tcp://172.21.1.60:40894
distributed.worker - INFO - Stopping worker at tcp://172.21.1.99:39696
distributed.worker - INFO - Stopping worker at tcp://172.21.10.169:45883
distributed.worker - INFO - Stopping worker at tcp://172.21.1.99:33622
distributed.worker - INFO - Stopping worker at tcp://172.21.10.169:45492
distributed.worker - INFO - Stopping worker at tcp://172.21.1.76:38120
distributed.worker - INFO - Stopping worker at tcp://172.21.1.89:44652
distributed.worker - INFO - Stopping worker at tcp://172.21.1.63:33572
distributed.worker - INFO - Stopping worker at tcp://172.21.1.63:37762
distributed.worker - INFO - Stopping worker at tcp://172.21.1.85:33578
distributed.worker - INFO - Stopping worker at tcp://172.21.1.85:35047
distributed.worker - INFO - Stopping worker at tcp://172.21.1.88:36120
distributed.worker - INFO - Stopping worker at tcp://172.21.1.65:33919
distributed.worker - INFO - Stopping worker at tcp://172.21.1.88:35875
distributed.worker - INFO - Stopping worker at tcp://172.21.1.65:45467
distributed.worker - INFO - Stopping worker at tcp://172.21.1.92:41420
distributed.worker - INFO - Stopping worker at tcp://172.21.1.92:40891
distributed.worker - INFO - Stopping worker at tcp://172.21.1.87:33247
distributed.worker - INFO - Stopping worker at tcp://172.21.1.87:45859
distributed.worker - INFO - Stopping worker at tcp://172.21.10.170:37307
distributed.worker - INFO - Stopping worker at tcp://172.21.10.170:43797
distributed.worker - INFO - Stopping worker at tcp://172.21.0.196:33621
distributed.worker - INFO - Stopping worker at tcp://172.21.1.109:39436
distributed.worker - INFO - Stopping worker at tcp://172.21.1.75:43380
distributed.worker - INFO - Stopping worker at tcp://172.21.10.174:42696
distributed.worker - INFO - Stopping worker at tcp://172.21.1.75:44738
distributed.worker - INFO - Stopping worker at tcp://172.21.0.196:40641
distributed.worker - INFO - Stopping worker at tcp://172.21.10.180:37965
distributed.worker - INFO - Stopping worker at tcp://172.21.10.178:43475
distributed.worker - INFO - Stopping worker at tcp://172.21.10.180:36247
distributed.worker - INFO - Stopping worker at tcp://172.21.10.178:35786
distributed.worker - INFO - Stopping worker at tcp://172.21.10.174:46146
distributed.worker - INFO - Stopping worker at tcp://172.21.1.109:39019
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40725 parent=40552 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40724 parent=40553 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29577 parent=29408 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35579 parent=35410 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36526 parent=36361 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41922 parent=41751 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36527 parent=36360 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41921 parent=41752 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83179 parent=83011 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13733 parent=13566 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62366 parent=62195 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43689 parent=43520 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43693 parent=43521 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14822 parent=14652 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83178 parent=83010 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13732 parent=13567 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28353 parent=28184 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58782 parent=58612 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97170 parent=97001 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97171 parent=97002 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4038 parent=3870 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67352 parent=67185 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70955 parent=70789 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54883 parent=54716 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2089 parent=1923 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24791 parent=22919 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39149 parent=38982 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46618 parent=46450 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70485 parent=70318 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68291 parent=68125 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54884 parent=54717 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96233 parent=96065 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27693 parent=25831 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67348 parent=67184 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4037 parent=3869 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96232 parent=96064 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14946 parent=14777 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29198 parent=29029 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23087 parent=22918 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47392 parent=47221 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39150 parent=38983 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46622 parent=46451 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29197 parent=29030 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68292 parent=68126 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70956 parent=70788 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14818 parent=14651 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10514 parent=10346 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17816 parent=17648 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47232 parent=47064 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29575 parent=29409 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39334 parent=39166 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28349 parent=28183 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=15417 parent=15248 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51271 parent=51099 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12903 parent=12733 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57759 parent=57589 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84743 parent=84573 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10515 parent=10347 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14945 parent=14778 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23570 parent=21694 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53533 parent=53362 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69462 parent=69293 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39249 parent=39082 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47391 parent=47222 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60594 parent=58739 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57758 parent=57588 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39333 parent=39165 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69461 parent=69294 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79076 parent=78909 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70486 parent=70319 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47231 parent=47065 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59378 parent=59211 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73586 parent=71722 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97796 parent=97630 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17815 parent=17647 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51646 parent=51473 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=15416 parent=15247 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59382 parent=59212 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60591 parent=58738 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53335 parent=51472 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4281 parent=2272 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74097 parent=73926 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14606 parent=12732 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55648 parent=55480 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4277 parent=2271 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77451 parent=77281 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74096 parent=73925 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79080 parent=78910 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75765 parent=75599 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97797 parent=97629 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23567 parent=21693 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73583 parent=71723 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25997 parent=25830 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58786 parent=58613 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2093 parent=1922 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35580 parent=35409 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51270 parent=51100 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75766 parent=75598 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84744 parent=84572 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53537 parent=53360 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62365 parent=62194 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39250 parent=39081 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=98149 parent=97983 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27488 parent=27321 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=98153 parent=97984 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22903 parent=22732 started daemon>
Traceback (most recent call last):
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2166 parent=97863 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22899 parent=22733 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55647 parent=55479 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77452 parent=77282 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31065 parent=29185 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=89767 parent=87899 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27487 parent=27320 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=89763 parent=87898 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2168 parent=97864 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68506 parent=66639 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31062 parent=29186 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32163 parent=31996 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32164 parent=31995 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88779 parent=88612 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16898 parent=16729 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66808 parent=66638 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75062 parent=74893 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75063 parent=74894 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88780 parent=88611 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18590 parent=16728 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71235 parent=71066 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71234 parent=71067 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
slurmstepd-irene1182: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1036: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene2397: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene2387: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1169: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1187: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1188: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
Exception in thread slurmstepd-irene1183: error: Detected 1 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene2389: error: Detected 3 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1185: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1182: task 50: Out Of Memory
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12c94c0)

Current thread 0x00002b1ec5c21b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbb94c0)

Current thread 0x00002ad236bebb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1efa4c0)

Current thread 0x00002b6cf48c3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc344c0)

Current thread 0x00002adf55dc4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12ea4c0)

Current thread 0x00002ab01b628b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b5f4c0)

Current thread 0x00002ba82fd1eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20b04c0)

Current thread 0x00002b2e246eeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1add4c0)

Current thread 0x00002b52aa9e9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x100d4c0)

Current thread 0x00002ba61621bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19e74c0)

Current thread 0x00002b084f48eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f3f4c0)

Current thread 0x00002ac003407b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x209b4c0)

Current thread 0x00002ab37023eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ca64c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17444c0)

Current thread 0x00002af3c90b5b80 (most recent call first):
<no Python frame>
Current thread 0x00002ada5851cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b654c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11484c0)

Current thread 0x00002ae48e03db80 (most recent call first):
<no Python frame>
Current thread 0x00002ac3ff717b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1bec4c0)

Current thread 0x00002b584c4a3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbd64c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13bf4c0)

Current thread 0x00002b6711dc4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5274c0)

Current thread 0x00002ae96c0f8b80 (most recent call first):
<no Python frame>
Current thread 0x00002b21c0c71b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10dd4c0)

Current thread 0x00002afd1a140b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23ce4c0)

Current thread 0x00002af34313fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16914c0)

Current thread 0x00002b110235fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8064c0)

Current thread 0x00002b9b4f161b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10724c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20c44c0)

Current thread 0x00002b0a30a43b80 (most recent call first):
<no Python frame>
Current thread 0x00002aef3ab0cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4db4c0)

Current thread 0x00002b8b78452b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14e14c0)

Current thread 0x00002ab2cb7deb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23b34c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb394c0)

Current thread 0x00002b500f21fb80 (most recent call first):
<no Python frame>
Current thread 0x00002ba7237d2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14b24c0)

Current thread 0x00002b08cec02b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fe34c0)

Current thread 0x00002b3213f8eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x198b4c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb104c0)

Current thread 0x00002ac709f74b80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x146f4c0)


Current thread 0x00002b9a62848b80 (most recent call first):
<no Python frame>
Current thread 0x00002ac6d0e99b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cfc4c0)

Current thread 0x00002abcd87eeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f024c0)

Current thread 0x00002aeb6cd2bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9924c0)

Current thread 0x00002ba43d472b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x241f4c0)

Current thread 0x00002ab21661bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ca54c0)

Current thread 0x00002b74fee45b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x240a4c0)

Current thread 0x00002b1ed5565b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7414c0)

Current thread 0x00002b7ce020db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12184c0)

Current thread 0x00002ab79e83bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x190b4c0)

Current thread 0x00002ab43f0b8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11d04c0)

Current thread 0x00002b1ab17edb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd184c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Current thread 0x00002b29f387db80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x14d94c0)

Current thread 0x00002b2c7798cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe8e4c0)

Current thread 0x00002ae837531b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15c34c0)

Current thread 0x00002b8d8aac5b80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x22e94c0)

Current thread 0x00002ab5ddc03b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18494c0)

Current thread 0x00002b5f9ea5db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16904c0)

Current thread 0x00002b1b40376b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xaa84c0)

Current thread 0x00002b0651a36b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7784c0)

Current thread 0x00002af71a341b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22434c0)

Current thread 0x00002b7db6505b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcb34c0)

Current thread 0x00002ad65a6a0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf644c0)

Current thread 0x00002af4fa602b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x119b4c0)

Current thread 0x00002b1db24c2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe164c0)

Current thread 0x00002b4f6536cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xee74c0)

Current thread 0x00002b3da1ca3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11854c0)

Current thread 0x00002b01ed2e5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1add4c0)

Current thread 0x00002b5e5fc56b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13a24c0)

Current thread 0x00002af8c22eeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8844c0)

Current thread 0x00002b9d23a65b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10304c0)

Current thread 0x00002b7abba28b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x232a4c0)

Current thread 0x00002b5bc68bab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20604c0)

Current thread 0x00002aef7928cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12f04c0)

Current thread 0x00002aea7c923b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfc74c0)

Current thread 0x00002b534e1a6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x133e4c0)

Current thread 0x00002b29b6063b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d304c0)

Current thread 0x00002acc864d6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb094c0)

Current thread 0x00002b8b8dc06b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa0f4c0)

Current thread 0x00002b5085a18b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x142f4c0)

Current thread 0x00002b362d21fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16af4c0)

Current thread 0x00002af1c477ab80 (most recent call first):
<no Python frame>
slurmstepd-irene1181: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1156: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1035: error: Detected 2 oom-kill event(s) in StepId=7728254.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1186: tasks 58-59: Aborted (core dumped)
srun: error: irene1171: tasks 36-37: Aborted (core dumped)
srun: error: irene2398: tasks 120-121: Aborted (core dumped)
srun: error: irene2396: tasks 116-117: Aborted (core dumped)
srun: error: irene1153: tasks 12-13: Aborted (core dumped)
srun: error: irene1198: tasks 74-75: Aborted (core dumped)
srun: error: irene1155: tasks 16-17: Aborted (core dumped)
srun: error: irene1197: tasks 72-73: Aborted (core dumped)
srun: error: irene1166: tasks 26-27: Aborted (core dumped)
srun: error: irene1199: tasks 76-77: Aborted (core dumped)
srun: error: irene1172: tasks 38-39: Aborted (core dumped)
srun: error: irene1201: task 80: Aborted (core dumped)
srun: error: irene2391: tasks 106-107: Aborted (core dumped)
srun: error: irene1170: tasks 34-35: Aborted (core dumped)
srun: error: irene2392: task 108: Aborted (core dumped)
srun: error: irene2383: task 90: Aborted (core dumped)
srun: error: irene2393: task 110: Aborted (core dumped)
srun: error: irene1167: task 28: Aborted (core dumped)
srun: error: irene2382: task 89: Aborted (core dumped)
srun: error: irene1195: task 68: Aborted (core dumped)
srun: error: irene1158: task 22: Aborted (core dumped)
srun: error: irene1157: task 20: Aborted (core dumped)
srun: error: irene1037: task 6: Aborted (core dumped)
srun: error: irene1102: task 11: Aborted (core dumped)
srun: error: irene1184: task 55: Aborted (core dumped)
srun: error: irene1161: task 24: Aborted (core dumped)
srun: error: irene1208: task 85: Aborted (core dumped)
srun: error: irene1034: task 1: Aborted (core dumped)
srun: error: irene1200: task 79: Aborted (core dumped)
srun: error: irene1192: task 66: Aborted (core dumped)
srun: error: irene1050: task 9: Aborted (core dumped)
srun: error: irene1179: task 44: Aborted (core dumped)
srun: error: irene1196: task 71: Aborted (core dumped)
srun: error: irene1180: task 47: Aborted (core dumped)
srun: error: irene1154: task 14: Aborted (core dumped)
srun: error: irene1191: task 65: Aborted (core dumped)
srun: error: irene2395: task 114: Aborted (core dumped)
srun: error: irene2385: task 95: Aborted (core dumped)
srun: error: irene1168: task 31: Aborted (core dumped)
srun: error: irene2388: task 101: Aborted (core dumped)
srun: error: irene2384: task 92: Aborted (core dumped)
srun: error: irene2399: task 123: Aborted (core dumped)
srun: error: irene1209: task 87: Aborted (core dumped)
srun: error: irene2394: task 113: Aborted (core dumped)
srun: error: irene2386: task 97: Aborted (core dumped)
srun: error: irene1173: task 40: Aborted (core dumped)
srun: error: irene1178: task 43: Aborted (core dumped)
srun: error: irene2400: task 125: Aborted (core dumped)
srun: error: irene2401: task 127: Aborted (core dumped)
