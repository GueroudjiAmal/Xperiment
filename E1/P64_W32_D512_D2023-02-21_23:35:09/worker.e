distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.25:43795'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.25:33787'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.24:39725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.24:33307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.20:36511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:46068'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:40260'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.27:46583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.27:40035'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:39203'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:38837'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:38769'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.31:40738'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.31:42615'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:42747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:37579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.21:37449'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.21:36611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:44489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.30:41780'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.20:38511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:36792'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:37507'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.30:38100'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:42162'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:36981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:41554'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:36078'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.28:41012'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.28:43328'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:44024'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:39607'
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.25:33250
distributed.worker - INFO -          Listening to:    tcp://172.21.3.25:33250
distributed.worker - INFO -          dashboard at:          172.21.3.25:46717
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.25:44600
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.25:44600
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9rf92edt
distributed.worker - INFO -          dashboard at:          172.21.3.25:44642
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.27:35579
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:43884
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:43884
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.27:35579
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.27:40329
distributed.worker - INFO -          dashboard at:          172.21.3.15:37912
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:36230
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4oumqxvt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:36230
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.27:40246
distributed.worker - INFO -          dashboard at:          172.21.3.15:38541
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dg3ghvzp
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-441owyaw
distributed.worker - INFO -          Listening to:    tcp://172.21.3.27:40246
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -          dashboard at:          172.21.3.27:33423
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wnskjv99
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zly8jy86
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:42470
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:42470
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:41500
distributed.worker - INFO -          dashboard at:          172.21.3.33:45022
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:41500
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.33:35574
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e09hx_ed
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-__y7pdhc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.21:39799
distributed.worker - INFO -          Listening to:    tcp://172.21.3.21:39799
distributed.worker - INFO -          dashboard at:          172.21.3.21:37950
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uue87m25
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.21:36181
distributed.worker - INFO -          Listening to:    tcp://172.21.3.21:36181
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.21:46297
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pazzmpdv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:36152
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:36152
distributed.worker - INFO -          dashboard at:          172.21.3.38:45524
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:43528
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:43528
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.38:37198
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fmhtad40
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_exrekja
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:38990
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:44751
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:38990
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:44751
distributed.worker - INFO -          dashboard at:          172.21.3.19:39860
distributed.worker - INFO -          dashboard at:          172.21.3.19:42169
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9zymc9sm
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qlvehv0r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.24:41311
distributed.worker - INFO -          Listening to:    tcp://172.21.3.24:41311
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.24:37958
distributed.worker - INFO -          dashboard at:          172.21.3.24:33770
distributed.worker - INFO -          Listening to:    tcp://172.21.3.24:37958
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.24:44272
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sufol0ra
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zzozs8dt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.20:45963
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.31:45253
distributed.worker - INFO -          Listening to:    tcp://172.21.3.20:45963
distributed.worker - INFO -          dashboard at:          172.21.3.20:43401
distributed.worker - INFO -          Listening to:    tcp://172.21.3.31:45253
distributed.worker - INFO -          dashboard at:          172.21.3.31:37998
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.20:41199
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xx01nv6c
distributed.worker - INFO -          Listening to:    tcp://172.21.3.20:41199
distributed.worker - INFO -          dashboard at:          172.21.3.20:42061
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-txqbn_xe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.31:37701
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.31:37701
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iq04b1mi
distributed.worker - INFO -          dashboard at:          172.21.3.31:44289
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ym03nvss
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:34102
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:34102
distributed.worker - INFO -          dashboard at:          172.21.3.18:42183
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.30:46035
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:33151
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:33151
distributed.worker - INFO -          dashboard at:          172.21.3.18:37807
distributed.worker - INFO -          Listening to:    tcp://172.21.3.30:46035
distributed.worker - INFO -          dashboard at:          172.21.3.30:34695
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_6ll0_d
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5no4asih
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.30:34101
distributed.worker - INFO -          Listening to:    tcp://172.21.3.30:34101
distributed.worker - INFO -          dashboard at:          172.21.3.30:38218
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d052xsab
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7l1vstdj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:34026
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:34026
distributed.worker - INFO -          dashboard at:           172.21.3.5:33624
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:40836
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:40836
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:           172.21.3.5:33814
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cz5rzx2r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-revk56sf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:38891
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:35386
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:38891
distributed.worker - INFO -          dashboard at:          172.21.3.17:34919
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:35386
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.17:37147
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2l11nrr6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e7tu0t9s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.28:41302
distributed.worker - INFO -          Listening to:    tcp://172.21.3.28:41302
distributed.worker - INFO -          dashboard at:          172.21.3.28:46328
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.28:37354
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.28:37354
distributed.worker - INFO -          dashboard at:          172.21.3.28:45979
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-08lsacea
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b0g9kunj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:39222
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:39222
distributed.worker - INFO -          dashboard at:          172.21.3.16:41937
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:42633
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:42633
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.16:40765
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.253:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zktd4vhy
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i33zze00
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.253:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1374: error: *** STEP 8468118.1 ON irene1374 CANCELLED AT 2023-02-22T09:10:17 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:36981'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.28:43328'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.24:33307'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.27:40035'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:46068'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:39607'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:39203'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.31:42615'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:36078'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.25:43795'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.21:37449'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:42747'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:37507'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.30:38100'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.20:38511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:38837'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:42162'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.28:41012'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.24:39725'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.27:46583'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:40260'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:44024'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:38769'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.31:40738'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:41554'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.25:33787'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.21:36611'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:37579'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:44489'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.30:41780'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.20:36511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:36792'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.24:37958
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:36152
distributed.worker - INFO - Stopping worker at tcp://172.21.3.31:37701
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:33151
distributed.worker - INFO - Stopping worker at tcp://172.21.3.20:45963
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:44751
distributed.worker - INFO - Stopping worker at tcp://172.21.3.24:41311
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:43528
distributed.worker - INFO - Stopping worker at tcp://172.21.3.31:45253
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:34102
distributed.worker - INFO - Stopping worker at tcp://172.21.3.20:41199
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:38990
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:42633
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:39222
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:42470
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:41500
distributed.worker - INFO - Stopping worker at tcp://172.21.3.30:46035
distributed.worker - INFO - Stopping worker at tcp://172.21.3.30:34101
distributed.worker - INFO - Stopping worker at tcp://172.21.3.21:39799
distributed.worker - INFO - Stopping worker at tcp://172.21.3.21:36181
distributed.worker - INFO - Stopping worker at tcp://172.21.3.28:41302
distributed.worker - INFO - Stopping worker at tcp://172.21.3.28:37354
distributed.worker - INFO - Stopping worker at tcp://172.21.3.25:33250
distributed.worker - INFO - Stopping worker at tcp://172.21.3.25:44600
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:35386
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:38891
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:43884
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:36230
distributed.worker - INFO - Stopping worker at tcp://172.21.3.27:35579
distributed.worker - INFO - Stopping worker at tcp://172.21.3.27:40246
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:40836
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:34026
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51748 parent=51649 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34552 parent=34451 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58113 parent=58015 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51749 parent=51648 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48503 parent=48401 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48502 parent=48400 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58114 parent=58014 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23547 parent=23444 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19607 parent=19507 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34551 parent=34452 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23548 parent=23445 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39883 parent=39781 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43686 parent=43585 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20273 parent=20173 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20272 parent=20172 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39882 parent=39782 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19608 parent=19508 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66221 parent=66123 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43685 parent=43586 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66222 parent=66122 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12061 parent=11955 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=92642 parent=92538 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=92641 parent=92539 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=3018 parent=2914 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12060 parent=11954 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47622 parent=47521 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=3019 parent=2915 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47623 parent=47522 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52389 parent=52288 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52388 parent=52287 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf3a4c0)

Current thread 0x00002abb6dee4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11fd4c0)

Current thread 0x00002b490030eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6f44c0)

Current thread 0x00002b4335be0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb364c0)

Current thread 0x00002b06d6b5fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22d94c0)

Current thread 0x00002ad6e6075b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19aa4c0)

Current thread 0x00002aeaacb9bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11e04c0)

Current thread 0x00002b69532f3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15d94c0)

Current thread 0x00002b7d42e24b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15854c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20de4c0)

Current thread 0x00002af22071bb80 (most recent call first):
<no Python frame>

Current thread 0x00002addce7d9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x103e4c0)

Current thread 0x00002b9921457b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x21fc4c0)

Current thread 0x00002b63a0826b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4ee4c0)

Current thread 0x00002b440e61fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11f24c0)

Current thread 0x00002b5e71b37b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1eea4c0)

Current thread 0x00002b949f902b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x117d4c0)

Current thread 0x00002b3f383d3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf9c4c0)

Current thread 0x00002b65d88cbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa634c0)

Current thread 0x00002ac5fc643b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5ee4c0)

Current thread 0x00002b478f741b80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x7554c0)

Current thread 0x00002b1734670b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f7d4c0)

Current thread 0x00002b21acdc4b80 (most recent call first):
<no Python frame>
srun: error: irene1374: tasks 0-1: Aborted (core dumped)
srun: error: irene1399: tasks 24-25: Aborted (core dumped)
srun: error: irene1400: tasks 26-27: Aborted (core dumped)
srun: error: irene1402: tasks 28-29: Aborted (core dumped)
srun: error: irene1388: tasks 10-11: Aborted (core dumped)
srun: error: irene1386: tasks 6-7: Aborted (core dumped)
srun: error: irene1389: tasks 12-13: Aborted (core dumped)
srun: error: irene1407: task 31: Aborted (core dumped)
srun: error: irene1393: task 16: Aborted (core dumped)
srun: error: irene1384: task 3: Aborted (core dumped)
srun: error: irene1397: task 22: Aborted (core dumped)
srun: error: irene1390: task 14: Aborted (core dumped)
srun: error: irene1394: task 19: Aborted (core dumped)
srun: error: irene1385: task 5: Aborted (core dumped)
