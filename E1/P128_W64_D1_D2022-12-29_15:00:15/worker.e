distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.12:39976'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.12:33451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.58:38694'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.214:43361'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.10:34844'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.15:34840'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.143:37014'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:39042'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.211:34388'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.210:43987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.203:45234'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.58:33999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.214:34916'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.10:39410'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.15:32916'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.49:32915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.143:40922'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:43491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.210:45384'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.13:32926'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.49:42700'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.13:34496'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.211:43557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.59:34209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.59:38163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:36003'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.217:33072'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:37625'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.217:37382'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.204:34799'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:33046'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.51:39745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.204:44052'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.51:35291'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:39239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:37781'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:46604'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:37868'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:43607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.14:37776'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:42761'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.11:33105'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:36759'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.14:33578'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.11:41458'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.220:37623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.50:46188'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.50:46179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.220:38885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.16:39230'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.16:37482'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.144:41628'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.144:44678'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.215:41405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.218:41007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:35766'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.218:40086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.219:39780'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:39844'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.219:44624'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.215:45590'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.216:37377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.216:35512'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.203:44618'
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:36284
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:36284
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.219:35823
distributed.worker - INFO -          Listening to:   tcp://172.21.6.219:35823
distributed.worker - INFO -          dashboard at:         172.21.6.206:37588
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.215:34474
distributed.worker - INFO -          Listening to:   tcp://172.21.6.215:34474
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.59:40548
distributed.worker - INFO -          Listening to:    tcp://172.21.1.59:40548
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:45267
distributed.worker - INFO -          dashboard at:          172.21.1.59:41878
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0r47hla5
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.217:44903
distributed.worker - INFO -          Listening to:   tcp://172.21.6.217:44903
distributed.worker - INFO -          dashboard at:         172.21.6.217:42996
distributed.worker - INFO -          dashboard at:         172.21.6.219:44438
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.204:39646
distributed.worker - INFO -          dashboard at:         172.21.6.215:35591
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:45267
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.206:38145
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.204:39646
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:45362
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:45362
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:45035
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:45035
distributed.worker - INFO -          dashboard at:         172.21.6.213:34141
distributed.worker - INFO -          dashboard at:         172.21.6.204:36810
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.13:39525
distributed.worker - INFO -          Listening to:    tcp://172.21.1.13:39525
distributed.worker - INFO -          dashboard at:          172.21.1.13:39772
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ahr2rlrh
distributed.worker - INFO -          dashboard at:         172.21.6.205:43888
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zsdu8a5f
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.215:43210
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-og5lbhpm
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.50:38806
distributed.worker - INFO -          Listening to:    tcp://172.21.1.50:38806
distributed.worker - INFO -          dashboard at:          172.21.1.50:38179
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.58:39497
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.214:44286
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.10:37566
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.15:35942
distributed.worker - INFO -          Listening to:    tcp://172.21.1.15:35942
distributed.worker - INFO -          dashboard at:          172.21.1.15:39503
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.220:41854
distributed.worker - INFO -          Listening to:   tcp://172.21.6.220:41854
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:36158
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:41457
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.216:44201
distributed.worker - INFO -          Listening to:   tcp://172.21.6.216:44201
distributed.worker - INFO -          dashboard at:         172.21.6.216:42298
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.49:34142
distributed.worker - INFO -          Listening to:    tcp://172.21.1.49:34142
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.143:43802
distributed.worker - INFO -          Listening to:   tcp://172.21.1.143:43802
distributed.worker - INFO -          dashboard at:         172.21.1.143:40026
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:35089
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:35089
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:40957
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:40957
distributed.worker - INFO -          dashboard at:         172.21.6.208:43441
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.211:32900
distributed.worker - INFO -          Listening to:   tcp://172.21.6.211:32900
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-onk1tok4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.12:40421
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.14:44709
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.16:35602
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.203:38738
distributed.worker - INFO -          Listening to:   tcp://172.21.6.203:38738
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.58:39497
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.214:34599
distributed.worker - INFO -          Listening to:   tcp://172.21.6.214:34599
distributed.worker - INFO -          dashboard at:         172.21.6.214:38275
distributed.worker - INFO -          Listening to:    tcp://172.21.1.10:37566
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.218:34542
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.15:38783
distributed.worker - INFO -          Listening to:    tcp://172.21.1.15:38783
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.220:45480
distributed.worker - INFO -          Listening to:   tcp://172.21.6.220:45480
distributed.worker - INFO -          dashboard at:         172.21.6.220:44337
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:36158
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.217:43772
distributed.worker - INFO -          Listening to:   tcp://172.21.6.217:43772
distributed.worker - INFO -          dashboard at:         172.21.6.217:41337
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:41457
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.216:36293
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.49:42971
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:36374
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.219:37559
distributed.worker - INFO -          Listening to:   tcp://172.21.6.219:37559
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.211:34142
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.51:41787
distributed.worker - INFO -          Listening to:    tcp://172.21.1.51:41787
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.59:41526
distributed.worker - INFO -          Listening to:    tcp://172.21.1.59:41526
distributed.worker - INFO -          Listening to:    tcp://172.21.1.12:40421
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.210:42047
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.13:45814
distributed.worker - INFO -          Listening to:    tcp://172.21.1.14:44709
distributed.worker - INFO -          Listening to:    tcp://172.21.1.16:35602
distributed.worker - INFO -          dashboard at:         172.21.6.203:40594
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.50:34252
distributed.worker - INFO -          dashboard at:          172.21.1.58:43512
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.144:44445
distributed.worker - INFO -          Listening to:   tcp://172.21.1.144:44445
distributed.worker - INFO -          dashboard at:         172.21.1.144:46586
distributed.worker - INFO -          Listening to:   tcp://172.21.6.214:44286
distributed.worker - INFO -          dashboard at:          172.21.1.10:42903
distributed.worker - INFO -          Listening to:   tcp://172.21.6.218:34542
distributed.worker - INFO -          dashboard at:          172.21.1.15:34831
distributed.worker - INFO -          dashboard at:         172.21.6.220:44908
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          dashboard at:         172.21.6.212:36317
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.207:34178
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          dashboard at:          172.21.1.49:44840
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.205:40019
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:43011
distributed.worker - INFO -          dashboard at:         172.21.6.219:32868
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          dashboard at:         172.21.6.211:38348
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x_86dqft
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.204:33091
distributed.worker - INFO -          Listening to:   tcp://172.21.6.215:43210
distributed.worker - INFO -          dashboard at:         172.21.6.215:37939
distributed.worker - INFO -          dashboard at:          172.21.1.51:35112
distributed.worker - INFO -          dashboard at:          172.21.1.59:40055
distributed.worker - INFO -          dashboard at:          172.21.1.12:34332
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.11:46660
distributed.worker - INFO -          Listening to:    tcp://172.21.1.11:46660
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.14:45541
distributed.worker - INFO -          dashboard at:          172.21.1.16:40163
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.10:39235
distributed.worker - INFO -          dashboard at:         172.21.6.218:41184
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x2k7nvx4
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.216:36293
distributed.worker - INFO -          dashboard at:         172.21.6.216:46839
distributed.worker - INFO -          Listening to:    tcp://172.21.1.49:42971
distributed.worker - INFO -          dashboard at:          172.21.1.49:39926
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:36374
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.211:34142
distributed.worker - INFO -          dashboard at:         172.21.6.211:45780
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.210:42047
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h0m920jf
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.50:34252
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          dashboard at:         172.21.6.214:44341
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.10:39235
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.143:37330
distributed.worker - INFO -          Listening to:   tcp://172.21.1.143:37330
distributed.worker - INFO -          dashboard at:         172.21.6.209:34054
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:43011
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:40076
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:40076
distributed.worker - INFO -          dashboard at:         172.21.6.213:36857
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t2gd2f65
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.210:41809
distributed.worker - INFO -          dashboard at:          172.21.1.11:38857
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.50:40867
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_thhpz40
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yag5iedb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4wxjlgn6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lmjvu_vm
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.204:33091
distributed.worker - INFO -          dashboard at:         172.21.6.204:45822
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.51:36159
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.11:42535
distributed.worker - INFO -          Listening to:    tcp://172.21.1.13:45814
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.16:37446
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2q5y7s0q
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oi7s_toz
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:39437
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.1.143:43090
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.208:37100
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4au_j0yi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jyf7habg
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q1bygzay
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.13:33283
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iq65nqoe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t36w2gzx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u3myz7yw
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.10:39305
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zykibzeq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t_h51j3j
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-11vvkm0y
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.51:36159
distributed.worker - INFO -          dashboard at:          172.21.1.51:43553
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b572pv5j
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8fwztkdb
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.11:42535
distributed.worker - INFO -          dashboard at:          172.21.1.11:40705
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dbmo6l9y
distributed.worker - INFO -          Listening to:    tcp://172.21.1.16:37446
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.203:38267
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iztyd4dq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7ppif9ow
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:39437
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4k1nq86x
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uz9y9jvq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l9aoxtzd
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u5h4j3v0
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-txo0tsck
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qlsvvi5s
distributed.worker - INFO -          Listening to:   tcp://172.21.6.203:38267
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eqqqpwgk
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.58:40167
distributed.worker - INFO -          Listening to:    tcp://172.21.1.58:40167
distributed.worker - INFO -          dashboard at:          172.21.1.58:46842
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.144:46678
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2ixkeayw
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.218:33076
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3wdyxdex
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:43917
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:43917
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l7n93e2y
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l_uz8vjp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.12:44199
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.210:44795
distributed.worker - INFO -          Listening to:   tcp://172.21.6.210:44795
distributed.worker - INFO -          dashboard at:         172.21.6.210:35521
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.14:36307
distributed.worker - INFO -          Listening to:    tcp://172.21.1.14:36307
distributed.worker - INFO -          dashboard at:          172.21.1.16:35152
distributed.worker - INFO -          dashboard at:         172.21.6.203:36260
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.144:46678
distributed.worker - INFO -          dashboard at:         172.21.1.144:45857
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_1wkseuv
distributed.worker - INFO -          Listening to:   tcp://172.21.6.218:33076
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.212:32843
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_xxadbjn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-80rnz9ea
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b8ncltkw
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wkw65qxk
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.12:44199
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.14:43244
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ie_mlpry
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -          dashboard at:         172.21.6.207:42109
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o1cbrplk
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gjyo4qfy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:33406
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.12:34872
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xcrcqdnb
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.218:39601
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:33406
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jvz837aq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0f9pzwle
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wrltj6bf
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y2gogkop
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.209:43562
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5hwgoe23
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bj610om7
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c66vmlqw
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hl1wdatx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ulocbgrh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bghztebb
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i1vqeur7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-exmzal_b
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xt7r1yon
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ji6maieg
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sjoxt4km
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 128.04 MiB from 436 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.1.49:42971
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4123, in _get_data
    response = await send_recv(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.21.1.51:57412 remote=tcp://172.21.1.49:42971>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.1.49:42971
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4123, in _get_data
    response = await send_recv(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.21.1.50:44388 remote=tcp://172.21.1.49:42971>: Stream is closed
distributed.nanny - INFO - Worker process 23442 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.49:45591
distributed.worker - INFO -          Listening to:    tcp://172.21.1.49:45591
distributed.worker - INFO -          dashboard at:          172.21.1.49:46315
distributed.worker - INFO - Waiting to connect to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dxcfhwro
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:      tcp://172.21.1.7:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1103: error: *** STEP 7728246.1 ON irene1103 CANCELLED AT 2022-12-29T18:57:20 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.50:46188'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.220:38885'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.219:44624'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:35766'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.210:43987'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.203:45234'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.50:46179'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.220:37623'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.219:39780'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:39844'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.210:45384'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.203:44618'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.16:39230'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.16:37482'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:39042'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:43491'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.12:39976'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.12:33451'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.14:33578'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.14:37776'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.143:37014'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:37625'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:36003'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.216:37377'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.11:41458'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.216:35512'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.143:40922'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.11:33105'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.218:41007'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.218:40086'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:42761'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:37868'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.10:34844'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:43607'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.215:41405'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.59:34209'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.13:32926'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.217:33072'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.10:39410'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.217:37382'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:39239'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.215:45590'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.13:34496'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.204:44052'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.144:41628'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.144:44678'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.204:34799'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.211:43557'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.211:34388'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.59:38163'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.214:43361'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.214:34916'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:37781'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:33046'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:46604'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:36759'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.58:33999'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.58:38694'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.49:32915'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.49:42700'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.51:39745'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.13:45814
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.1.13:39525
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.51:35291'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.203:38267
distributed.worker - INFO - Stopping worker at tcp://172.21.6.220:45480
distributed.worker - INFO - Stopping worker at tcp://172.21.6.203:38738
distributed.worker - INFO - Stopping worker at tcp://172.21.6.218:34542
distributed.worker - INFO - Stopping worker at tcp://172.21.6.220:41854
distributed.worker - INFO - Stopping worker at tcp://172.21.6.218:33076
distributed.worker - INFO - Stopping worker at tcp://172.21.6.219:37559
distributed.worker - INFO - Stopping worker at tcp://172.21.6.219:35823
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:40076
distributed.worker - INFO - Stopping worker at tcp://172.21.1.14:36307
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:45035
distributed.worker - INFO - Stopping worker at tcp://172.21.1.14:44709
distributed.worker - INFO - Stopping worker at tcp://172.21.6.210:42047
distributed.worker - INFO - Stopping worker at tcp://172.21.6.210:44795
distributed.worker - INFO - Stopping worker at tcp://172.21.1.50:38806
distributed.worker - INFO - Stopping worker at tcp://172.21.1.50:34252
distributed.worker - INFO - Stopping worker at tcp://172.21.6.216:36293
distributed.worker - INFO - Stopping worker at tcp://172.21.1.144:46678
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:33406
distributed.worker - INFO - Stopping worker at tcp://172.21.1.144:44445
distributed.worker - INFO - Stopping worker at tcp://172.21.6.214:44286
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:45267
distributed.worker - INFO - Stopping worker at tcp://172.21.6.217:44903
distributed.worker - INFO - Stopping worker at tcp://172.21.6.216:44201
distributed.worker - INFO - Stopping worker at tcp://172.21.1.143:37330
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:36374
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:35089
distributed.worker - INFO - Stopping worker at tcp://172.21.6.204:33091
distributed.worker - INFO - Stopping worker at tcp://172.21.1.12:40421
distributed.worker - INFO - Stopping worker at tcp://172.21.1.11:46660
distributed.worker - INFO - Stopping worker at tcp://172.21.1.16:35602
distributed.worker - INFO - Stopping worker at tcp://172.21.6.214:34599
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:36284
distributed.worker - INFO - Stopping worker at tcp://172.21.6.217:43772
distributed.worker - INFO - Stopping worker at tcp://172.21.1.143:43802
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:45362
distributed.worker - INFO - Stopping worker at tcp://172.21.6.204:39646
distributed.worker - INFO - Stopping worker at tcp://172.21.1.12:44199
distributed.worker - INFO - Stopping worker at tcp://172.21.1.11:42535
distributed.worker - INFO - Stopping worker at tcp://172.21.1.16:37446
distributed.worker - INFO - Stopping worker at tcp://172.21.6.211:32900
distributed.worker - INFO - Stopping worker at tcp://172.21.6.211:34142
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:43011
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:40957
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:41457
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:39437
distributed.worker - INFO - Stopping worker at tcp://172.21.1.49:34142
distributed.worker - INFO - Stopping worker at tcp://172.21.1.49:45591
distributed.worker - INFO - Stopping worker at tcp://172.21.1.59:40548
distributed.worker - INFO - Stopping worker at tcp://172.21.1.59:41526
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:43917
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:36158
distributed.worker - INFO - Stopping worker at tcp://172.21.6.215:43210
distributed.worker - INFO - Stopping worker at tcp://172.21.6.215:34474
distributed.worker - INFO - Stopping worker at tcp://172.21.1.10:37566
distributed.worker - INFO - Stopping worker at tcp://172.21.1.10:39235
distributed.worker - INFO - Stopping worker at tcp://172.21.1.51:41787
distributed.worker - INFO - Stopping worker at tcp://172.21.1.51:36159
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.15:34840'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.15:32916'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.15:38783
distributed.worker - INFO - Stopping worker at tcp://172.21.1.15:35942
distributed.worker - INFO - Stopping worker at tcp://172.21.1.58:39497
distributed.worker - INFO - Stopping worker at tcp://172.21.1.58:40167
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73491 parent=73381 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24296 parent=24188 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69996 parent=69887 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18824 parent=18709 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13279 parent=13171 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50308 parent=50198 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48413 parent=48303 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21142 parent=21027 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40295 parent=40180 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50307 parent=50197 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24297 parent=24187 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5000 parent=4892 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97383 parent=97273 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28141 parent=28029 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21143 parent=21026 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18823 parent=18710 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46378 parent=46266 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7554 parent=7442 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36548 parent=36440 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28142 parent=28028 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69997 parent=69888 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44306 parent=44197 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22282 parent=22171 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88285 parent=88175 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48414 parent=48302 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13280 parent=13170 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44305 parent=44196 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46672 parent=46561 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46377 parent=46267 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85011 parent=84901 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=11627 parent=11515 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40294 parent=40179 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97384 parent=97274 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73492 parent=73382 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22281 parent=22170 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85010 parent=84900 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7553 parent=7443 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61372 parent=61261 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5001 parent=4891 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65806 parent=65696 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=11626 parent=11514 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65716 parent=65605 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36549 parent=36439 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65715 parent=65604 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65807 parent=65697 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86714 parent=86604 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46671 parent=46562 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88284 parent=88176 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61373 parent=61260 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10302 parent=10191 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81276 parent=81167 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79789 parent=79677 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86713 parent=86605 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10303 parent=10190 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79790 parent=79678 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81277 parent=81166 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=15313 parent=15201 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=15314 parent=15202 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24923 parent=23329 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23441 parent=23330 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8382 parent=8268 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8381 parent=8269 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32013 parent=31902 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32014 parent=31901 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
slurmstepd-irene1142: error: Detected 1 oom-kill event(s) in StepId=7728246.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x202d4c0)

Current thread 0x00002b3396bf8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fee4c0)

Current thread 0x00002b99ef189b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8164c0)

Current thread 0x00002b43e6c94b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7734c0)

Current thread 0x00002b1fa48c1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22534c0)

Current thread 0x00002b5a97f36b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x131c4c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ef44c0)

Current thread 0x00002ab8ac582b80 (most recent call first):
<no Python frame>

Current thread 0x00002b6ee97a6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xefc4c0)

Current thread 0x00002b1c29a88b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1bf54c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Current thread 0x00002b5281959b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10774c0)

Python runtime state: finalizing (tstate=0x20cc4c0)

Current thread 0x00002af97bc82b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xaf14c0)

Current thread 0x00002ad43b553b80 (most recent call first):
<no Python frame>
Current thread 0x00002b15292c7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20794c0)

Current thread 0x00002b833d6dbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e9f4c0)

Current thread 0x00002b469a7e4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18ed4c0)

Current thread 0x00002b6e7427db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4e74c0)

Current thread 0x00002b4a0af4fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17fb4c0)

Current thread 0x00002adc6d295b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe964c0)

Current thread 0x00002b8f78c6bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b244c0)

Current thread 0x00002b0c6855ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xeea4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1aa64c0)

Current thread 0x00002b397539eb80 (most recent call first):
<no Python frame>
Current thread 0x00002b90b9da8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x73a4c0)

Current thread 0x00002ac8021c3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cf74c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcd54c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cd44c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16a04c0)

Current thread 0x00002b421c1aab80 (most recent call first):
<no Python frame>
Current thread 0x00002b632b821b80 (most recent call first):
<no Python frame>
Current thread 0x00002b3ad5a66b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Current thread 0x00002b0cd22c6b80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x1ef34c0)

Current thread 0x00002ae0e1933b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xaa34c0)

Current thread 0x00002b6ad4e12b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x21dd4c0)

Current thread 0x00002b3de242db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x89a4c0)

Current thread 0x00002b491bda2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16cd4c0)

Current thread 0x00002ba18d017b80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc944c0)

Current thread 0x00002b1a73d83b80 (most recent call first):
<no Python frame>
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d3c4c0)

Current thread 0x00002b4cfa1c4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fe54c0)

Current thread 0x00002b1bf3ae4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5174c0)

Current thread 0x00002adc2a5dcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16e44c0)

Current thread 0x00002b101f576b80 (most recent call first):
<no Python frame>
srun: error: irene1142: task 14: Out Of Memory
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x103d4c0)

Current thread 0x00002b9727960b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x64d4c0)

Current thread 0x00002b31e9bb1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11674c0)

Current thread 0x00002b46bbc4eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12754c0)

Current thread 0x00002ab330073b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18604c0)

Current thread 0x00002b9af13edb80 (most recent call first):
<no Python frame>
srun: error: irene1888: tasks 62-63: Aborted (core dumped)
srun: error: irene1873: tasks 32-33: Aborted (core dumped)
srun: error: irene1237: tasks 26-27: Aborted (core dumped)
srun: error: irene1104: tasks 2-3: Aborted (core dumped)
srun: error: irene1151: tasks 20-21: Aborted (core dumped)
srun: error: irene1881: tasks 48-49: Aborted (core dumped)
srun: error: irene1872: tasks 30-31: Aborted (core dumped)
srun: error: irene1877: tasks 40-41: Aborted (core dumped)
srun: error: irene1874: tasks 34-35: Aborted (core dumped)
srun: error: irene1109: task 13: Aborted (core dumped)
srun: error: irene1103: task 1: Aborted (core dumped)
srun: error: irene1236: task 25: Aborted (core dumped)
srun: error: irene1883: task 53: Aborted (core dumped)
srun: error: irene1106: task 6: Aborted (core dumped)
srun: error: irene1875: task 36: Aborted (core dumped)
srun: error: irene1876: task 39: Aborted (core dumped)
srun: error: irene1143: task 17: Aborted (core dumped)
srun: error: irene1105: task 5: Aborted (core dumped)
srun: error: irene1880: task 46: Aborted (core dumped)
srun: error: irene1152: task 23: Aborted (core dumped)
srun: error: irene1108: task 11: Aborted (core dumped)
srun: error: irene1144: task 18: Aborted (core dumped)
srun: error: irene1107: task 9: Aborted (core dumped)
srun: error: irene1886: task 58: Aborted (core dumped)
srun: error: irene1884: task 54: Aborted (core dumped)
srun: error: irene1871: task 29: Aborted (core dumped)
srun: error: irene1879: task 45: Aborted (core dumped)
srun: error: irene1878: task 42: Aborted (core dumped)
srun: error: irene1885: task 56: Aborted (core dumped)
srun: error: irene1887: task 60: Aborted (core dumped)
srun: error: irene1882: task 50: Aborted (core dumped)
