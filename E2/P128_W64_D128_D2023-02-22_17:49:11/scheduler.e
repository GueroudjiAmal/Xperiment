distributed.scheduler - INFO - -----------------------------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://172.21.0.161:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Receive client connection: Client-7b12ef58-b3fa-11ed-8ddf-080038b5512f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.172:44689', name: tcp://172.21.0.172:44689, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.172:44689
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.172:46073', name: tcp://172.21.0.172:46073, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.172:46073
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.176:33748', name: tcp://172.21.0.176:33748, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.176:33748
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.176:33760', name: tcp://172.21.0.176:33760, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.176:33760
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.192:45886', name: tcp://172.21.0.192:45886, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.192:45886
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.192:35361', name: tcp://172.21.0.192:35361, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.192:35361
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.180:46046', name: tcp://172.21.0.180:46046, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.180:46046
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.180:36100', name: tcp://172.21.0.180:36100, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.180:36100
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.166:39947', name: tcp://172.21.0.166:39947, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.166:39947
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.166:35061', name: tcp://172.21.0.166:35061, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.166:35061
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.174:33047', name: tcp://172.21.0.174:33047, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.174:33047
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.174:41838', name: tcp://172.21.0.174:41838, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.174:41838
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.189:44439', name: tcp://172.21.0.189:44439, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.189:44439
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.189:33968', name: tcp://172.21.0.189:33968, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.189:33968
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.187:35284', name: tcp://172.21.0.187:35284, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.187:35284
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.187:36534', name: tcp://172.21.0.187:36534, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.187:36534
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.191:40846', name: tcp://172.21.0.191:40846, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.191:40846
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.191:35141', name: tcp://172.21.0.191:35141, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.191:35141
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.249:43990', name: tcp://172.21.10.249:43990, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.249:43990
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.249:40159', name: tcp://172.21.10.249:40159, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.249:40159
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.3:36904', name: tcp://172.21.11.3:36904, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.3:36904
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.3:45769', name: tcp://172.21.11.3:45769, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.3:45769
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.165:46031', name: tcp://172.21.10.165:46031, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.165:46031
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.165:35482', name: tcp://172.21.10.165:35482, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.165:35482
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.237:34006', name: tcp://172.21.10.237:34006, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.237:34006
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.237:39707', name: tcp://172.21.10.237:39707, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.237:39707
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.16:39491', name: tcp://172.21.11.16:39491, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.16:39491
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.16:44795', name: tcp://172.21.11.16:44795, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.16:44795
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.21:42455', name: tcp://172.21.11.21:42455, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.21:42455
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.21:46069', name: tcp://172.21.11.21:46069, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.21:46069
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.177:43987', name: tcp://172.21.0.177:43987, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.177:43987
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.188:42672', name: tcp://172.21.0.188:42672, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.188:42672
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.177:37867', name: tcp://172.21.0.177:37867, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.177:37867
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.188:33593', name: tcp://172.21.0.188:33593, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.188:33593
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.182:43510', name: tcp://172.21.0.182:43510, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.182:43510
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.182:37424', name: tcp://172.21.0.182:37424, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.182:37424
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.168:39961', name: tcp://172.21.0.168:39961, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.168:39961
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.181:41029', name: tcp://172.21.0.181:41029, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.181:41029
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.181:38506', name: tcp://172.21.0.181:38506, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.181:38506
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.168:44089', name: tcp://172.21.0.168:44089, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.168:44089
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.190:32824', name: tcp://172.21.0.190:32824, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.190:32824
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.0.190:45164', name: tcp://172.21.0.190:45164, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.0.190:45164
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.22:40841', name: tcp://172.21.11.22:40841, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.22:40841
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.22:39711', name: tcp://172.21.11.22:39711, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.22:39711
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.12:40196', name: tcp://172.21.11.12:40196, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.12:40196
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.12:42559', name: tcp://172.21.11.12:42559, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.12:42559
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.248:46037', name: tcp://172.21.10.248:46037, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.248:46037
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.248:46531', name: tcp://172.21.10.248:46531, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.248:46531
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.10:45623', name: tcp://172.21.11.10:45623, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.10:45623
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.10:32813', name: tcp://172.21.11.10:32813, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.10:32813
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.14:35933', name: tcp://172.21.11.14:35933, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.14:35933
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.14:45605', name: tcp://172.21.11.14:45605, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.14:45605
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.15:45714', name: tcp://172.21.11.15:45714, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.15:45714
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.15:41595', name: tcp://172.21.11.15:41595, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.15:41595
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.11:43993', name: tcp://172.21.11.11:43993, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.11:43993
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.11:42385', name: tcp://172.21.11.11:42385, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.11:42385
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.1:43676', name: tcp://172.21.11.1:43676, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.1:43676
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.1:35864', name: tcp://172.21.11.1:35864, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.1:35864
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.244:34716', name: tcp://172.21.10.244:34716, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.244:34716
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.244:39955', name: tcp://172.21.10.244:39955, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.244:39955
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.23:36212', name: tcp://172.21.11.23:36212, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.23:36212
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.11.23:45731', name: tcp://172.21.11.23:45731, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.11.23:45731
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfaa821-b3fa-11ed-8edf-080038b514ba
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7df9e349-b3fa-11ed-8ede-080038b514ba
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfca559-b3fa-11ed-b4e4-080038b550fd
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfcddca-b3fa-11ed-b4e5-080038b550fd
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfd0c8e-b3fa-11ed-bf01-080038b53fa5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfd0601-b3fa-11ed-bf00-080038b53fa5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfe7e01-b3fa-11ed-abda-080038b550f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfdf487-b3fa-11ed-abd9-080038b550f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfd66cb-b3fa-11ed-b82d-080038b555f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7dfd735e-b3fa-11ed-b82e-080038b555f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e00b2c7-b3fa-11ed-b416-080038b552a6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e00671c-b3fa-11ed-b417-080038b552a6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e023678-b3fa-11ed-9e43-080038b5514d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e024323-b3fa-11ed-9e44-080038b5514d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e024417-b3fa-11ed-a749-080038b53e10
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0226e8-b3fa-11ed-a74a-080038b53e10
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.204:38916', name: tcp://172.21.10.204:38916, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.204:38916
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e03110f-b3fa-11ed-9efe-080038b54bf3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e030ce9-b3fa-11ed-9efd-080038b54bf3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0464d2-b3fa-11ed-83bc-080038b553f0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e05c2bf-b3fa-11ed-83bd-080038b553f0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.10.204:45991', name: tcp://172.21.10.204:45991, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.10.204:45991
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0d5567-b3fa-11ed-88aa-080038b5148d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0ae39b-b3fa-11ed-bcb8-080038b54a72
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0c54d4-b3fa-11ed-88a9-080038b5148d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0aec3a-b3fa-11ed-bcb9-080038b54a72
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0b28c3-b3fa-11ed-9905-080038b54554
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0b2d98-b3fa-11ed-9906-080038b54554
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0ebf52-b3fa-11ed-b496-080038b53d2a
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0de51a-b3fa-11ed-b495-080038b53d2a
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0e54ec-b3fa-11ed-be3b-080038b54243
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0e5c61-b3fa-11ed-be3c-080038b54243
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0fe1b7-b3fa-11ed-ac63-080038b56115
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e0fcd36-b3fa-11ed-ac64-080038b56115
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e12c640-b3fa-11ed-bfd9-080038b5530f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1179b5-b3fa-11ed-bfd8-080038b5530f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e12fc38-b3fa-11ed-b5d2-080038b550f3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e13594a-b3fa-11ed-b5d1-080038b550f3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e117f98-b3fa-11ed-a6dc-080038b5145b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e117b04-b3fa-11ed-a6dd-080038b5145b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e11dbb3-b3fa-11ed-b6d6-080038b543a6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e11e31e-b3fa-11ed-b6d7-080038b543a6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e13c046-b3fa-11ed-a3a1-080038b50ec5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e13640c-b3fa-11ed-aee0-080038b53cd5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1368b8-b3fa-11ed-aee1-080038b53cd5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e141090-b3fa-11ed-ba66-080038b55279
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1419d9-b3fa-11ed-ba67-080038b55279
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1480bf-b3fa-11ed-9328-080038b54d06
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e147cd9-b3fa-11ed-9329-080038b54d06
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e14ea26-b3fa-11ed-a3a2-080038b50ec5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e163f30-b3fa-11ed-a3ff-080038b55346
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e174bd2-b3fa-11ed-bbd8-080038b57a0b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e16c4b4-b3fa-11ed-a83c-080038b55337
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e17c56d-b3fa-11ed-a400-080038b55346
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e175206-b3fa-11ed-bbd9-080038b57a0b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e16a9d8-b3fa-11ed-9003-080038b56a0c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e15a72c-b3fa-11ed-9002-080038b56a0c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e16b47a-b3fa-11ed-a83d-080038b55337
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e18fda8-b3fa-11ed-a05f-080038b54d1f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e19029c-b3fa-11ed-a060-080038b54d1f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1a0f21-b3fa-11ed-98f7-080038b546b7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1a1b3f-b3fa-11ed-98f8-080038b546b7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1bbd07-b3fa-11ed-b528-080038b54ef0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1bc4dc-b3fa-11ed-b529-080038b54ef0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1b57a5-b3fa-11ed-b86f-080038b54487
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1b5cf6-b3fa-11ed-b870-080038b54487
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1aef70-b3fa-11ed-bb34-080038b55535
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1afa42-b3fa-11ed-bb33-080038b55535
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1d4319-b3fa-11ed-b9dd-080038b574cf
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1c2031-b3fa-11ed-ad50-080038b55355
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1d042d-b3fa-11ed-ad51-080038b55355
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1dded1-b3fa-11ed-bdcc-080038b5474d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1c4a13-b3fa-11ed-bdcb-080038b5474d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1cce80-b3fa-11ed-aadd-080038b513b1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1d6580-b3fa-11ed-aade-080038b513b1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1d4852-b3fa-11ed-b9de-080038b574cf
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1edd8f-b3fa-11ed-b618-080038b560cf
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1e9636-b3fa-11ed-b617-080038b560cf
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1dcec1-b3fa-11ed-8371-080038b53b4f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1ebf70-b3fa-11ed-8372-080038b53b4f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1f0e18-b3fa-11ed-bbc7-080038b55198
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1f098a-b3fa-11ed-bbc8-080038b55198
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1fc5ef-b3fa-11ed-9ada-080038b55201
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1fccfc-b3fa-11ed-9adb-080038b55201
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1f0065-b3fa-11ed-af9a-080038b54180
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1e0f97-b3fa-11ed-af99-080038b54180
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e20c528-b3fa-11ed-bae9-080038b547f7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e20a0c3-b3fa-11ed-bae8-080038b547f7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2101e9-b3fa-11ed-991e-080038b551f7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1f65b4-b3fa-11ed-9dec-080038b553ff
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e1faa0e-b3fa-11ed-9deb-080038b553ff
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e22dce5-b3fa-11ed-991f-080038b551f7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e229765-b3fa-11ed-82e1-080038b555da
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e21c0ca-b3fa-11ed-82e0-080038b555da
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e234192-b3fa-11ed-a931-080038b53ce9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e23457a-b3fa-11ed-a930-080038b53ce9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2630ec-b3fa-11ed-9d9e-080038b54324
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e25651c-b3fa-11ed-9d9d-080038b54324
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e258f18-b3fa-11ed-b73e-080038b54ab8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e25cf51-b3fa-11ed-b574-080038b6bdf5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e25937a-b3fa-11ed-b73f-080038b54ab8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e25d4db-b3fa-11ed-b575-080038b6bdf5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2767ee-b3fa-11ed-857c-080038b56138
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e27637d-b3fa-11ed-857b-080038b56138
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e283c2d-b3fa-11ed-8289-080038b6a341
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2749e3-b3fa-11ed-8288-080038b6a341
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e298e51-b3fa-11ed-b96a-080038b53e38
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e28c9e3-b3fa-11ed-b969-080038b53e38
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2a4c35-b3fa-11ed-b14d-080038b51064
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e293485-b3fa-11ed-a432-080038b57a24
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e298038-b3fa-11ed-b14c-080038b51064
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e29050d-b3fa-11ed-a431-080038b57a24
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e264296-b3fa-11ed-a851-080038b54162
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e264853-b3fa-11ed-a852-080038b54162
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2c24a1-b3fa-11ed-a1ea-080038b571a0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2b6315-b3fa-11ed-a1e9-080038b571a0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2d901c-b3fa-11ed-a1bd-080038b55341
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2c1976-b3fa-11ed-a1bc-080038b55341
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2d6a1f-b3fa-11ed-a139-080038b5146f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2d7517-b3fa-11ed-a138-080038b5146f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2d9be8-b3fa-11ed-963d-080038b541a3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2d5365-b3fa-11ed-963e-080038b541a3
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2fc5ef-b3fa-11ed-b8ff-080038b50bb9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2fd0a6-b3fa-11ed-b900-080038b50bb9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2fba5c-b3fa-11ed-b790-080038b541bc
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e2f3357-b3fa-11ed-b78f-080038b541bc
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e346b85-b3fa-11ed-8da4-080038b549e1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e33d935-b3fa-11ed-8da3-080038b549e1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e34777a-b3fa-11ed-abd4-080038b51677
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-7e34807d-b3fa-11ed-abd5-080038b51677
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'lambda'
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99310>, <Task finished name='Task-9068' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99820>, <Task finished name='Task-9069' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99940>, <Task finished name='Task-9070' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99a60>, <Task finished name='Task-9071' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99b80>, <Task finished name='Task-9072' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99ca0>, <Task finished name='Task-9073' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99dc0>, <Task finished name='Task-9074' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf99ee0>, <Task finished name='Task-9075' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf40040>, <Task finished name='Task-9076' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf40160>, <Task finished name='Task-9077' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf40280>, <Task finished name='Task-9078' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf403a0>, <Task finished name='Task-9079' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf404c0>, <Task finished name='Task-9080' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badf993a0>, <Task finished name='Task-9094' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd74280>, <Task finished name='Task-9095' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd74310>, <Task finished name='Task-9096' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd741f0>, <Task finished name='Task-9097' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd740d0>, <Task finished name='Task-9098' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd75280>, <Task finished name='Task-9099' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2badd75160>, <Task finished name='Task-9100' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.176:33748', name: tcp://172.21.0.176:33748, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.176:33748
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.180:36100', name: tcp://172.21.0.180:36100, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.180:36100
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.168:39961', name: tcp://172.21.0.168:39961, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.168:39961
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.187:35284', name: tcp://172.21.0.187:35284, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.187:35284
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.176:33760', name: tcp://172.21.0.176:33760, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.176:33760
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.172:46073', name: tcp://172.21.0.172:46073, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.172:46073
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.192:45886', name: tcp://172.21.0.192:45886, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.192:45886
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.166:39947', name: tcp://172.21.0.166:39947, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.166:39947
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.188:33593', name: tcp://172.21.0.188:33593, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.188:33593
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.188:42672', name: tcp://172.21.0.188:42672, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.188:42672
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.181:41029', name: tcp://172.21.0.181:41029, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.181:41029
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.181:38506', name: tcp://172.21.0.181:38506, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.181:38506
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.172:44689', name: tcp://172.21.0.172:44689, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.172:44689
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.191:40846', name: tcp://172.21.0.191:40846, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.191:40846
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.166:35061', name: tcp://172.21.0.166:35061, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.166:35061
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.180:46046', name: tcp://172.21.0.180:46046, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.180:46046
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.191:35141', name: tcp://172.21.0.191:35141, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.191:35141
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.187:36534', name: tcp://172.21.0.187:36534, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.187:36534
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.177:43987', name: tcp://172.21.0.177:43987, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.177:43987
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.192:35361', name: tcp://172.21.0.192:35361, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.192:35361
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.190:32824', name: tcp://172.21.0.190:32824, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.190:32824
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.182:37424', name: tcp://172.21.0.182:37424, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.182:37424
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.174:33047', name: tcp://172.21.0.174:33047, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.174:33047
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.189:44439', name: tcp://172.21.0.189:44439, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.189:44439
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.174:41838', name: tcp://172.21.0.174:41838, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.174:41838
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.189:33968', name: tcp://172.21.0.189:33968, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.189:33968
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.177:37867', name: tcp://172.21.0.177:37867, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.177:37867
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.182:43510', name: tcp://172.21.0.182:43510, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.182:43510
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.190:45164', name: tcp://172.21.0.190:45164, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.190:45164
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.0.168:44089', name: tcp://172.21.0.168:44089, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.0.168:44089
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.21:42455', name: tcp://172.21.11.21:42455, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.21:42455
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.12:42559', name: tcp://172.21.11.12:42559, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.12:42559
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.10:45623', name: tcp://172.21.11.10:45623, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.10:45623
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.12:40196', name: tcp://172.21.11.12:40196, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.12:40196
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.22:40841', name: tcp://172.21.11.22:40841, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.22:40841
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.1:43676', name: tcp://172.21.11.1:43676, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.1:43676
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.23:45731', name: tcp://172.21.11.23:45731, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.23:45731
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.3:45769', name: tcp://172.21.11.3:45769, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.3:45769
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.22:39711', name: tcp://172.21.11.22:39711, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.22:39711
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.21:46069', name: tcp://172.21.11.21:46069, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.21:46069
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.165:46031', name: tcp://172.21.10.165:46031, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.165:46031
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.237:34006', name: tcp://172.21.10.237:34006, status: closing, memory: 23, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.237:34006
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.237:39707', name: tcp://172.21.10.237:39707, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.237:39707
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.248:46037', name: tcp://172.21.10.248:46037, status: closing, memory: 25, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.248:46037
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.3:36904', name: tcp://172.21.11.3:36904, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.3:36904
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.1:35864', name: tcp://172.21.11.1:35864, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.1:35864
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.16:39491', name: tcp://172.21.11.16:39491, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.16:39491
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.165:35482', name: tcp://172.21.10.165:35482, status: closing, memory: 23, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.165:35482
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.11:43993', name: tcp://172.21.11.11:43993, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.11:43993
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.244:34716', name: tcp://172.21.10.244:34716, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.244:34716
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.15:45714', name: tcp://172.21.11.15:45714, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.15:45714
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.11:42385', name: tcp://172.21.11.11:42385, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.11:42385
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.249:43990', name: tcp://172.21.10.249:43990, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.249:43990
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.16:44795', name: tcp://172.21.11.16:44795, status: closing, memory: 23, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.16:44795
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.248:46531', name: tcp://172.21.10.248:46531, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.248:46531
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.204:45991', name: tcp://172.21.10.204:45991, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.204:45991
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.204:38916', name: tcp://172.21.10.204:38916, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.204:38916
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.244:39955', name: tcp://172.21.10.244:39955, status: closing, memory: 23, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.244:39955
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.10.249:40159', name: tcp://172.21.10.249:40159, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.10.249:40159
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.23:36212', name: tcp://172.21.11.23:36212, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.23:36212
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.14:35933', name: tcp://172.21.11.14:35933, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.14:35933
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.15:41595', name: tcp://172.21.11.15:41595, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.15:41595
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.14:45605', name: tcp://172.21.11.14:45605, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.14:45605
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.11.10:32813', name: tcp://172.21.11.10:32813, status: closing, memory: 24, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.11.10:32813
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing all comms
distributed.scheduler - INFO - End scheduler at 'tcp://172.21.0.161:8786'
