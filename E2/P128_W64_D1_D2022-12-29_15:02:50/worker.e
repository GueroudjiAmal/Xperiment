distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.188:37810'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.189:35633'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.164:46000'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.179:36818'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.188:43423'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.191:46131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.213:35530'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.209:45763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.211:43431'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.180:32843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.183:34419'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.189:46484'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.179:44643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.175:39894'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.191:42606'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.213:45261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.209:40339'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.211:35802'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.180:45696'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.175:33366'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.164:38938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.245:41630'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.163:42678'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.207:39316'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.205:41271'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.207:41501'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.205:36431'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.173:33363'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.173:42079'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.248:39322'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.166:39591'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.248:34458'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.166:41697'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.206:37736'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.206:43415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.172:45684'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.172:34917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.167:37490'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.167:42121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.247:39939'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.204:41885'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.246:34850'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.212:39353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.204:33901'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.212:42660'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.247:35313'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.246:37554'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.208:36406'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.208:38496'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.176:44216'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.176:38442'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.187:33754'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.249:44330'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.187:42718'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.190:44624'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.177:43195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.190:46786'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.177:39175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.174:45578'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.249:36762'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.174:35831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.183:33481'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.245:37182'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.163:43101'
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.207:33628
distributed.worker - INFO -          Listening to:   tcp://172.21.0.207:33628
distributed.worker - INFO -          dashboard at:         172.21.0.207:37092
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-obud2m07
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.207:42906
distributed.worker - INFO -          Listening to:   tcp://172.21.0.207:42906
distributed.worker - INFO -          dashboard at:         172.21.0.207:37356
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ijr4q1hh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.249:41815
distributed.worker - INFO -          Listening to:   tcp://172.21.0.249:41815
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.249:45101
distributed.worker - INFO -          Listening to:   tcp://172.21.0.249:45101
distributed.worker - INFO -          dashboard at:         172.21.0.249:34132
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.249:38484
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rmyjudfs
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yo2lxf4w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.167:44559
distributed.worker - INFO -          Listening to:   tcp://172.21.0.167:44559
distributed.worker - INFO -          dashboard at:         172.21.0.167:34994
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bjk71ou6
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.174:35004
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.167:38542
distributed.worker - INFO -          Listening to:   tcp://172.21.0.167:38542
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.174:36731
distributed.worker - INFO -          Listening to:   tcp://172.21.0.174:36731
distributed.worker - INFO -          dashboard at:         172.21.0.174:33714
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          dashboard at:         172.21.0.167:45285
distributed.worker - INFO -          Listening to:   tcp://172.21.0.174:35004
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          dashboard at:         172.21.0.174:44237
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.246:46732
distributed.worker - INFO -          Listening to:   tcp://172.21.0.246:46732
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.246:39905
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h0ld0r9z
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r4n9fqj3
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.246:39420
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-obdc_fh8
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.187:42896
distributed.worker - INFO -          Listening to:   tcp://172.21.0.187:42896
distributed.worker - INFO -          Listening to:   tcp://172.21.0.246:39420
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.246:39360
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.164:41350
distributed.worker - INFO -          Listening to:   tcp://172.21.0.164:41350
distributed.worker - INFO -          dashboard at:         172.21.0.187:42459
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.164:39062
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5lvy_jwl
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.247:44290
distributed.worker - INFO -          Listening to:   tcp://172.21.0.247:44290
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-08jy3jq3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.247:46757
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y__ay3x8
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.247:44645
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.247:44645
distributed.worker - INFO -          dashboard at:         172.21.0.247:32888
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u08dd533
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.190:36075
distributed.worker - INFO -          Listening to:   tcp://172.21.0.190:36075
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          dashboard at:         172.21.0.190:38228
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.190:35433
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.187:37732
distributed.worker - INFO -          Listening to:   tcp://172.21.0.187:37732
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.172:40220
distributed.worker - INFO -          Listening to:   tcp://172.21.0.172:40220
distributed.worker - INFO -          dashboard at:         172.21.0.172:38201
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.179:40203
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rna9ah2k
distributed.worker - INFO -          Listening to:   tcp://172.21.0.190:35433
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.180:46477
distributed.worker - INFO -          Listening to:   tcp://172.21.0.180:46477
distributed.worker - INFO -          dashboard at:         172.21.0.190:44587
distributed.worker - INFO -          dashboard at:         172.21.0.187:34067
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8y5fydog
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-39ctx107
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.180:45314
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.164:33130
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.0.179:40203
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.164:33130
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.179:46343
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.208:44489
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.164:39290
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g4od455_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-erkig3os
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.180:38162
distributed.worker - INFO -          Listening to:   tcp://172.21.0.180:38162
distributed.worker - INFO -          dashboard at:         172.21.0.180:38200
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.179:35083
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.208:44489
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cujky23b
distributed.worker - INFO -          Listening to:   tcp://172.21.0.179:35083
distributed.worker - INFO -          dashboard at:         172.21.0.208:33180
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.172:46430
distributed.worker - INFO -          Listening to:   tcp://172.21.0.172:46430
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          dashboard at:         172.21.0.172:35221
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ze7i699d
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.183:37982
distributed.worker - INFO -          Listening to:   tcp://172.21.0.183:37982
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.176:33467
distributed.worker - INFO -          Listening to:   tcp://172.21.0.176:33467
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.191:35844
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.183:37239
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oq586fkq
distributed.worker - INFO -          dashboard at:         172.21.0.179:45972
distributed.worker - INFO -          dashboard at:         172.21.0.176:37240
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.0.191:35844
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.183:33877
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nugw8i8m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.191:45106
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.208:33395
distributed.worker - INFO -          Listening to:   tcp://172.21.0.208:33395
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.212:44340
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.163:34572
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.176:44865
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.205:36616
distributed.worker - INFO -          Listening to:   tcp://172.21.0.205:36616
distributed.worker - INFO -          dashboard at:         172.21.0.205:35735
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cu0ct0nf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jukx75hb
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.177:40970
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.208:46408
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.212:44340
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8z04xlzq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b7c5u8er
distributed.worker - INFO -          Listening to:   tcp://172.21.0.163:34572
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_tqr1slf
distributed.worker - INFO -          Listening to:   tcp://172.21.0.176:44865
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.212:39201
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.163:35178
distributed.worker - INFO -          Listening to:   tcp://172.21.0.177:40970
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.206:35733
distributed.worker - INFO -          Listening to:   tcp://172.21.0.206:35733
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.183:33877
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.173:32817
distributed.worker - INFO -          Listening to:   tcp://172.21.0.173:32817
distributed.worker - INFO -          dashboard at:         172.21.0.173:38397
distributed.worker - INFO -          dashboard at:         172.21.0.177:42863
distributed.worker - INFO -          dashboard at:         172.21.0.176:44830
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-12ahfyaa
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.183:41064
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8o8c8a6w
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.188:46058
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.206:40683
distributed.worker - INFO -          Listening to:   tcp://172.21.0.206:40683
distributed.worker - INFO -          dashboard at:         172.21.0.206:41676
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3dnb2tm7
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          dashboard at:         172.21.0.206:37080
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.191:33740
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.212:35011
distributed.worker - INFO -          Listening to:   tcp://172.21.0.212:35011
distributed.worker - INFO -          dashboard at:         172.21.0.212:38666
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.205:46440
distributed.worker - INFO -          Listening to:   tcp://172.21.0.191:33740
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bx9_tx5s
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-btgpseuy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y2kdf1tw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.0.188:46058
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m4lc0kq2
distributed.worker - INFO -          dashboard at:         172.21.0.191:39249
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.173:36159
distributed.worker - INFO -          Listening to:   tcp://172.21.0.173:36159
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7fe8da4_
distributed.worker - INFO -          dashboard at:         172.21.0.188:42926
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.205:46440
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.213:44310
distributed.worker - INFO -          Listening to:   tcp://172.21.0.213:44310
distributed.worker - INFO -          dashboard at:         172.21.0.213:41070
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e6pw20yj
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.245:35290
distributed.worker - INFO -          Listening to:   tcp://172.21.0.245:35290
distributed.worker - INFO -          dashboard at:         172.21.0.245:43756
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.163:34525
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-72ud5al2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.205:41348
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.175:33422
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.163:34525
distributed.worker - INFO -          dashboard at:         172.21.0.173:39513
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.177:39588
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.166:32772
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.163:34563
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.248:35860
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.188:34536
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-40pao5up
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.245:46345
distributed.worker - INFO -          Listening to:   tcp://172.21.0.245:46345
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.177:39588
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-117hjlzh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.175:36592
distributed.worker - INFO -          Listening to:   tcp://172.21.0.175:36592
distributed.worker - INFO -          dashboard at:         172.21.0.175:34972
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wj6bgb91
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.177:46530
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.248:41511
distributed.worker - INFO -          Listening to:   tcp://172.21.0.188:34536
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j5ibsfky
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.175:33422
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.0.166:32772
distributed.worker - INFO -          dashboard at:         172.21.0.245:39276
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.0.248:41511
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ac6ir7e4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.213:36545
distributed.worker - INFO -          dashboard at:         172.21.0.166:42727
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n9iakgb5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.248:42660
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.188:43807
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-49cxy7tl
distributed.worker - INFO -          dashboard at:         172.21.0.175:44794
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t5gilf11
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-erixi84q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-liields3
distributed.worker - INFO -          Listening to:   tcp://172.21.0.213:36545
distributed.worker - INFO -          dashboard at:         172.21.0.213:46673
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.166:44628
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9xtov5ux
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hb9ekmwc
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l8c_iv67
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fxrx5wfi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.204:38532
distributed.worker - INFO -          Listening to:   tcp://172.21.0.204:38532
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.166:44628
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.204:39281
distributed.worker - INFO -          Listening to:   tcp://172.21.0.248:35860
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j54fl49_
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.248:33660
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tip1n25m
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0h0v7c7q
distributed.worker - INFO -          dashboard at:         172.21.0.166:39671
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.204:37975
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f9hg9q71
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.204:37975
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-99c2vftb
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5wshz45
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dj2muesm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.204:45476
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gx4llq9i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.209:35697
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.209:35912
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.189:45448
distributed.worker - INFO -          Listening to:   tcp://172.21.0.189:45448
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.211:38743
distributed.worker - INFO -          Listening to:   tcp://172.21.0.209:35697
distributed.worker - INFO -          dashboard at:         172.21.0.189:35043
distributed.worker - INFO -          Listening to:   tcp://172.21.0.209:35912
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.211:38743
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.189:36409
distributed.worker - INFO -          Listening to:   tcp://172.21.0.189:36409
distributed.worker - INFO -          dashboard at:         172.21.0.209:35470
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.209:34577
distributed.worker - INFO -          dashboard at:         172.21.0.211:44334
distributed.worker - INFO -          dashboard at:         172.21.0.189:43903
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_xtcxjk1
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8xzmpson
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wxwcmyvj
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-32y3tc3e
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a4pyy17s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.211:39470
distributed.worker - INFO -          Listening to:   tcp://172.21.0.211:39470
distributed.worker - INFO -          dashboard at:         172.21.0.211:46087
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oxvgz2gm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 85450 was killed by signal 9
distributed.nanny - INFO - Worker process 85449 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 4010 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.utils_perf - INFO - full garbage collection released 8.01 GiB from 59 reference cycles (threshold: 9.54 MiB)
distributed.nanny - INFO - Worker process 29622 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.utils_perf - INFO - full garbage collection released 8.00 GiB from 59 reference cycles (threshold: 9.54 MiB)
distributed.nanny - INFO - Worker process 4011 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.208:40521
distributed.worker - INFO -          Listening to:   tcp://172.21.0.208:40521
distributed.worker - INFO -          dashboard at:         172.21.0.208:37068
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c730d9z5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.208:39891
distributed.worker - INFO -          Listening to:   tcp://172.21.0.208:39891
distributed.worker - INFO -          dashboard at:         172.21.0.208:38775
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m2ypa3m3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.173:44578
distributed.worker - INFO -          Listening to:   tcp://172.21.0.173:44578
distributed.worker - INFO -          dashboard at:         172.21.0.173:45719
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.173:38959
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wflmebbk
distributed.worker - INFO -          Listening to:   tcp://172.21.0.173:38959
distributed.worker - INFO -          dashboard at:         172.21.0.173:40104
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fx7dt1zp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.206:44399
distributed.worker - INFO -          Listening to:   tcp://172.21.0.206:44399
distributed.worker - INFO -          dashboard at:         172.21.0.206:39965
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qtrh8pfo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1003: error: *** STEP 7728301.1 ON irene1003 CANCELLED AT 2022-12-29T21:07:01 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.167:37490'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.247:39939'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.213:35530'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.211:43431'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.180:45696'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.189:35633'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.164:38938'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.204:41885'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.190:46786'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.179:36818'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.187:33754'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.176:44216'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.246:37554'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.206:43415'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.191:42606'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.166:39591'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.172:34917'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.207:39316'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.183:34419'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.189:46484'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.164:46000'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.204:33901'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.190:44624'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.179:44643'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.187:42718'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.176:38442'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.246:34850'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.206:37736'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.167:42121'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.191:46131'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.247:35313'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.213:45261'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.166:41697'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.211:35802'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.172:45684'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.207:41501'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.180:32843'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.183:33481'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.249:36762'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.245:41630'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.249:44330'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.245:37182'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.173:33363'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.173:42079'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.208:38496'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.208:36406'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.163:42678'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.163:43101'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.213:36545
distributed.worker - INFO - Stopping worker at tcp://172.21.0.213:44310
distributed.worker - INFO - Stopping worker at tcp://172.21.0.191:33740
distributed.worker - INFO - Stopping worker at tcp://172.21.0.191:35844
distributed.worker - INFO - Stopping worker at tcp://172.21.0.211:38743
distributed.worker - INFO - Stopping worker at tcp://172.21.0.211:39470
distributed.worker - INFO - Stopping worker at tcp://172.21.0.206:44399
distributed.worker - INFO - Stopping worker at tcp://172.21.0.206:40683
distributed.worker - INFO - Stopping worker at tcp://172.21.0.249:45101
distributed.worker - INFO - Stopping worker at tcp://172.21.0.249:41815
distributed.worker - INFO - Stopping worker at tcp://172.21.0.208:39891
distributed.worker - INFO - Stopping worker at tcp://172.21.0.208:40521
distributed.worker - INFO - Stopping worker at tcp://172.21.0.179:40203
distributed.worker - INFO - Stopping worker at tcp://172.21.0.179:35083
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.176:33467
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.176:44865
distributed.worker - INFO - Stopping worker at tcp://172.21.0.187:42896
distributed.worker - INFO - Stopping worker at tcp://172.21.0.164:33130
distributed.worker - INFO - Stopping worker at tcp://172.21.0.187:37732
distributed.worker - INFO - Stopping worker at tcp://172.21.0.164:41350
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.173:44578
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.173:38959
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.0.247:44290
distributed.worker - INFO - Stopping worker at tcp://172.21.0.180:46477
distributed.worker - INFO - Stopping worker at tcp://172.21.0.180:38162
distributed.worker - INFO - Stopping worker at tcp://172.21.0.247:44645
distributed.worker - INFO - Stopping worker at tcp://172.21.0.190:36075
distributed.worker - INFO - Stopping worker at tcp://172.21.0.190:35433
distributed.worker - INFO - Stopping worker at tcp://172.21.0.166:44628
distributed.worker - INFO - Stopping worker at tcp://172.21.0.166:32772
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.177:43195'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.177:39175'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.167:38542
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.248:39322'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.209:45763'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.212:42660'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.248:34458'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.209:40339'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.212:39353'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.189:45448
distributed.worker - INFO - Stopping worker at tcp://172.21.0.167:44559
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.175:39894'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.189:36409
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.175:33366'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.246:39420
distributed.worker - INFO - Stopping worker at tcp://172.21.0.246:46732
distributed.worker - INFO - Stopping worker at tcp://172.21.0.204:38532
distributed.worker - INFO - Stopping worker at tcp://172.21.0.172:46430
distributed.worker - INFO - Stopping worker at tcp://172.21.0.204:37975
distributed.worker - INFO - Stopping worker at tcp://172.21.0.172:40220
distributed.worker - INFO - Stopping worker at tcp://172.21.0.207:42906
distributed.worker - INFO - Stopping worker at tcp://172.21.0.207:33628
distributed.worker - INFO - Stopping worker at tcp://172.21.0.177:40970
distributed.worker - INFO - Stopping worker at tcp://172.21.0.177:39588
distributed.worker - INFO - Stopping worker at tcp://172.21.0.163:34525
distributed.worker - INFO - Stopping worker at tcp://172.21.0.163:34572
distributed.worker - INFO - Stopping worker at tcp://172.21.0.183:33877
distributed.worker - INFO - Stopping worker at tcp://172.21.0.183:37982
distributed.worker - INFO - Stopping worker at tcp://172.21.0.175:33422
distributed.worker - INFO - Stopping worker at tcp://172.21.0.175:36592
distributed.worker - INFO - Stopping worker at tcp://172.21.0.248:41511
distributed.worker - INFO - Stopping worker at tcp://172.21.0.248:35860
distributed.worker - INFO - Stopping worker at tcp://172.21.0.212:44340
distributed.worker - INFO - Stopping worker at tcp://172.21.0.212:35011
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.188:37810'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.188:43423'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.174:45578'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.174:35831'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.205:36431'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.205:41271'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.209:35912
distributed.worker - INFO - Stopping worker at tcp://172.21.0.209:35697
distributed.worker - INFO - Stopping worker at tcp://172.21.0.174:35004
distributed.worker - INFO - Stopping worker at tcp://172.21.0.174:36731
distributed.worker - INFO - Stopping worker at tcp://172.21.0.188:34536
distributed.worker - INFO - Stopping worker at tcp://172.21.0.205:36616
distributed.worker - INFO - Stopping worker at tcp://172.21.0.205:46440
distributed.worker - INFO - Stopping worker at tcp://172.21.0.188:46058
distributed.worker - INFO - Stopping worker at tcp://172.21.0.245:35290
distributed.worker - INFO - Stopping worker at tcp://172.21.0.245:46345
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87451 parent=87332 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94912 parent=94795 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54310 parent=54194 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54311 parent=54195 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87449 parent=87333 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44637 parent=44520 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51117 parent=51001 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26370 parent=26253 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36889 parent=36773 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36890 parent=36772 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94911 parent=94796 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64692 parent=64576 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58264 parent=58148 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44636 parent=44519 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51118 parent=51000 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56625 parent=56508 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82147 parent=82028 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26369 parent=26252 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42837 parent=42720 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73654 parent=73538 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58263 parent=58147 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64693 parent=64577 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42838 parent=42721 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26124 parent=26009 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56626 parent=56507 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73655 parent=73539 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14512 parent=14395 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26126 parent=26008 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82146 parent=82027 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14511 parent=14396 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=80483 parent=80368 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88581 parent=88462 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=80484 parent=80367 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54551 parent=54435 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88580 parent=88463 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54552 parent=54434 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38303 parent=38186 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38302 parent=38187 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64789 parent=64671 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64788 parent=64672 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95755 parent=95640 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4348 parent=3892 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29621 parent=29505 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29949 parent=29504 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4342 parent=3893 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85780 parent=85334 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85781 parent=85333 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95756 parent=95639 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26611 parent=26493 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26610 parent=26492 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68398 parent=68283 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52498 parent=52380 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47279 parent=47162 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69484 parent=69369 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69486 parent=69368 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47278 parent=47163 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68399 parent=68282 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52497 parent=52381 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64515 parent=64398 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37195 parent=37078 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37194 parent=37077 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46500 parent=46382 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46499 parent=46383 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64514 parent=64397 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
slurmstepd-irene1046: error: Detected 1 oom-kill event(s) in StepId=7728301.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1013: error: Detected 4 oom-kill event(s) in StepId=7728301.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8704c0)

Current thread 0x00002ad09dcf6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16844c0)

Current thread 0x00002b9fa9fefb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x117a4c0)

Current thread 0x00002ab9b54b6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x49c4c0)

Current thread 0x00002ba89240db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x236d4c0)

Current thread 0x00002b69d30c3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e944c0)

Current thread 0x00002ae1b68c0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7154c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6c84c0)

Current thread 0x00002b31ce77ab80 (most recent call first):
<no Python frame>
Current thread 0x00002b70bdd71b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b534c0)

Current thread 0x00002ae0d1988b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17524c0)

Current thread 0x00002b058e577b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x154d4c0)

Current thread 0x00002abdbace7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ea24c0)

Current thread 0x00002ab893b99b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcca4c0)

Current thread 0x00002b72d718db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b814c0)

Current thread 0x00002b8d2db99b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x45e4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc044c0)

Current thread 0x00002b93b35d0b80 (most recent call first):
<no Python frame>
Current thread 0x00002b30d415bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17914c0)

Current thread 0x00002b9fa9923b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d354c0)

Current thread 0x00002aab62fd8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ca64c0)

Current thread 0x00002b19aba84b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a494c0)

Current thread 0x00002abf8cfd6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x192b4c0)

Current thread 0x00002b5498baeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19c24c0)

Current thread 0x00002b3ce4b73b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13c64c0)

Current thread 0x00002abdb7ca8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19954c0)

Current thread 0x00002b2947327b80 (most recent call first):
<no Python frame>
srun: error: irene1046: task 40: Out Of Memory
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe764c0)

Current thread 0x00002b752cf77b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4fd4c0)

Current thread 0x00002b1dc4826b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d1b4c0)

Current thread 0x00002b88484b0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b8f4c0)

Current thread 0x00002ba09af83b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5e94c0)

Current thread 0x00002ae0db479b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6d74c0)

Current thread 0x00002b4bd6b97b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x47c4c0)

Current thread 0x00002ba6e5f88b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16874c0)

Current thread 0x00002b606c611b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f3e4c0)

Current thread 0x00002abe308c0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcb24c0)

Current thread 0x00002b45d01aab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf304c0)

Current thread 0x00002b2dc3adeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x138f4c0)

Current thread 0x00002b97b146db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x237c4c0)

Current thread 0x00002ba7117ccb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc954c0)

Current thread 0x00002b7f416a0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x63a4c0)

Current thread 0x00002b69eebe6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8d84c0)

Current thread 0x00002b5344b39b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x178d4c0)

Current thread 0x00002b1323b6fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13c54c0)

Current thread 0x00002ac6c8686b80 (most recent call first):
<no Python frame>
slurmstepd-irene1048: error: Detected 2 oom-kill event(s) in StepId=7728301.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1007: tasks 6-7: Aborted (core dumped)
srun: error: irene1047: tasks 42-43: Aborted (core dumped)
srun: error: irene1085: tasks 54-55: Aborted (core dumped)
srun: error: irene1017: tasks 18-19: Aborted (core dumped)
srun: error: irene1045: tasks 38-39: Aborted (core dumped)
srun: error: irene1087: tasks 58-59: Aborted (core dumped)
srun: error: irene1051: tasks 48-49: Aborted (core dumped)
srun: error: irene1088: tasks 60-61: Aborted (core dumped)
srun: error: irene1052: tasks 50-51: Aborted (core dumped)
srun: error: irene1029: tasks 30-31: Aborted (core dumped)
srun: error: irene1053: tasks 52-53: Aborted (core dumped)
srun: error: irene1023: task 25: Aborted (core dumped)
srun: error: irene1015: task 14: Aborted (core dumped)
srun: error: irene1031: task 34: Aborted (core dumped)
srun: error: irene1012: task 9: Aborted (core dumped)
srun: error: irene1006: task 5: Aborted (core dumped)
srun: error: irene1020: task 22: Aborted (core dumped)
srun: error: irene1004: task 3: Aborted (core dumped)
srun: error: irene1044: task 37: Aborted (core dumped)
srun: error: irene1027: task 27: Aborted (core dumped)
srun: error: irene1049: task 46: Aborted (core dumped)
srun: error: irene1030: task 32: Aborted (core dumped)
srun: error: irene1089: task 63: Aborted (core dumped)
srun: error: irene1086: task 56: Aborted (core dumped)
srun: error: irene1028: task 28: Aborted (core dumped)
srun: error: irene1019: task 21: Aborted (core dumped)
srun: error: irene1016: task 16: Aborted (core dumped)
srun: error: irene1003: task 1: Aborted (core dumped)
