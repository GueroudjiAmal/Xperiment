distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.22:35302'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.22:44157'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.74:45070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.11:36947'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.148:33402'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.74:45714'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.11:41772'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.121:45637'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.127:36636'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.129:46179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.128:40099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.129:37718'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.187:40086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.187:37407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.73:38729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.100:45815'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.100:42526'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.75:43557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.148:35355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.128:39946'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.121:37205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.127:42016'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.75:38121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.73:34861'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.78:41785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.78:36110'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.143:33611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.143:36194'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.77:44100'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.77:42334'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.76:35738'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.76:44085'
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.127:39188
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.121:33328
distributed.worker - INFO -          Listening to:   tcp://172.21.3.121:33328
distributed.worker - INFO -          dashboard at:         172.21.3.121:36846
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.74:33165
distributed.worker - INFO -          Listening to:    tcp://172.21.3.74:33165
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.129:35849
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.3.127:39188
distributed.worker - INFO -          dashboard at:          172.21.3.74:44534
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.3.127:33656
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.121:33287
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.74:45498
distributed.worker - INFO -          Listening to:   tcp://172.21.3.121:33287
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.127:34771
distributed.worker - INFO -          Listening to:    tcp://172.21.3.74:45498
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ipyiy6jv
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.3.121:45783
distributed.worker - INFO -          Listening to:   tcp://172.21.3.127:34771
distributed.worker - INFO -          dashboard at:          172.21.3.74:35633
distributed.worker - INFO -          Listening to:   tcp://172.21.3.129:35849
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5bfu6l4g
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.129:37500
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.3.127:32810
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -          dashboard at:         172.21.3.129:34615
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wf8vmo2d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.3.129:37500
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4bugzkm_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.3.129:36519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.22:36645
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.22:36645
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8abqz_fd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.22:39939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.22:42094
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pntj83ac
distributed.worker - INFO -          Listening to:    tcp://172.21.3.22:39939
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jgfn0k3r
distributed.worker - INFO -          dashboard at:          172.21.3.22:41353
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nalmkzap
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wr_g6g2e
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zjnxkxl0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.73:43254
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.75:35329
distributed.worker - INFO -          Listening to:    tcp://172.21.3.73:43254
distributed.worker - INFO -          dashboard at:          172.21.3.73:36017
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.73:41076
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.73:41076
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.100:39110
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.73:46111
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zeq6u4yo
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jhtr330k
distributed.worker - INFO -          Listening to:   tcp://172.21.3.100:39110
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.3.100:45477
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.100:38586
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.3.100:38586
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.3.100:36079
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9voqzhli
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.75:35329
distributed.worker - INFO -          dashboard at:          172.21.3.75:36834
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.75:41483
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.75:41483
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.75:41760
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a9bg8qmr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mcf41d35
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oc76x0f5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.148:36894
distributed.worker - INFO -          Listening to:   tcp://172.21.3.148:36894
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.148:37112
distributed.worker - INFO -          dashboard at:         172.21.3.148:37273
distributed.worker - INFO -          Listening to:   tcp://172.21.3.148:37112
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.3.148:40275
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ilp7244q
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tlguxiq_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.187:34826
distributed.worker - INFO -          Listening to:   tcp://172.21.2.187:34826
distributed.worker - INFO -          dashboard at:         172.21.2.187:43607
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cyn8cngg
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.187:43294
distributed.worker - INFO -          Listening to:   tcp://172.21.2.187:43294
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.187:36044
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.128:36796
distributed.worker - INFO -          Listening to:   tcp://172.21.3.128:36796
distributed.worker - INFO -          dashboard at:         172.21.3.128:41848
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7t3wopq_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.128:34848
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.3.128:34848
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.3.128:38434
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_f7bfoc7
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ynom5ryk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.11:35086
distributed.worker - INFO -          Listening to:    tcp://172.21.3.11:35086
distributed.worker - INFO -          dashboard at:          172.21.3.11:35627
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.11:40844
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.11:40844
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.11:39424
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d34254h6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t9zrwshb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.78:46626
distributed.worker - INFO -          Listening to:    tcp://172.21.3.78:46626
distributed.worker - INFO -          dashboard at:          172.21.3.78:35355
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hi1jl0wn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.78:46542
distributed.worker - INFO -          Listening to:    tcp://172.21.3.78:46542
distributed.worker - INFO -          dashboard at:          172.21.3.78:37725
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xugox8h6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.77:44179
distributed.worker - INFO -          Listening to:    tcp://172.21.3.77:44179
distributed.worker - INFO -          dashboard at:          172.21.3.77:36255
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zh4f0vgd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.77:44922
distributed.worker - INFO -          Listening to:    tcp://172.21.3.77:44922
distributed.worker - INFO -          dashboard at:          172.21.3.77:45558
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_95mw2rw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.143:45082
distributed.worker - INFO -          Listening to:   tcp://172.21.3.143:45082
distributed.worker - INFO -          dashboard at:         172.21.3.143:44133
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ghplkbel
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.143:35587
distributed.worker - INFO -          Listening to:   tcp://172.21.3.143:35587
distributed.worker - INFO -          dashboard at:         172.21.3.143:39341
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xk383813
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.76:40519
distributed.worker - INFO -          Listening to:    tcp://172.21.3.76:40519
distributed.worker - INFO -          dashboard at:          172.21.3.76:40958
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.76:41110
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.76:41110
distributed.worker - INFO -          dashboard at:          172.21.3.76:35864
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e8sqom_d
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mj9ico63
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.161:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1303: error: *** STEP 8467222.1 ON irene1303 CANCELLED AT 2023-02-22T07:11:35 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.74:45714'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.129:37718'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.11:36947'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.76:44085'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.187:40086'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.100:42526'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.22:35302'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.143:33611'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.148:35355'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.78:41785'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.75:43557'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.77:42334'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.73:34861'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.128:40099'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.121:45637'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.127:42016'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.74:45070'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.129:46179'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.11:41772'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.76:35738'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.187:37407'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.100:45815'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.22:44157'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.143:36194'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.148:33402'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.78:36110'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.75:38121'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.77:44100'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.73:38729'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.128:39946'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.121:37205'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.127:36636'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.74:45498
distributed.worker - INFO - Stopping worker at tcp://172.21.2.187:43294
distributed.worker - INFO - Stopping worker at tcp://172.21.3.100:39110
distributed.worker - INFO - Stopping worker at tcp://172.21.3.22:39939
distributed.worker - INFO - Stopping worker at tcp://172.21.3.73:43254
distributed.worker - INFO - Stopping worker at tcp://172.21.3.74:33165
distributed.worker - INFO - Stopping worker at tcp://172.21.2.187:34826
distributed.worker - INFO - Stopping worker at tcp://172.21.3.100:38586
distributed.worker - INFO - Stopping worker at tcp://172.21.3.22:36645
distributed.worker - INFO - Stopping worker at tcp://172.21.3.73:41076
distributed.worker - INFO - Stopping worker at tcp://172.21.3.121:33287
distributed.worker - INFO - Stopping worker at tcp://172.21.3.121:33328
distributed.worker - INFO - Stopping worker at tcp://172.21.3.127:39188
distributed.worker - INFO - Stopping worker at tcp://172.21.3.127:34771
distributed.worker - INFO - Stopping worker at tcp://172.21.3.78:46542
distributed.worker - INFO - Stopping worker at tcp://172.21.3.78:46626
distributed.worker - INFO - Stopping worker at tcp://172.21.3.129:37500
distributed.worker - INFO - Stopping worker at tcp://172.21.3.129:35849
distributed.worker - INFO - Stopping worker at tcp://172.21.3.143:35587
distributed.worker - INFO - Stopping worker at tcp://172.21.3.143:45082
distributed.worker - INFO - Stopping worker at tcp://172.21.3.76:41110
distributed.worker - INFO - Stopping worker at tcp://172.21.3.76:40519
distributed.worker - INFO - Stopping worker at tcp://172.21.3.148:36894
distributed.worker - INFO - Stopping worker at tcp://172.21.3.148:37112
distributed.worker - INFO - Stopping worker at tcp://172.21.3.75:35329
distributed.worker - INFO - Stopping worker at tcp://172.21.3.75:41483
distributed.worker - INFO - Stopping worker at tcp://172.21.3.128:36796
distributed.worker - INFO - Stopping worker at tcp://172.21.3.128:34848
distributed.worker - INFO - Stopping worker at tcp://172.21.3.11:40844
distributed.worker - INFO - Stopping worker at tcp://172.21.3.77:44922
distributed.worker - INFO - Stopping worker at tcp://172.21.3.11:35086
distributed.worker - INFO - Stopping worker at tcp://172.21.3.77:44179
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23232 parent=23127 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23231 parent=23128 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6557 parent=6453 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6558 parent=6454 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13367 parent=13263 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57124 parent=57017 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57125 parent=57016 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13368 parent=13262 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=89363 parent=89259 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16111 parent=15975 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29738 parent=29633 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83963 parent=83860 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49900 parent=49795 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=89364 parent=89258 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19364 parent=19257 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21711 parent=21602 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29737 parent=29634 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19363 parent=19258 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16112 parent=15974 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21712 parent=21601 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49901 parent=49796 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83964 parent=83859 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6338 parent=6232 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6337 parent=6233 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28713 parent=28608 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28712 parent=28607 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57865 parent=57760 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57864 parent=57759 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96281 parent=96174 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96282 parent=96175 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f594c0)

Current thread 0x00002b71139e1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5204c0)

Current thread 0x00002b6333728b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e384c0)

Current thread 0x00002b3dad03fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1af14c0)

Current thread 0x00002b3bf1d44b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9064c0)

Current thread 0x00002b4695140b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23d54c0)

Current thread 0x00002af573cd8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa764c0)

Current thread 0x00002b511085fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c504c0)

Current thread 0x00002abd471d5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e994c0)

Current thread 0x00002b40c0730b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x189a4c0)

Python runtime state: finalizing (tstate=0x1a0c4c0)

Current thread 0x00002b575854bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd024c0)

Current thread 0x00002ab99c45bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13b24c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4464c0)

Current thread 0x00002b46ffd29b80 (most recent call first):
<no Python frame>
Current thread 0x00002b107d278b80 (most recent call first):
<no Python frame>
Current thread 0x00002b0a94637b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x167b4c0)

Current thread 0x00002ba9924a5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c804c0)

Current thread 0x00002b4f677a5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c324c0)

Current thread 0x00002b0454cb3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbac4c0)

Current thread 0x00002aac33b32b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x207f4c0)

Current thread 0x00002b25caf78b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa764c0)

Current thread 0x00002b9d55204b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13cc4c0)

Current thread 0x00002ba37dbb7b80 (most recent call first):
<no Python frame>
srun: error: irene1443: tasks 8-9: Aborted (core dumped)
srun: error: irene1490: tasks 20-21: Aborted (core dumped)
srun: error: irene1446: tasks 14-15: Aborted (core dumped)
srun: error: irene1469: tasks 18-19: Aborted (core dumped)
srun: error: irene1445: tasks 12-13: Aborted (core dumped)
srun: error: irene1380: tasks 2-3: Aborted (core dumped)
srun: error: irene1517: tasks 30-31: Aborted (core dumped)
srun: error: irene1447: task 17: Aborted (core dumped)
srun: error: irene1497: task 24: Aborted (core dumped)
srun: error: irene1303: task 1: Aborted (core dumped)
srun: error: irene1444: task 11: Aborted (core dumped)
srun: error: irene1391: task 5: Aborted (core dumped)
srun: error: irene1512: task 29: Aborted (core dumped)
srun: error: irene1496: task 22: Aborted (core dumped)
