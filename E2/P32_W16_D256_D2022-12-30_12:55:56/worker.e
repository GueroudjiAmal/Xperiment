distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.215:40836'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.215:43417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.207:46565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.207:46112'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.203:43774'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.203:39999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.204:34364'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.204:43006'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.209:36223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.209:41070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.213:39209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.213:42028'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.208:37251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.205:34206'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.208:44132'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.205:35574'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.230:46376'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.230:42377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.228:39534'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.228:35237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.211:34335'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.211:35007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.227:43086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.212:34613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.212:40448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.229:37834'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.229:36849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.210:38421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.210:43635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.214:46347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.214:35710'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.227:37354'
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.205:46491
distributed.worker - INFO -          Listening to:  tcp://172.21.10.205:46491
distributed.worker - INFO -          dashboard at:        172.21.10.205:34999
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.205:37403
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.10.205:37403
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-azznr5fm
distributed.worker - INFO -          dashboard at:        172.21.10.205:38185
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.209:34903
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.209:39219
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6vc1obwo
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.215:42623
distributed.worker - INFO -          Listening to:  tcp://172.21.10.209:34903
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.215:42623
distributed.worker - INFO -          dashboard at:        172.21.10.209:38686
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.215:39971
distributed.worker - INFO -          Listening to:  tcp://172.21.10.209:39219
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.215:38685
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.209:41199
distributed.worker - INFO -          Listening to:  tcp://172.21.10.215:38685
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -          dashboard at:        172.21.10.215:45531
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1otl6ufn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tfzkn25u
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.207:46260
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zbmuhl0c
distributed.worker - INFO -          Listening to:  tcp://172.21.10.207:46260
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.203:45507
distributed.worker - INFO -          dashboard at:        172.21.10.207:45123
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ospwy_ev
distributed.worker - INFO -          Listening to:  tcp://172.21.10.203:45507
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.207:34391
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.203:43225
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.203:39438
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -          Listening to:  tcp://172.21.10.207:34391
distributed.worker - INFO -          Listening to:  tcp://172.21.10.203:39438
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.207:44951
distributed.worker - INFO -          dashboard at:        172.21.10.203:34865
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.230:41004
distributed.worker - INFO -          Listening to:  tcp://172.21.10.230:41004
distributed.worker - INFO -          dashboard at:        172.21.10.230:44899
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ajst1jfl
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.230:40673
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gqkyjx8g
distributed.worker - INFO -          Listening to:  tcp://172.21.10.230:40673
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_wae68w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.230:42736
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_0y17apw
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-69wbcow9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mjoox57s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.204:35477
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.208:34634
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.204:35580
distributed.worker - INFO -          Listening to:  tcp://172.21.10.208:34634
distributed.worker - INFO -          Listening to:  tcp://172.21.10.204:35580
distributed.worker - INFO -          dashboard at:        172.21.10.208:42904
distributed.worker - INFO -          dashboard at:        172.21.10.204:46787
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.208:43455
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d1734fzd
distributed.worker - INFO -          Listening to:  tcp://172.21.10.208:43455
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.208:35084
distributed.worker - INFO -          Listening to:  tcp://172.21.10.204:35477
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -          dashboard at:        172.21.10.204:42149
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3cbq5zpn
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2qaecehg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kwuiy_nd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.213:40354
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.213:45356
distributed.worker - INFO -          Listening to:  tcp://172.21.10.213:40354
distributed.worker - INFO -          Listening to:  tcp://172.21.10.213:45356
distributed.worker - INFO -          dashboard at:        172.21.10.213:41599
distributed.worker - INFO -          dashboard at:        172.21.10.213:45243
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.228:40735
distributed.worker - INFO -          Listening to:  tcp://172.21.10.228:40735
distributed.worker - INFO -          dashboard at:        172.21.10.228:36603
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.228:40394
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:  tcp://172.21.10.228:40394
distributed.worker - INFO -          dashboard at:        172.21.10.228:40532
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fvzdiwyn
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qkf_6qze
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jixbrilk
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-50jevvg5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.212:35915
distributed.worker - INFO -          Listening to:  tcp://172.21.10.212:35915
distributed.worker - INFO -          dashboard at:        172.21.10.212:44807
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o8m8u3vn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.214:43321
distributed.worker - INFO -          Listening to:  tcp://172.21.10.214:43321
distributed.worker - INFO -          dashboard at:        172.21.10.214:35668
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.214:32876
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.10.214:32876
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.214:44374
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_eqaq0px
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.211:44202
distributed.worker - INFO -          Listening to:  tcp://172.21.10.211:44202
distributed.worker - INFO -          dashboard at:        172.21.10.211:40226
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.227:38913
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.227:38913
distributed.worker - INFO -          dashboard at:        172.21.10.227:39378
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.227:44990
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.211:42135
distributed.worker - INFO -          Listening to:  tcp://172.21.10.227:44990
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.212:44966
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.212:44966
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mtxxv_3f
distributed.worker - INFO -          dashboard at:        172.21.10.227:39306
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -          dashboard at:        172.21.10.212:41219
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-81jflfbd
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.210:46747
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t298mmvn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6kuvxy7q
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.210:38767
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0zlvy6b3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.211:42135
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:        172.21.10.211:45077
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:  tcp://172.21.10.210:46747
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.210:42169
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-25ewl657
distributed.worker - INFO -          Listening to:  tcp://172.21.10.210:38767
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -          dashboard at:        172.21.10.210:46273
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5yosqf21
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hnhrb1u7
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.229:38204
distributed.worker - INFO -          Listening to:  tcp://172.21.10.229:38204
distributed.worker - INFO -          dashboard at:        172.21.10.229:43207
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO -       Start worker at:  tcp://172.21.10.229:45023
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.10.229:45023
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.10.229:41966
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9tv2hpy4
distributed.worker - INFO - Waiting to connect to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iwijrlc9
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.21.10.201:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 18.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.205:46491
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.212:35915
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.205:46491 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.205:46491
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.227:38913
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.212:35915 after 30 s
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.205:46491 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.210:38767
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.227:38913 after 30 s
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.210:38767 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.212:35915
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.212:35915 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.227:38913
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.212:44966
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.227:38913 after 30 s
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.212:44966 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.212:44966
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.212:44966 after 30 s
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.208:34634
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.208:34634
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.208:34634 after 30 s
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.208:34634 after 30 s
distributed.core - INFO - Event loop was unresponsive in Worker for 102.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:47700 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.207:36098 closed before handshake completed
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.210:38767
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.205:37403
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.210:38767 after 30 s
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.205:37403 after 30 s
distributed.core - INFO - Event loop was unresponsive in Worker for 137.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:41788 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 141.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.207:51756 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.215:37964 closed before handshake completed
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.10.212:35915
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 319, in connect
    handshake = await asyncio.wait_for(comm.read(), time_left())
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4120, in _get_data
    comm = await rpc.connect(worker)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1081, in connect
    raise exc
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 1065, in connect
    comm = await fut
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 324, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://172.21.10.212:35915 after 30 s
distributed.core - INFO - Event loop was unresponsive in Worker for 34.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 71.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 163.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.228:47438 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.215:38770 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 170.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 173.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.207:40476 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:46486 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 177.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:52050 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.215:46764 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:52076 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 232.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.213:34094 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.10.207:37538 closed before handshake completed
distributed.worker - INFO - Stopping worker at tcp://172.21.10.203:39438
distributed.worker - INFO - Stopping worker at tcp://172.21.10.203:45507
distributed.worker - INFO - Stopping worker at tcp://172.21.10.204:35580
distributed.worker - INFO - Stopping worker at tcp://172.21.10.204:35477
distributed.worker - INFO - Stopping worker at tcp://172.21.10.208:43455
distributed.worker - INFO - Stopping worker at tcp://172.21.10.207:34391
distributed.worker - INFO - Stopping worker at tcp://172.21.10.208:34634
distributed.worker - INFO - Stopping worker at tcp://172.21.10.207:46260
distributed.worker - INFO - Stopping worker at tcp://172.21.10.205:37403
distributed.worker - INFO - Stopping worker at tcp://172.21.10.209:34903
distributed.worker - INFO - Stopping worker at tcp://172.21.10.209:39219
distributed.worker - INFO - Stopping worker at tcp://172.21.10.210:38767
distributed.worker - INFO - Stopping worker at tcp://172.21.10.210:46747
distributed.worker - INFO - Stopping worker at tcp://172.21.10.205:46491
distributed.worker - INFO - Stopping worker at tcp://172.21.10.211:42135
distributed.worker - INFO - Stopping worker at tcp://172.21.10.214:32876
distributed.worker - INFO - Stopping worker at tcp://172.21.10.213:45356
distributed.worker - INFO - Stopping worker at tcp://172.21.10.211:44202
distributed.worker - INFO - Stopping worker at tcp://172.21.10.212:35915
distributed.worker - INFO - Stopping worker at tcp://172.21.10.214:43321
distributed.worker - INFO - Stopping worker at tcp://172.21.10.213:40354
distributed.worker - INFO - Stopping worker at tcp://172.21.10.215:42623
distributed.worker - INFO - Stopping worker at tcp://172.21.10.212:44966
distributed.worker - INFO - Stopping worker at tcp://172.21.10.215:38685
distributed.worker - INFO - Stopping worker at tcp://172.21.10.227:38913
distributed.worker - INFO - Stopping worker at tcp://172.21.10.227:44990
distributed.worker - INFO - Stopping worker at tcp://172.21.10.228:40394
distributed.worker - INFO - Stopping worker at tcp://172.21.10.228:40735
distributed.worker - INFO - Stopping worker at tcp://172.21.10.229:38204
distributed.worker - INFO - Stopping worker at tcp://172.21.10.230:41004
distributed.worker - INFO - Stopping worker at tcp://172.21.10.230:40673
distributed.worker - INFO - Stopping worker at tcp://172.21.10.229:45023
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.203:43774'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.210:43635'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.203:39999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.209:36223'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.209:41070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.210:38421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.227:43086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.204:34364'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.213:42028'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.207:46112'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.204:43006'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.207:46565'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.215:43417'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.213:39209'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.215:40836'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.227:37354'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.212:40448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.212:34613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.214:46347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.214:35710'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.208:37251'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.208:44132'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.211:34335'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.205:34206'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.211:35007'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.205:35574'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.230:42377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.228:39534'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.228:35237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.230:46376'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.229:36849'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.10.229:37834'
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Worker closed
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.204:34364'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.211:34335'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.215:40836'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.207:46112'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.203:39999'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.208:44132'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.204:43006'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.229:36849'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.203:43774'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.211:35007'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.215:43417'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.209:41070'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.207:46565'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.210:43635'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.208:37251'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.212:40448'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.227:37354'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.228:39534'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.230:46376'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.205:34206'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.229:37834'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.209:36223'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.213:39209'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.210:38421'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.227:43086'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.212:34613'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.230:42377'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.228:35237'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.214:35710'
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.205:35574'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.213:42028'
distributed.dask_worker - INFO - End worker
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.10.214:46347'
distributed.dask_worker - INFO - End worker
