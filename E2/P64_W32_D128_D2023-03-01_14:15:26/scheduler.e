distributed.scheduler - INFO - -----------------------------------------------
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - -----------------------------------------------
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:   tcp://172.21.0.252:8786
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.148:34064', name: tcp://172.21.3.148:34064, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.148:34064
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.148:39176', name: tcp://172.21.3.148:39176, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.148:39176
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.61:46762', name: tcp://172.21.1.61:46762, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.61:46762
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.61:39890', name: tcp://172.21.1.61:39890, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.61:39890
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.41:39394', name: tcp://172.21.1.41:39394, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.41:39394
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.41:38809', name: tcp://172.21.1.41:38809, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.41:38809
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.55:44975', name: tcp://172.21.1.55:44975, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.55:44975
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.55:40717', name: tcp://172.21.1.55:40717, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.55:40717
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.45:38187', name: tcp://172.21.1.45:38187, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.45:38187
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.45:34291', name: tcp://172.21.1.45:34291, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.45:34291
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.149:44252', name: tcp://172.21.3.149:44252, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.149:44252
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.149:44927', name: tcp://172.21.3.149:44927, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.149:44927
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.150:36135', name: tcp://172.21.3.150:36135, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.150:36135
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.150:34375', name: tcp://172.21.3.150:34375, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.150:34375
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.110:33630', name: tcp://172.21.3.110:33630, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.110:33630
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.110:45125', name: tcp://172.21.3.110:45125, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.110:45125
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.130:38633', name: tcp://172.21.3.130:38633, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.130:38633
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.146:32896', name: tcp://172.21.3.146:32896, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.146:32896
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.130:40159', name: tcp://172.21.3.130:40159, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.130:40159
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.108:46514', name: tcp://172.21.3.108:46514, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.108:46514
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.108:33808', name: tcp://172.21.3.108:33808, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.108:33808
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.146:46026', name: tcp://172.21.3.146:46026, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.146:46026
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.29:37517', name: tcp://172.21.1.29:37517, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.29:37517
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.1.29:36730', name: tcp://172.21.1.29:36730, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.1.29:36730
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.84:45049', name: tcp://172.21.3.84:45049, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.84:45049
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.84:41919', name: tcp://172.21.3.84:41919, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.84:41919
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.144:46109', name: tcp://172.21.3.144:46109, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.144:46109
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.144:46340', name: tcp://172.21.3.144:46340, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.144:46340
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.127:34491', name: tcp://172.21.3.127:34491, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.127:34491
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.127:46257', name: tcp://172.21.3.127:46257, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.127:46257
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.78:33316', name: tcp://172.21.3.78:33316, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.78:33316
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.21.3.78:40060', name: tcp://172.21.3.78:40060, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.21.3.78:40060
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc51d7de-b84f-11ed-bf61-080038b549b9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc51cfa5-b84f-11ed-bf60-080038b549b9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52095a-b84f-11ed-8dd2-080038b54496
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc520408-b84f-11ed-8dd3-080038b54496
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc522842-b84f-11ed-a0b0-080038b54d6f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc522da6-b84f-11ed-a0af-080038b54d6f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc527c23-b84f-11ed-af7d-080038b53a78
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc527699-b84f-11ed-af7c-080038b53a78
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52a835-b84f-11ed-b6e0-080038b547b6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52a382-b84f-11ed-b6df-080038b547b6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc521868-b84f-11ed-aa00-080038b50614
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc521ea4-b84f-11ed-a9ff-080038b50614
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5235cf-b84f-11ed-9c8a-080038b512f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52307e-b84f-11ed-9c89-080038b512f8
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52fb32-b84f-11ed-b880-080038b514f1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc530316-b84f-11ed-b881-080038b514f1
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc532dff-b84f-11ed-9f0d-080038b512ee
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53182f-b84f-11ed-8d0b-080038b516e0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53328b-b84f-11ed-9f0c-080038b512ee
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5313e8-b84f-11ed-8d0c-080038b516e0
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc537421-b84f-11ed-a29b-080038b512c6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5339b4-b84f-11ed-a29a-080038b512c6
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc535d23-b84f-11ed-ac2c-080038b544d7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc539b14-b84f-11ed-8a49-080038b53eb5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52651b-b84f-11ed-b09e-080038b5476b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5396b7-b84f-11ed-8a48-080038b53eb5
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc538576-b84f-11ed-b86f-080038b54766
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc526b34-b84f-11ed-b09f-080038b5476b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53818c-b84f-11ed-b870-080038b54766
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc540290-b84f-11ed-8460-080038b50097
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc545586-b84f-11ed-954b-080038b54d24
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53d5e1-b84f-11ed-96dc-080038b5121c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53e0e9-b84f-11ed-96db-080038b5121c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54070b-b84f-11ed-8461-080038b50097
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc545a62-b84f-11ed-954c-080038b54d24
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc545e02-b84f-11ed-a394-080038b5538c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54999f-b84f-11ed-ac2d-080038b544d7
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5401b0-b84f-11ed-a393-080038b5538c
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52a3f4-b84f-11ed-b147-080038b51695
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc52aa94-b84f-11ed-b148-080038b51695
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53ee71-b84f-11ed-ad72-080038b562b4
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53ea28-b84f-11ed-ad71-080038b562b4
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc546da9-b84f-11ed-973e-080038b54d92
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc547c79-b84f-11ed-99da-080038b5157d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc547857-b84f-11ed-99db-080038b5157d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc5471d5-b84f-11ed-973f-080038b54d92
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53dad5-b84f-11ed-a3c3-080038b51307
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc53dfa6-b84f-11ed-a3c4-080038b51307
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc549143-b84f-11ed-a3cd-080038b53a7d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc548c9c-b84f-11ed-a3ce-080038b53a7d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54cecb-b84f-11ed-9bed-080038b54f18
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54301c-b84f-11ed-8d5e-080038b53f5f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54cabf-b84f-11ed-9bee-080038b54f18
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54ad7b-b84f-11ed-8d4a-080038b54f1d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc543cd1-b84f-11ed-b6b4-080038b51753
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc543851-b84f-11ed-b6b3-080038b51753
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc543c37-b84f-11ed-8d5f-080038b53f5f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54a7ea-b84f-11ed-8d49-080038b54f1d
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54d463-b84f-11ed-aca3-080038b53b8b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54c793-b84f-11ed-aca2-080038b53b8b
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54102e-b84f-11ed-8f0b-080038b510b9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54089d-b84f-11ed-8f0a-080038b510b9
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54b9ac-b84f-11ed-ad19-080038b53a0f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc54ade6-b84f-11ed-ad18-080038b53a0f
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Receive client connection: Client-fc7694d3-b84f-11ed-b467-080038b53bc7
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'lambda'
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO - Clear task state
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2a3bca3a60>, <Task finished name='Task-4724' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2a3bca3af0>, <Task finished name='Task-4725' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x2b2a3bca3c10>, <Task finished name='Task-4726' coro=<BaseTCPListener._handle_stream() done, defined at /ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py:502> exception=OSError(98, 'Address already in use')>)
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 519, in _handle_stream
    await self.comm_handler(comm)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 449, in handle_comm
    await self
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 275, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/scheduler.py", line 3988, in start
    await self.listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 421, in listen
    listener = await listen(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/core.py", line 207, in _
    await self.start()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 477, in start
    sockets = netutil.bind_sockets(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/tornado/netutil.py", line 161, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.41:38809', name: tcp://172.21.1.41:38809, status: closing, memory: 23, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.41:38809
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.110:45125', name: tcp://172.21.3.110:45125, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.110:45125
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.110:33630', name: tcp://172.21.3.110:33630, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.110:33630
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.78:40060', name: tcp://172.21.3.78:40060, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.78:40060
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.84:45049', name: tcp://172.21.3.84:45049, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.84:45049
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.29:36730', name: tcp://172.21.1.29:36730, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.29:36730
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.29:37517', name: tcp://172.21.1.29:37517, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.29:37517
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.127:46257', name: tcp://172.21.3.127:46257, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.127:46257
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.144:46109', name: tcp://172.21.3.144:46109, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.144:46109
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.150:36135', name: tcp://172.21.3.150:36135, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.150:36135
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.146:32896', name: tcp://172.21.3.146:32896, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.146:32896
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.149:44252', name: tcp://172.21.3.149:44252, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.149:44252
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.45:38187', name: tcp://172.21.1.45:38187, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.45:38187
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.130:40159', name: tcp://172.21.3.130:40159, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.130:40159
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.55:40717', name: tcp://172.21.1.55:40717, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.55:40717
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.55:44975', name: tcp://172.21.1.55:44975, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.55:44975
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.127:34491', name: tcp://172.21.3.127:34491, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.127:34491
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.108:46514', name: tcp://172.21.3.108:46514, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.108:46514
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.61:46762', name: tcp://172.21.1.61:46762, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.61:46762
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.61:39890', name: tcp://172.21.1.61:39890, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.61:39890
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.148:34064', name: tcp://172.21.3.148:34064, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.148:34064
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.146:46026', name: tcp://172.21.3.146:46026, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.146:46026
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.149:44927', name: tcp://172.21.3.149:44927, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.149:44927
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.41:39394', name: tcp://172.21.1.41:39394, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.41:39394
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.150:34375', name: tcp://172.21.3.150:34375, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.150:34375
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.108:33808', name: tcp://172.21.3.108:33808, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.108:33808
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.84:41919', name: tcp://172.21.3.84:41919, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.84:41919
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.144:46340', name: tcp://172.21.3.144:46340, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.144:46340
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.78:33316', name: tcp://172.21.3.78:33316, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.78:33316
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.148:39176', name: tcp://172.21.3.148:39176, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.148:39176
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.3.130:38633', name: tcp://172.21.3.130:38633, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.3.130:38633
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.21.1.45:34291', name: tcp://172.21.1.45:34291, status: closing, memory: 22, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.21.1.45:34291
distributed.scheduler - INFO - Lost all workers
distributed.scheduler - INFO - Scheduler closing all comms
distributed.scheduler - INFO - End scheduler at 'tcp://172.21.0.252:8786'
