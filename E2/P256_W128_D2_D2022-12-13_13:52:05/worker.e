distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.100:43496'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.100:44498'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.158:43322'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.126:34489'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.158:42707'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.126:42937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.93:36461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.70:40123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.136:43734'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.133:44113'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.70:43184'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.93:41529'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.92:42375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.136:45710'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.24:33221'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.24:34595'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.233:42701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.233:37038'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.98:39101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.92:34817'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.98:44934'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.133:46340'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.245:45937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.245:34698'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.40:33777'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.40:40031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.237:43388'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.237:41128'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.239:35219'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.239:35268'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.83:34643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.159:38568'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.83:33311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.79:33800'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.159:46331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.235:39177'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.235:45099'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.79:38088'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.94:35164'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.38:40376'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.23:43459'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.23:40032'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.38:38525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.94:37969'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.80:37921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.80:40846'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.78:41804'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.189:39763'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.189:41269'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.77:41165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.46:42430'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.46:33015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.77:34162'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.137:38368'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.49:38740'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.22:38139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.22:40125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.137:33533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.160:45659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.182:36725'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.49:43368'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.160:33513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.182:44656'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.86:46053'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.86:38990'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.105:43630'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.105:37915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.109:45065'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.74:35982'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.246:39121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.138:36492'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.99:36421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.246:34995'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.99:38535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.74:40895'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.138:45561'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.200:34816'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.132:33232'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.132:44171'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.103:36402'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.200:44233'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.90:33023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.90:34844'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.103:45023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.89:41525'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.157:46467'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.41:40160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.39:41702'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.240:44681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.41:46026'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.48:45490'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.157:33349'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.48:39727'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.82:43496'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.240:40748'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.154:36138'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.39:32962'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.95:38597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.89:46013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.242:37031'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.154:37981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.82:40102'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.102:46513'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.242:36144'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.95:38317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.232:33808'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.232:42833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.97:43469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.102:35801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.76:38114'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.97:46228'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.76:34841'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.75:33988'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.75:45334'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.241:43136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.199:33850'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.241:36581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.199:44448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.243:33284'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.243:38907'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.81:44804'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.81:42709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.123:44620'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.130:41716'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.123:33607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.130:46567'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.78:40652'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.109:46328'
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.100:43784
distributed.worker - INFO -          Listening to:   tcp://172.21.1.100:43784
distributed.worker - INFO -          dashboard at:         172.21.1.100:41721
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_5x7f959
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.98:44049
distributed.worker - INFO -          Listening to:    tcp://172.21.1.98:44049
distributed.worker - INFO -          dashboard at:          172.21.1.98:46144
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.100:34877
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.80:40710
distributed.worker - INFO -          Listening to:    tcp://172.21.1.80:40710
distributed.worker - INFO -          dashboard at:          172.21.1.80:35783
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.240:37275
distributed.worker - INFO -          Listening to:   tcp://172.21.0.240:37275
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.160:44309
distributed.worker - INFO -          Listening to:   tcp://172.21.1.160:44309
distributed.worker - INFO -          dashboard at:         172.21.1.160:36897
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.237:43638
distributed.worker - INFO -          Listening to:   tcp://172.21.0.237:43638
distributed.worker - INFO -          dashboard at:         172.21.0.237:40030
distributed.worker - INFO -          Listening to:   tcp://172.21.1.100:34877
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.182:45758
distributed.worker - INFO -          Listening to:   tcp://172.21.0.182:45758
distributed.worker - INFO -          dashboard at:         172.21.0.240:38889
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.86:34266
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.100:35927
distributed.worker - INFO -          dashboard at:         172.21.0.182:42434
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.78:42182
distributed.worker - INFO -          Listening to:    tcp://172.21.1.78:42182
distributed.worker - INFO -          dashboard at:          172.21.1.78:34188
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.81:34812
distributed.worker - INFO -          Listening to:    tcp://172.21.1.81:34812
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.77:38815
distributed.worker - INFO -          Listening to:    tcp://172.21.1.77:38815
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.189:40167
distributed.worker - INFO -          Listening to:   tcp://172.21.0.189:40167
distributed.worker - INFO -          Listening to:    tcp://172.21.1.86:34266
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.200:42138
distributed.worker - INFO -          Listening to:   tcp://172.21.0.200:42138
distributed.worker - INFO -          dashboard at:         172.21.0.200:32774
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.90:42304
distributed.worker - INFO -          Listening to:    tcp://172.21.1.90:42304
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.40:37595
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.94:34122
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.235:35843
distributed.worker - INFO -          Listening to:   tcp://172.21.0.235:35843
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.99:45119
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.76:37887
distributed.worker - INFO -          Listening to:    tcp://172.21.1.76:37887
distributed.worker - INFO -          dashboard at:          172.21.1.76:40165
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.154:46634
distributed.worker - INFO -          Listening to:   tcp://172.21.1.154:46634
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.22:38391
distributed.worker - INFO -          Listening to:    tcp://172.21.1.22:38391
distributed.worker - INFO -          dashboard at:          172.21.1.22:37113
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.130:35400
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.159:40733
distributed.worker - INFO -          Listening to:   tcp://172.21.1.159:40733
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.157:38641
distributed.worker - INFO -          Listening to:   tcp://172.21.1.157:38641
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.102:42325
distributed.worker - INFO -          Listening to:   tcp://172.21.1.102:42325
distributed.worker - INFO -          dashboard at:         172.21.1.102:43050
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.46:44063
distributed.worker - INFO -          Listening to:    tcp://172.21.1.46:44063
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.74:45793
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6uw3qg0c
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.239:41257
distributed.worker - INFO -          Listening to:   tcp://172.21.0.239:41257
distributed.worker - INFO -          dashboard at:         172.21.0.239:42336
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.93:38116
distributed.worker - INFO -          Listening to:    tcp://172.21.1.93:38116
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.199:33967
distributed.worker - INFO -          Listening to:   tcp://172.21.0.199:33967
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.132:38957
distributed.worker - INFO -          Listening to:   tcp://172.21.1.132:38957
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.70:43216
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.75:33059
distributed.worker - INFO -          Listening to:    tcp://172.21.1.75:33059
distributed.worker - INFO -          dashboard at:          172.21.1.75:44022
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.48:36206
distributed.worker - INFO -          Listening to:    tcp://172.21.1.48:36206
distributed.worker - INFO -          dashboard at:          172.21.1.48:36275
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.245:40890
distributed.worker - INFO -          Listening to:   tcp://172.21.0.245:40890
distributed.worker - INFO -          dashboard at:         172.21.0.245:46607
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.242:34170
distributed.worker - INFO -          Listening to:   tcp://172.21.0.242:34170
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.97:33788
distributed.worker - INFO -          Listening to:    tcp://172.21.1.97:33788
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.95:42698
distributed.worker - INFO -          Listening to:    tcp://172.21.1.95:42698
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.138:45242
distributed.worker - INFO -          Listening to:   tcp://172.21.1.138:45242
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.136:41612
distributed.worker - INFO -          Listening to:   tcp://172.21.1.136:41612
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.243:39586
distributed.worker - INFO -          Listening to:   tcp://172.21.0.243:39586
distributed.worker - INFO -          dashboard at:         172.21.0.243:43030
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.158:45083
distributed.worker - INFO -          Listening to:   tcp://172.21.1.158:45083
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.83:34477
distributed.worker - INFO -          Listening to:    tcp://172.21.1.83:34477
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.105:45491
distributed.worker - INFO -          Listening to:   tcp://172.21.1.105:45491
distributed.worker - INFO -          dashboard at:         172.21.1.105:41451
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.241:37509
distributed.worker - INFO -          Listening to:   tcp://172.21.0.241:37509
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.39:40037
distributed.worker - INFO -          Listening to:    tcp://172.21.1.39:40037
distributed.worker - INFO -          dashboard at:          172.21.1.39:44970
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.126:37012
distributed.worker - INFO -          Listening to:   tcp://172.21.1.126:37012
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.133:43166
distributed.worker - INFO -          Listening to:   tcp://172.21.1.133:43166
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.123:36102
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.232:33596
distributed.worker - INFO -          Listening to:   tcp://172.21.0.232:33596
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.24:38007
distributed.worker - INFO -          Listening to:    tcp://172.21.1.24:38007
distributed.worker - INFO -          dashboard at:          172.21.1.24:45470
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.78:43745
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.109:46714
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:46405
distributed.worker - INFO -          dashboard at:          172.21.1.77:45243
distributed.worker - INFO -          dashboard at:          172.21.1.86:46085
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.90:43418
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.40:37595
distributed.worker - INFO -          Listening to:    tcp://172.21.1.94:34122
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.235:34576
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.99:45119
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.154:41794
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.130:35400
distributed.worker - INFO -          dashboard at:         172.21.1.159:42635
distributed.worker - INFO -          dashboard at:         172.21.1.157:45052
distributed.worker - INFO -          dashboard at:          172.21.1.46:46189
distributed.worker - INFO -          Listening to:    tcp://172.21.1.74:45793
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.38:40399
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.0.199:37454
distributed.worker - INFO -          dashboard at:         172.21.1.132:45930
distributed.worker - INFO -          Listening to:    tcp://172.21.1.70:43216
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.75:42269
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.0.242:38216
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.97:42261
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.138:35618
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.136:46362
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.1.158:43586
distributed.worker - INFO -          dashboard at:          172.21.1.83:36722
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.0.241:40228
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.39:39746
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.49:34169
distributed.worker - INFO -          Listening to:    tcp://172.21.1.49:34169
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.233:38145
distributed.worker - INFO -          dashboard at:         172.21.1.126:44688
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.123:36102
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.232:38870
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.24:36414
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.109:46714
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.103:39063
distributed.worker - INFO -          Listening to:   tcp://172.21.1.103:39063
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.40:39360
distributed.worker - INFO -          dashboard at:          172.21.1.94:35158
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4zhjjchn
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.235:39145
distributed.worker - INFO -          Listening to:   tcp://172.21.0.235:39145
distributed.worker - INFO -          dashboard at:          172.21.1.99:34954
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jf4ewtid
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.130:40687
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.159:34314
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.74:45933
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.98:34696
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.93:42588
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.132:42361
distributed.worker - INFO -          Listening to:   tcp://172.21.1.132:42361
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.23:35809
distributed.worker - INFO -          Listening to:    tcp://172.21.1.23:35809
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.137:32882
distributed.worker - INFO -          Listening to:   tcp://172.21.1.137:32882
distributed.worker - INFO -          dashboard at:         172.21.1.137:41399
distributed.worker - INFO -          dashboard at:          172.21.1.70:37222
distributed.worker - INFO -          Listening to:    tcp://172.21.1.75:42269
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.89:41840
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.242:38022
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.95:38742
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.158:41064
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.79:41456
distributed.worker - INFO -          Listening to:    tcp://172.21.1.79:41456
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.133:45160
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.123:42800
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ac5ch5bu
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.109:45863
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:46405
distributed.worker - INFO -          dashboard at:          172.21.1.81:34829
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.189:33568
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.40:35189
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.246:40919
distributed.worker - INFO -          Listening to:   tcp://172.21.0.246:40919
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.82:34114
distributed.worker - INFO -          Listening to:    tcp://172.21.1.82:34114
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.46:42009
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.74:44536
distributed.worker - INFO -          Listening to:    tcp://172.21.1.98:34696
distributed.worker - INFO -          Listening to:    tcp://172.21.1.38:40399
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1vi0rt6r
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rmc89yo6
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.24:36414
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.109:37490
distributed.worker - INFO -          dashboard at:         172.21.1.103:46784
distributed.worker - INFO -          dashboard at:          172.21.1.92:41597
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.189:37053
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.160:43742
distributed.worker - INFO -          Listening to:   tcp://172.21.1.160:43742
distributed.worker - INFO -          dashboard at:         172.21.0.235:43791
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.80:39616
distributed.worker - INFO -          Listening to:    tcp://172.21.1.80:39616
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.76:34455
distributed.worker - INFO -          Listening to:    tcp://172.21.1.76:34455
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.74:44536
distributed.worker - INFO -          dashboard at:          172.21.1.98:36585
distributed.worker - INFO -          dashboard at:          172.21.1.38:38738
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yq06vjxc
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.1.132:42516
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.41:39141
distributed.worker - INFO -          Listening to:    tcp://172.21.1.41:39141
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.75:38921
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.0.242:38022
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-znb59e6i
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.1.158:41064
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mhwn58fw
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.79:45671
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.49:40704
distributed.worker - INFO -          Listening to:    tcp://172.21.1.49:40704
distributed.worker - INFO -          dashboard at:          172.21.1.49:34055
distributed.worker - INFO -          Listening to:   tcp://172.21.0.233:38145
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.123:43868
distributed.worker - INFO -          Listening to:   tcp://172.21.1.123:43868
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.240:38514
distributed.worker - INFO -          Listening to:   tcp://172.21.0.240:38514
distributed.worker - INFO -          dashboard at:         172.21.0.240:36933
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.78:43745
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.86:39629
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.200:37077
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hs8wz56i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.1.160:44984
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.80:46420
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gdtwodjb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ev97pbp
distributed.worker - INFO -          dashboard at:         172.21.0.246:44258
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.22:40440
distributed.worker - INFO -          Listening to:    tcp://172.21.1.22:40440
distributed.worker - INFO -          dashboard at:          172.21.1.22:35986
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.1.159:34314
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.82:39004
distributed.worker - INFO -          Listening to:    tcp://172.21.1.82:39004
distributed.worker - INFO -          dashboard at:          172.21.1.82:45003
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x2pkhynk
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_hab1u_h
distributed.worker - INFO -          Listening to:    tcp://172.21.1.46:42009
distributed.worker - INFO -          dashboard at:          172.21.1.46:38041
distributed.worker - INFO -          dashboard at:          172.21.1.74:33518
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.93:35579
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.23:36837
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lpc2efrg
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.89:39527
distributed.worker - INFO -          Listening to:    tcp://172.21.1.89:39527
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mo6mx8r0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cy513m5c
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.237:37728
distributed.worker - INFO -          Listening to:   tcp://172.21.0.237:37728
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5a61x13y
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.138:37008
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z355ldxs
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.182:42900
distributed.worker - INFO -          Listening to:   tcp://172.21.0.182:42900
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zc50ied3
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.39:39746
distributed.worker - INFO -          dashboard at:          172.21.1.49:45747
distributed.worker - INFO -          dashboard at:         172.21.0.233:36162
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rqjsjtc3
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.232:42465
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i8j87men
distributed.worker - INFO -          Listening to:   tcp://172.21.0.189:37053
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-psc7xtu6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yrqybakn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.40:35189
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.94:33605
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.76:45867
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.154:34294
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_c1ovbmy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.82:33095
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.239:38304
distributed.worker - INFO -          Listening to:    tcp://172.21.1.93:35579
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4fkya3ib
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.137:36063
distributed.worker - INFO -          dashboard at:          172.21.1.41:38988
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.89:41840
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nsp9nwot
distributed.worker - INFO -          dashboard at:         172.21.0.237:35589
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-020jj0ih
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wdlwcyxz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.182:38236
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qcu9ic13
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.105:32772
distributed.worker - INFO -          Listening to:   tcp://172.21.1.105:32772
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.39:42197
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.133:42347
distributed.worker - INFO -          dashboard at:         172.21.1.123:42878
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q4dvzdvj
distributed.worker - INFO -          dashboard at:          172.21.1.78:33695
distributed.worker - INFO -          Listening to:   tcp://172.21.1.109:37490
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.1.86:39629
distributed.worker - INFO -          dashboard at:          172.21.1.86:45796
distributed.worker - INFO -          Listening to:   tcp://172.21.0.200:37077
distributed.worker - INFO -          dashboard at:         172.21.0.200:35566
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.90:43067
distributed.worker - INFO -          Listening to:    tcp://172.21.1.90:43067
distributed.worker - INFO -          dashboard at:          172.21.1.90:41099
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6fgh_df9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y4hlie5b
distributed.worker - INFO -          dashboard at:         172.21.1.159:36995
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.157:33703
distributed.worker - INFO -          Listening to:   tcp://172.21.1.157:33703
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.102:37283
distributed.worker - INFO -          Listening to:   tcp://172.21.1.102:37283
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.0.239:38304
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.199:40796
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-46cn1bk_
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-azap7hmg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.89:36264
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.48:35880
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.245:38920
distributed.worker - INFO -          dashboard at:         172.21.0.242:34158
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.97:35827
distributed.worker - INFO -          Listening to:    tcp://172.21.1.97:35827
distributed.worker - INFO -          dashboard at:          172.21.1.97:34831
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.1.138:37008
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.136:36955
distributed.worker - INFO -          Listening to:   tcp://172.21.1.136:36955
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xza5iqxa
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.158:40319
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.83:44338
distributed.worker - INFO -          Listening to:    tcp://172.21.1.83:44338
distributed.worker - INFO -          dashboard at:          172.21.1.83:34554
distributed.worker - INFO -          dashboard at:         172.21.1.105:39586
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.241:35264
distributed.worker - INFO -          Listening to:   tcp://172.21.0.241:35264
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.126:36156
distributed.worker - INFO -          Listening to:   tcp://172.21.1.126:36156
distributed.worker - INFO -          dashboard at:         172.21.1.126:42114
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nj6x8t4a
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p9k84by_
distributed.worker - INFO -          dashboard at:          172.21.1.24:34219
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e4r3gh71
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gbwwcr24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.77:45085
distributed.worker - INFO -          Listening to:    tcp://172.21.1.77:45085
distributed.worker - INFO -          dashboard at:          172.21.1.77:33092
distributed.worker - INFO -          dashboard at:         172.21.0.189:35835
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aidrwk3a
distributed.worker - INFO -          Listening to:    tcp://172.21.1.94:33605
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.1.154:34294
distributed.worker - INFO -          dashboard at:         172.21.1.154:38412
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.157:43954
distributed.worker - INFO -          dashboard at:         172.21.1.102:42992
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2ygkizm_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.239:37061
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8_hmo42z
distributed.worker - INFO -          Listening to:   tcp://172.21.0.199:40796
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1sp623co
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.1.137:36063
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.89:41562
distributed.worker - INFO -          Listening to:    tcp://172.21.1.48:35880
distributed.worker - INFO -          Listening to:   tcp://172.21.0.245:38920
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.95:45368
distributed.worker - INFO -          Listening to:    tcp://172.21.1.95:45368
distributed.worker - INFO -          dashboard at:          172.21.1.95:33223
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.136:40230
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-88cann9l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.0.241:38878
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kjv_cfpr
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rknu0j1_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.1.133:42347
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e39him93
distributed.worker - INFO -          Listening to:   tcp://172.21.0.232:42465
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mqozzisx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hp1hjs2w
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.40:37394
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t8y6kn4f
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uv40dt_t
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.99:36719
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pnm2fs4t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.130:37698
distributed.worker - INFO -          Listening to:   tcp://172.21.1.130:37698
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cehzw2a3
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-629n1u_q
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.93:42691
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-61v23avy
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.41:36412
distributed.worker - INFO -          Listening to:    tcp://172.21.1.41:36412
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.70:38373
distributed.worker - INFO -          Listening to:    tcp://172.21.1.70:38373
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.48:39669
distributed.worker - INFO -          dashboard at:         172.21.0.245:32902
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.138:38044
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.243:35659
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.133:46332
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ofac5wwx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.1.109:33323
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.103:41439
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.92:40529
distributed.worker - INFO -          Listening to:    tcp://172.21.1.92:40529
distributed.worker - INFO -          dashboard at:          172.21.1.92:45634
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.81:46197
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.94:45215
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y9xnjamh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.99:36719
distributed.worker - INFO -          dashboard at:          172.21.1.99:38274
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f6cov9w_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bnk4cxmn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.246:42355
distributed.worker - INFO -          Listening to:   tcp://172.21.0.246:42355
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.1.130:41607
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-54pswhms
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.199:38983
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.23:33522
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t9te1inr
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qd15mcz8
distributed.worker - INFO -          dashboard at:          172.21.1.70:32769
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9b604un0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g77611mq
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.0.243:35659
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.79:32847
distributed.worker - INFO -          Listening to:    tcp://172.21.1.79:32847
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2qno6lkj
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8t5wt7k_
distributed.worker - INFO -          dashboard at:         172.21.0.232:34206
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o6ypnmwg
distributed.worker - INFO -          Listening to:   tcp://172.21.1.103:41439
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.81:46197
distributed.worker - INFO -          dashboard at:          172.21.1.81:35722
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5fa10u6u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y3mm9tol
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.38:42112
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.23:33522
distributed.worker - INFO -          dashboard at:         172.21.1.137:37988
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:          172.21.1.41:42820
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nr1gaaq1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-caxxxsjw
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.0.243:45427
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1w6_7nl_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.79:35311
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -          dashboard at:         172.21.1.103:45298
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qeyin30u
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yhrpl7ft
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.0.246:35562
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vu53bhod
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.1.38:42112
distributed.worker - INFO -          dashboard at:          172.21.1.38:42329
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3t5lnmwe
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ga6aigfe
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tonry0wy
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.233:44207
distributed.worker - INFO -          Listening to:   tcp://172.21.0.233:44207
distributed.worker - INFO -          dashboard at:         172.21.0.233:44224
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8ibim67_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jb9s3zx_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0qdnj4vq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hciu87fy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eeoo3vwz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qfso3k_3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jr2p8yce
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7zxmwa9a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-scj1e1j0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-54zvrfhk
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2urb47kd
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.23:45126
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pf7tjotk
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-omjeh8bk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-okke072s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1pw4k5ww
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dddgyz_2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oir1es4k
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.164:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lt5i7p3_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pvlfrov1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lii9_bvi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5xe7b4ax
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-upj6iydy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5vu2eoew
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ba0ahilo
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-odfwff5f
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x455n1b2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7vwsz4ot
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5nw4i8wn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qv3fs1s0
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-atagtu34
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xmmi3j_o
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xt6bgbf_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hlv9d837
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6nkmnv__
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-37jn4ood
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2jmpjjy0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oqjhngz5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z1ssj3xx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cmar3u_j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-skmktuom
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-174b660x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ya44gxn0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rgkm5y5v
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-py0wv2w3
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s4oakia0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9f_5ckjc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-su29jb7r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_bs9lqg1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.164:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 6.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 18.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 18.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 9.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 21.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 11.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 17.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 12.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.83s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 14.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1022: error: *** STEP 7588892.2 ON irene1022 CANCELLED AT 2022-12-13T23:50:53 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.81:44804'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.99:38535'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.154:36138'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.102:46513'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.239:35219'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.49:43368'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.103:36402'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.92:42375'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.77:34162'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.189:39763'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.86:46053'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.200:34816'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.90:33023'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.40:33777'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.94:37969'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.160:33513'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.235:39177'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.80:40846'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.76:34841'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.246:39121'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.22:40125'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.130:41716'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.159:38568'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.82:43496'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.157:33349'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.46:33015'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.74:35982'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.98:44934'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.38:38525'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.93:36461'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.199:33850'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.132:33232'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.23:43459'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.137:38368'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.41:40160'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.70:40123'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.75:33988'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.89:46013'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.245:34698'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.242:37031'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.237:43388'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.97:46228'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.95:38597'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.138:45561'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.100:43496'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.136:43734'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.243:33284'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.182:36725'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.158:43322'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.83:34643'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.241:43136'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.79:33800'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.39:32962'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.233:37038'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.126:34489'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.133:46340'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.123:33607'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.240:40748'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.232:33808'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.24:33221'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.78:41804'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.109:46328'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.103:45023'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.92:34817'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.81:42709'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.77:41165'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.189:41269'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.86:38990'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.200:44233'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.90:34844'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.40:40031'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.94:35164'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.160:45659'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.235:45099'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.99:36421'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.80:37921'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.76:38114'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.154:37981'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.246:34995'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.22:38139'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.130:46567'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.159:46331'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.82:40102'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.157:46467'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.102:35801'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.46:42430'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.74:40895'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.98:39101'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.38:40376'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.239:35268'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.93:41529'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.199:44448'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.132:44171'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.23:40032'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.137:33533'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.41:46026'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.70:43184'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.75:45334'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.89:41525'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.245:45937'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.242:36144'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.237:41128'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.97:43469'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.95:38317'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.138:36492'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.100:44498'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.136:45710'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.243:38907'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.182:44656'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.158:42707'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.83:33311'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.241:36581'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.79:38088'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.39:41702'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.49:38740'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.233:42701'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.126:42937'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.133:44113'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.123:44620'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.240:44681'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.232:42833'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.24:34595'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.78:40652'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.109:45065'
distributed.worker - INFO - Stopping worker at tcp://172.21.1.103:41439
distributed.worker - INFO - Stopping worker at tcp://172.21.1.92:46405
distributed.worker - INFO - Stopping worker at tcp://172.21.1.81:34812
distributed.worker - INFO - Stopping worker at tcp://172.21.1.77:45085
distributed.worker - INFO - Stopping worker at tcp://172.21.0.189:37053
distributed.worker - INFO - Stopping worker at tcp://172.21.1.86:39629
distributed.worker - INFO - Stopping worker at tcp://172.21.0.200:37077
distributed.worker - INFO - Stopping worker at tcp://172.21.1.40:37595
distributed.worker - INFO - Stopping worker at tcp://172.21.1.94:34122
distributed.worker - INFO - Stopping worker at tcp://172.21.1.160:43742
distributed.worker - INFO - Stopping worker at tcp://172.21.0.235:35843
distributed.worker - INFO - Stopping worker at tcp://172.21.1.99:36719
distributed.worker - INFO - Stopping worker at tcp://172.21.1.80:40710
distributed.worker - INFO - Stopping worker at tcp://172.21.1.76:34455
distributed.worker - INFO - Stopping worker at tcp://172.21.1.154:46634
distributed.worker - INFO - Stopping worker at tcp://172.21.0.246:40919
distributed.worker - INFO - Stopping worker at tcp://172.21.1.22:40440
distributed.worker - INFO - Stopping worker at tcp://172.21.1.130:37698
distributed.worker - INFO - Stopping worker at tcp://172.21.1.159:40733
distributed.worker - INFO - Stopping worker at tcp://172.21.1.157:33703
distributed.worker - INFO - Stopping worker at tcp://172.21.1.102:37283
distributed.worker - INFO - Stopping worker at tcp://172.21.1.46:42009
distributed.worker - INFO - Stopping worker at tcp://172.21.1.74:44536
distributed.worker - INFO - Stopping worker at tcp://172.21.1.98:34696
distributed.worker - INFO - Stopping worker at tcp://172.21.1.38:40399
distributed.worker - INFO - Stopping worker at tcp://172.21.0.239:41257
distributed.worker - INFO - Stopping worker at tcp://172.21.1.93:38116
distributed.worker - INFO - Stopping worker at tcp://172.21.0.199:33967
distributed.worker - INFO - Stopping worker at tcp://172.21.1.132:38957
distributed.worker - INFO - Stopping worker at tcp://172.21.1.23:35809
distributed.worker - INFO - Stopping worker at tcp://172.21.1.137:36063
distributed.worker - INFO - Stopping worker at tcp://172.21.1.70:43216
distributed.worker - INFO - Stopping worker at tcp://172.21.1.75:42269
distributed.worker - INFO - Stopping worker at tcp://172.21.0.245:38920
distributed.worker - INFO - Stopping worker at tcp://172.21.0.242:38022
distributed.worker - INFO - Stopping worker at tcp://172.21.1.97:35827
distributed.worker - INFO - Stopping worker at tcp://172.21.1.95:42698
distributed.worker - INFO - Stopping worker at tcp://172.21.1.138:45242
distributed.worker - INFO - Stopping worker at tcp://172.21.1.100:43784
distributed.worker - INFO - Stopping worker at tcp://172.21.0.243:39586
distributed.worker - INFO - Stopping worker at tcp://172.21.0.182:45758
distributed.worker - INFO - Stopping worker at tcp://172.21.1.158:41064
distributed.worker - INFO - Stopping worker at tcp://172.21.1.83:44338
distributed.worker - INFO - Stopping worker at tcp://172.21.0.241:37509
distributed.worker - INFO - Stopping worker at tcp://172.21.1.79:32847
distributed.worker - INFO - Stopping worker at tcp://172.21.1.39:39746
distributed.worker - INFO - Stopping worker at tcp://172.21.1.49:34169
distributed.worker - INFO - Stopping worker at tcp://172.21.0.233:38145
distributed.worker - INFO - Stopping worker at tcp://172.21.1.133:42347
distributed.worker - INFO - Stopping worker at tcp://172.21.1.123:36102
distributed.worker - INFO - Stopping worker at tcp://172.21.0.240:37275
distributed.worker - INFO - Stopping worker at tcp://172.21.0.232:42465
distributed.worker - INFO - Stopping worker at tcp://172.21.1.24:36414
distributed.worker - INFO - Stopping worker at tcp://172.21.1.78:43745
distributed.worker - INFO - Stopping worker at tcp://172.21.1.109:46714
distributed.worker - INFO - Stopping worker at tcp://172.21.1.103:39063
distributed.worker - INFO - Stopping worker at tcp://172.21.1.92:40529
distributed.worker - INFO - Stopping worker at tcp://172.21.1.81:46197
distributed.worker - INFO - Stopping worker at tcp://172.21.1.77:38815
distributed.worker - INFO - Stopping worker at tcp://172.21.0.189:40167
distributed.worker - INFO - Stopping worker at tcp://172.21.1.86:34266
distributed.worker - INFO - Stopping worker at tcp://172.21.0.200:42138
distributed.worker - INFO - Stopping worker at tcp://172.21.1.40:35189
distributed.worker - INFO - Stopping worker at tcp://172.21.1.94:33605
distributed.worker - INFO - Stopping worker at tcp://172.21.1.160:44309
distributed.worker - INFO - Stopping worker at tcp://172.21.0.235:39145
distributed.worker - INFO - Stopping worker at tcp://172.21.1.99:45119
distributed.worker - INFO - Stopping worker at tcp://172.21.1.80:39616
distributed.worker - INFO - Stopping worker at tcp://172.21.1.76:37887
distributed.worker - INFO - Stopping worker at tcp://172.21.1.154:34294
distributed.worker - INFO - Stopping worker at tcp://172.21.0.246:42355
distributed.worker - INFO - Stopping worker at tcp://172.21.1.22:38391
distributed.worker - INFO - Stopping worker at tcp://172.21.1.130:35400
distributed.worker - INFO - Stopping worker at tcp://172.21.1.159:34314
distributed.worker - INFO - Stopping worker at tcp://172.21.1.157:38641
distributed.worker - INFO - Stopping worker at tcp://172.21.1.102:42325
distributed.worker - INFO - Stopping worker at tcp://172.21.1.46:44063
distributed.worker - INFO - Stopping worker at tcp://172.21.1.74:45793
distributed.worker - INFO - Stopping worker at tcp://172.21.1.98:44049
distributed.worker - INFO - Stopping worker at tcp://172.21.1.38:42112
distributed.worker - INFO - Stopping worker at tcp://172.21.0.239:38304
distributed.worker - INFO - Stopping worker at tcp://172.21.1.93:35579
distributed.worker - INFO - Stopping worker at tcp://172.21.0.199:40796
distributed.worker - INFO - Stopping worker at tcp://172.21.1.132:42361
distributed.worker - INFO - Stopping worker at tcp://172.21.1.23:33522
distributed.worker - INFO - Stopping worker at tcp://172.21.1.137:32882
distributed.worker - INFO - Stopping worker at tcp://172.21.1.70:38373
distributed.worker - INFO - Stopping worker at tcp://172.21.1.75:33059
distributed.worker - INFO - Stopping worker at tcp://172.21.0.245:40890
distributed.worker - INFO - Stopping worker at tcp://172.21.0.242:34170
distributed.worker - INFO - Stopping worker at tcp://172.21.1.97:33788
distributed.worker - INFO - Stopping worker at tcp://172.21.1.95:45368
distributed.worker - INFO - Stopping worker at tcp://172.21.1.138:37008
distributed.worker - INFO - Stopping worker at tcp://172.21.1.100:34877
distributed.worker - INFO - Stopping worker at tcp://172.21.0.243:35659
distributed.worker - INFO - Stopping worker at tcp://172.21.0.182:42900
distributed.worker - INFO - Stopping worker at tcp://172.21.1.158:45083
distributed.worker - INFO - Stopping worker at tcp://172.21.1.83:34477
distributed.worker - INFO - Stopping worker at tcp://172.21.0.241:35264
distributed.worker - INFO - Stopping worker at tcp://172.21.1.79:41456
distributed.worker - INFO - Stopping worker at tcp://172.21.1.39:40037
distributed.worker - INFO - Stopping worker at tcp://172.21.1.49:40704
distributed.worker - INFO - Stopping worker at tcp://172.21.0.233:44207
distributed.worker - INFO - Stopping worker at tcp://172.21.1.133:43166
distributed.worker - INFO - Stopping worker at tcp://172.21.1.123:43868
distributed.worker - INFO - Stopping worker at tcp://172.21.0.240:38514
distributed.worker - INFO - Stopping worker at tcp://172.21.0.232:33596
distributed.worker - INFO - Stopping worker at tcp://172.21.1.24:38007
distributed.worker - INFO - Stopping worker at tcp://172.21.1.78:42182
distributed.worker - INFO - Stopping worker at tcp://172.21.1.109:37490
distributed.worker - INFO - Stopping worker at tcp://172.21.1.90:42304
distributed.worker - INFO - Stopping worker at tcp://172.21.1.90:43067
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.105:37915'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.105:43630'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.48:45490'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.48:39727'
distributed.worker - INFO - Stopping worker at tcp://172.21.0.237:43638
distributed.worker - INFO - Stopping worker at tcp://172.21.0.237:37728
distributed.worker - INFO - Stopping worker at tcp://172.21.1.89:39527
distributed.worker - INFO - Stopping worker at tcp://172.21.1.89:41840
distributed.worker - INFO - Stopping worker at tcp://172.21.1.41:36412
distributed.worker - INFO - Stopping worker at tcp://172.21.1.41:39141
distributed.worker - INFO - Stopping worker at tcp://172.21.1.82:34114
distributed.worker - INFO - Stopping worker at tcp://172.21.1.82:39004
distributed.worker - INFO - Stopping worker at tcp://172.21.1.105:45491
distributed.worker - INFO - Stopping worker at tcp://172.21.1.105:32772
distributed.worker - INFO - Stopping worker at tcp://172.21.1.136:36955
distributed.worker - INFO - Stopping worker at tcp://172.21.1.136:41612
distributed.worker - INFO - Stopping worker at tcp://172.21.1.126:37012
distributed.worker - INFO - Stopping worker at tcp://172.21.1.126:36156
distributed.worker - INFO - Stopping worker at tcp://172.21.1.48:36206
distributed.worker - INFO - Stopping worker at tcp://172.21.1.48:35880
distributed.comm.tcp - WARNING - Closing dangling stream in <TCP  local=tcp://172.21.0.246:40502 remote=tcp://172.21.0.164:8786>
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.1.92:42324 remote=tcp://172.21.0.164:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.1.23:47722 remote=tcp://172.21.0.164:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.1.82:43772 remote=tcp://172.21.0.164:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.0.246:39530 remote=tcp://172.21.0.164:8786>: Stream is closed
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57430 parent=56215 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23303 parent=21960 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13196 parent=11963 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16477 parent=15256 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35077 parent=33848 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57428 parent=56216 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47238 parent=46023 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47237 parent=46024 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17237 parent=16021 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13197 parent=11962 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23304 parent=21961 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76269 parent=75063 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27732 parent=26517 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76270 parent=75062 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48762 parent=47538 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42864 parent=41619 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16162 parent=14947 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42863 parent=41618 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16163 parent=14948 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65809 parent=64540 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52001 parent=50789 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48761 parent=47539 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74049 parent=72833 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74048 parent=72834 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52002 parent=50788 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65808 parent=64541 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43094 parent=41884 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70003 parent=68786 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47160 parent=45947 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70004 parent=68784 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59337 parent=58127 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52433 parent=51219 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16476 parent=15257 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25299 parent=24090 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43093 parent=41885 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9634 parent=8416 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43702 parent=42485 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59338 parent=58128 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14021 parent=12803 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46631 parent=45422 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47159 parent=45948 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28096 parent=26878 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9633 parent=8417 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32027 parent=30815 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27901 parent=26679 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16742 parent=15522 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44618 parent=43402 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16741 parent=15523 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52434 parent=51220 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33409 parent=32194 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35078 parent=33847 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43704 parent=42484 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28095 parent=26879 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14022 parent=12802 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33949 parent=32732 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60638 parent=59420 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94535 parent=93319 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27900 parent=26680 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25300 parent=24089 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53399 parent=52060 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86028 parent=84788 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87937 parent=86723 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67248 parent=66029 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9693 parent=8474 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46630 parent=45421 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=27731 parent=26518 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17236 parent=16020 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49841 parent=48604 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32026 parent=30814 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33408 parent=32195 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1457 parent=97906 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70778 parent=69564 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31670 parent=30454 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53397 parent=52061 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31669 parent=30455 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6697 parent=5473 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37539 parent=36320 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33948 parent=32733 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37127 parent=35917 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74700 parent=73488 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87938 parent=86724 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53349 parent=52133 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53348 parent=52134 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67247 parent=66030 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9692 parent=8475 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57473 parent=56262 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86027 parent=84789 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37126 parent=35918 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22330 parent=21102 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48280 parent=47059 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48279 parent=47058 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52254 parent=51037 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22329 parent=21101 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26456 parent=25236 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14559 parent=13349 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70777 parent=69565 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64864 parent=63634 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6696 parent=5474 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94534 parent=93320 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34882 parent=33667 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87464 parent=86246 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49840 parent=48605 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87465 parent=86247 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34883 parent=33666 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60637 parent=59421 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26457 parent=25237 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=14560 parent=13348 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44617 parent=43401 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57474 parent=56261 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67876 parent=66663 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25434 parent=24216 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61278 parent=60068 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61279 parent=60067 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85357 parent=84130 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=1458 parent=97905 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67875 parent=66664 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=64863 parent=63635 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97145 parent=95936 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52255 parent=51038 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25433 parent=24217 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97146 parent=95937 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85356 parent=84129 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37538 parent=36321 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74699 parent=73487 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49771 parent=48552 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5200 parent=3967 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49770 parent=48553 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5201 parent=3966 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5d64c0)

Current thread 0x00002b5ac8bc7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcfe4c0)

Current thread 0x00002b01f6cdbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c6c4c0)

Current thread 0x00002ba394671b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf674c0)

Current thread 0x00002ac11b3e9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xeba4c0)

Current thread 0x00002ab5ad34cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xca24c0)

Current thread 0x00002ad56f2aeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x216f4c0)

Current thread 0x00002ba3fdc10b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18944c0)

Current thread 0x00002b6934665b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x220b4c0)

Current thread 0x00002b3ef224db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20f64c0)

Current thread 0x00002ad6a786db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f024c0)

Current thread 0x00002b769be38b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x50d4c0)

Current thread 0x00002b711aa47b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e604c0)

Current thread 0x00002b0dec9b9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f004c0)

Current thread 0x00002b7876049b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5444c0)

Current thread 0x00002ae194df3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb904c0)

Current thread 0x00002ae07910eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10cd4c0)

Current thread 0x00002ba19cc18b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4804c0)

Current thread 0x00002b02c3c39b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10a74c0)

Current thread 0x00002b6440977b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x209c4c0)

Current thread 0x00002b2c153bdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a4d4c0)

Current thread 0x00002b6baedbcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4e94c0)

Current thread 0x00002b5aeaf5ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfac4c0)

Current thread 0x00002b3585efbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa684c0)

Current thread 0x00002b03e08d6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13674c0)

Current thread 0x00002b8181cb2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe7d4c0)

Current thread 0x00002b4dece6fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19724c0)

Current thread 0x00002accbb598b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8274c0)

Current thread 0x00002b20acaf4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xce14c0)

Current thread 0x00002b8c5d7e5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9ba4c0)

Current thread 0x00002ac53596ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1dad4c0)

Current thread 0x00002b1a88b11b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22214c0)

Current thread 0x00002ac5399ffb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf494c0)

Current thread 0x00002b471d379b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a0e4c0)

Current thread 0x00002b23ebe80b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6844c0)

Current thread 0x00002b258b236b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12da4c0)

Current thread 0x00002b15306b7b80 (most recent call first):
<no Python frame>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
srun: error: irene1029: task 3: Aborted (core dumped)
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
srun: error: irene1173: task 64: Aborted (core dumped)
srun: error: irene1188: task 85: Aborted (core dumped)
srun: error: irene1039: task 5: Aborted (core dumped)
srun: error: irene1247: task 118: Aborted (core dumped)
srun: error: irene1171: task 61: Aborted (core dumped)
srun: error: irene1176: task 71: Aborted (core dumped)
srun: error: irene1116: task 33: Aborted (core dumped)
srun: error: irene1080: task 18: Aborted (core dumped)
srun: error: irene1075: task 13: Aborted (core dumped)
srun: error: irene1223: task 107: Aborted (core dumped)
srun: error: irene1231: task 116: Aborted (core dumped)
srun: error: irene1168: task 54: Aborted (core dumped)
srun: error: irene1225: task 108: Aborted (core dumped)
srun: error: irene1182: task 75: Aborted (core dumped)
srun: error: irene1086: task 29: Aborted (core dumped)
srun: error: irene1230: task 114: Aborted (core dumped)
srun: error: irene1179: task 73: Aborted (core dumped)
srun: error: irene1250: task 121: Aborted (core dumped)
srun: error: irene1198: task 99: Aborted (core dumped)
srun: error: irene1175: task 69: Aborted (core dumped)
srun: error: irene1132: task 39: Aborted (core dumped)
srun: error: irene1185: task 79: Aborted (core dumped)
srun: error: irene1073: task 11: Aborted (core dumped)
srun: error: irene1196: task 96: Aborted (core dumped)
srun: error: irene1085: task 27: Aborted (core dumped)
srun: error: irene1169: task 57: Aborted (core dumped)
srun: error: irene1115: task 31: Aborted (core dumped)
srun: error: irene1186: task 81: Aborted (core dumped)
srun: error: irene1117: task 35: Aborted (core dumped)
srun: error: irene1072: task 8: Aborted (core dumped)
srun: error: irene1082: task 23: Aborted (core dumped)
srun: error: irene1167: task 53: Aborted (core dumped)
srun: error: irene1251: task 123: Aborted (core dumped)
srun: error: irene1083: task 24: Aborted (core dumped)
srun: error: irene1077: task 14: Aborted (core dumped)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7794c0)

Current thread 0x00002b455fd7cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12444c0)

Current thread 0x00002b55df34db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b9b4c0)

Current thread 0x00002b18df047b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xdc44c0)

Current thread 0x00002b77b5366b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4824c0)

Current thread 0x00002b8bea874b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x200f4c0)

Current thread 0x00002af472b74b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1de54c0)

Current thread 0x00002b0e741b9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18e44c0)

Current thread 0x00002b280a1f7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19de4c0)

Current thread 0x00002ad15296ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10b44c0)

Current thread 0x00002b1df090fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8bb4c0)

Current thread 0x00002aaf142c0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4a34c0)

Current thread 0x00002b38c1bc5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb674c0)

Current thread 0x00002b8a6d254b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8ba4c0)

Current thread 0x00002af0f46efb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c8b4c0)

Current thread 0x00002b3bb92d5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19e24c0)

Current thread 0x00002b32d6612b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c084c0)

Current thread 0x00002b337c480b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b814c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7d44c0)

Current thread 0x00002b6c9ebc7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a394c0)

Current thread 0x00002ad4cf865b80 (most recent call first):
<no Python frame>
Current thread 0x00002b02ecebdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e8f4c0)

Current thread 0x00002ad6fe040b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13844c0)

Current thread 0x00002b0eaecd8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d194c0)

Current thread 0x00002b646d592b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b634c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcbb4c0)

Current thread 0x00002b0ffd276b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x141d4c0)

Current thread 0x00002abd0aff2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6c24c0)

Current thread 0x00002ba890e96b80 (most recent call first):
<no Python frame>
Current thread 0x00002ad4cc731b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcdb4c0)

Current thread 0x00002aed181dfb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10f34c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc964c0)

Current thread 0x00002ba62e93db80 (most recent call first):
<no Python frame>
Current thread 0x00002ad46b62cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ac14c0)

Current thread 0x00002af7fbba0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8fc4c0)

Current thread 0x00002b38f0213b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x71a4c0)

Current thread 0x00002af0a10c5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x121b4c0)

Current thread 0x00002b3a0680db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x122d4c0)

Current thread 0x00002aeddd93fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xeda4c0)

Current thread 0x00002b21a7f6eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x21914c0)

Current thread 0x00002abd9471db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x218f4c0)

Current thread 0x00002b00b6a54b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18dc4c0)

Current thread 0x00002ad301733b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf604c0)

Current thread 0x00002af4bfeceb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf1b4c0)

Current thread 0x00002afc20f74b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16154c0)

Current thread 0x00002b5bda7feb80 (most recent call first):
<no Python frame>
srun: error: irene1170: task 59: Aborted (core dumped)
srun: error: irene1226: task 110: Aborted (core dumped)
srun: error: irene1253: tasks 126-127: Aborted (core dumped)
srun: error: irene1182: task 74: Aborted (core dumped)
srun: error: irene1167: task 52: Aborted (core dumped)
srun: error: irene1188: task 84: Aborted (core dumped)
srun: error: irene1085: task 26: Aborted (core dumped)
srun: error: irene1117: task 34: Aborted (core dumped)
srun: error: irene1169: task 56: Aborted (core dumped)
srun: error: irene1231: task 117: Aborted (core dumped)
srun: error: irene1116: task 32: Aborted (core dumped)
srun: error: irene1077: task 15: Aborted (core dumped)
srun: error: irene1176: task 70: Aborted (core dumped)
srun: error: irene1072: task 9: Aborted (core dumped)
srun: error: irene1083: task 25: Aborted (core dumped)
srun: error: irene1185: task 78: Aborted (core dumped)
srun: error: irene1251: task 122: Aborted (core dumped)
srun: error: irene1186: task 80: Aborted (core dumped)
srun: error: irene1132: task 38: Aborted (core dumped)
srun: error: irene1198: task 98: Aborted (core dumped)
srun: error: irene1230: task 115: Aborted (core dumped)
srun: error: irene1073: task 10: Aborted (core dumped)
srun: error: irene1179: task 72: Aborted (core dumped)
srun: error: irene1168: task 55: Aborted (core dumped)
srun: error: irene1086: task 28: Aborted (core dumped)
srun: error: irene1175: task 68: Aborted (core dumped)
srun: error: irene1247: task 119: Aborted (core dumped)
srun: error: irene1196: task 97: Aborted (core dumped)
srun: error: irene1173: task 65: Aborted (core dumped)
srun: error: irene1193: task 92: Aborted (core dumped)
srun: error: irene1195: task 95: Aborted (core dumped)
srun: error: irene1202: task 101: Aborted (core dumped)
srun: error: irene1252: task 125: Aborted (core dumped)
srun: error: irene1131: task 36: Aborted (core dumped)
srun: error: irene1081: task 20: Aborted (core dumped)
srun: error: irene1190: task 86: Aborted (core dumped)
srun: error: irene1040: task 6: Aborted (core dumped)
srun: error: irene1216: task 102: Aborted (core dumped)
srun: error: irene1139: task 44: Aborted (core dumped)
srun: error: irene1172: task 62: Aborted (core dumped)
srun: error: irene1229: task 112: Aborted (core dumped)
