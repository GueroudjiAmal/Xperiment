distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.17:33909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.238:45907'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.247:45244'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.240:35259'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.238:34318'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.244:41443'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.244:46707'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.33:40045'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.33:41109'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.246:38953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.79:45441'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.246:45919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.41:39308'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:35261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.41:36486'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:34522'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.239:45956'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.239:37217'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.21:46137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.21:38623'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.24:42066'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.245:42806'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.245:44440'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.34:38574'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.17:46120'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.247:34511'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.240:39062'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:41775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:41261'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.79:44798'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.24:45925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.34:36671'
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.247:43300
distributed.worker - INFO -          Listening to:   tcp://172.21.8.247:43300
distributed.worker - INFO -          dashboard at:         172.21.8.247:32828
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.247:33955
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.247:33955
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.247:40518
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-89plzo59
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.21:40758
distributed.worker - INFO -          Listening to:    tcp://172.21.9.21:40758
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xqba51vw
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.21:36220
distributed.worker - INFO -          Listening to:    tcp://172.21.9.21:36220
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:42785
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.79:34764
distributed.worker - INFO -          dashboard at:          172.21.9.21:41094
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:32942
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:42785
distributed.worker - INFO -          Listening to:    tcp://172.21.9.79:34764
distributed.worker - INFO -          dashboard at:          172.21.9.21:44214
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:32942
distributed.worker - INFO -          dashboard at:         172.21.8.242:39907
distributed.worker - INFO -          dashboard at:          172.21.9.79:46153
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.239:46466
distributed.worker - INFO -          Listening to:   tcp://172.21.8.239:46466
distributed.worker - INFO -          dashboard at:         172.21.8.239:41724
distributed.worker - INFO -          dashboard at:          172.21.9.20:40397
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.34:40062
distributed.worker - INFO -          Listening to:    tcp://172.21.9.34:40062
distributed.worker - INFO -          dashboard at:          172.21.9.34:41589
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.79:39913
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.239:45186
distributed.worker - INFO -          Listening to:   tcp://172.21.8.239:45186
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:44971
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.240:45974
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.17:43478
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.240:45974
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.9.79:39913
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.9.17:43478
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:44971
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.240:46484
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.239:36526
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.9.17:41501
distributed.worker - INFO -          dashboard at:         172.21.8.242:41435
distributed.worker - INFO -          dashboard at:         172.21.8.240:43982
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.34:36075
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.24:46078
distributed.worker - INFO -          Listening to:    tcp://172.21.9.24:46078
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.238:33847
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.33:46561
distributed.worker - INFO -          Listening to:    tcp://172.21.9.33:46561
distributed.worker - INFO -          dashboard at:          172.21.9.33:39464
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wsjt110d
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:45199
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.9.79:44576
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.24:45976
distributed.worker - INFO -          Listening to:    tcp://172.21.9.24:45976
distributed.worker - INFO -          Listening to:   tcp://172.21.8.238:33847
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.33:40943
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-enzfxasv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.240:46484
distributed.worker - INFO -          Listening to:    tcp://172.21.9.34:36075
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:          172.21.9.24:41154
distributed.worker - INFO -          dashboard at:         172.21.8.238:44446
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xtpcxnas
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:45199
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q644zcte
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.33:40943
distributed.worker - INFO -          dashboard at:          172.21.9.33:40372
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.17:36610
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.240:33429
distributed.worker - INFO -          dashboard at:          172.21.9.34:40094
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.238:39514
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dn0faaml
distributed.worker - INFO -          dashboard at:          172.21.9.20:39689
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_86ujgsn
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cn8lqy4s
distributed.worker - INFO -          dashboard at:          172.21.9.24:45870
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.17:36610
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ldshwrnl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xg1tbjiz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.238:39514
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.9.17:32832
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.238:32780
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vxgbj1xm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n0m__cjg
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kmepbw7w
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_ip2uwrk
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-joqk57nr
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bufl5aej
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8oaflhhw
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x_dzt2ct
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cgoc5xje
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f_6keibz
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dxf0mws6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bloy9x45
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w4sda9cw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.41:40988
distributed.worker - INFO -          Listening to:    tcp://172.21.9.41:40988
distributed.worker - INFO -          dashboard at:          172.21.9.41:35435
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.41:36573
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.9.41:36573
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.9.41:35617
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1trg042x
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.246:41171
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.246:41171
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.246:40559
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-423eavso
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.246:36262
distributed.worker - INFO -          Listening to:   tcp://172.21.8.246:36262
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:         172.21.8.246:34335
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4d25uecm
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jqifa7qe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.245:38108
distributed.worker - INFO -          Listening to:   tcp://172.21.8.245:38108
distributed.worker - INFO -          dashboard at:         172.21.8.245:46860
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.245:41529
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.8.245:41529
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.245:35746
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z1gcfd04
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p13mis9r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.244:33183
distributed.worker - INFO -          Listening to:   tcp://172.21.8.244:33183
distributed.worker - INFO -          dashboard at:         172.21.8.244:42234
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.244:43884
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.244:43884
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.244:38910
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0ehletw2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-032xje1r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene2182: error: *** STEP 8487724.1 ON irene2182 CANCELLED AT 2023-02-24T01:32:59 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.33:41109'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.21:46137'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.33:40045'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.21:38623'
distributed.worker - INFO - Stopping worker at tcp://172.21.9.33:46561
distributed.worker - INFO - Stopping worker at tcp://172.21.9.33:40943
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.34:36671'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.34:38574'
distributed.worker - INFO - Stopping worker at tcp://172.21.9.21:40758
distributed.worker - INFO - Stopping worker at tcp://172.21.9.21:36220
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.79:44798'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.79:45441'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.41:39308'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.41:36486'
distributed.worker - INFO - Stopping worker at tcp://172.21.9.41:36573
distributed.worker - INFO - Stopping worker at tcp://172.21.9.41:40988
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:35261'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.24:45925'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.24:42066'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.239:45956'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.246:45919'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.239:37217'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.246:38953'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.245:42806'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.247:45244'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.240:39062'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.245:44440'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.17:46120'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.247:34511'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:34522'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.17:33909'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:41261'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:41775'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.238:45907'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.238:34318'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.240:35259'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.244:41443'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.244:46707'
distributed.worker - INFO - Stopping worker at tcp://172.21.8.246:41171
distributed.worker - INFO - Stopping worker at tcp://172.21.8.246:36262
distributed.worker - INFO - Stopping worker at tcp://172.21.8.247:43300
distributed.worker - INFO - Stopping worker at tcp://172.21.8.247:33955
distributed.worker - INFO - Stopping worker at tcp://172.21.8.239:45186
distributed.worker - INFO - Stopping worker at tcp://172.21.8.239:46466
distributed.worker - INFO - Stopping worker at tcp://172.21.9.79:34764
distributed.worker - INFO - Stopping worker at tcp://172.21.9.79:39913
distributed.worker - INFO - Stopping worker at tcp://172.21.9.24:45976
distributed.worker - INFO - Stopping worker at tcp://172.21.9.24:46078
distributed.worker - INFO - Stopping worker at tcp://172.21.9.17:43478
distributed.worker - INFO - Stopping worker at tcp://172.21.9.17:36610
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:44971
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:42785
distributed.worker - INFO - Stopping worker at tcp://172.21.8.244:33183
distributed.worker - INFO - Stopping worker at tcp://172.21.8.244:43884
distributed.worker - INFO - Stopping worker at tcp://172.21.8.238:33847
distributed.worker - INFO - Stopping worker at tcp://172.21.8.238:39514
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:32942
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:45199
distributed.worker - INFO - Stopping worker at tcp://172.21.8.240:46484
distributed.worker - INFO - Stopping worker at tcp://172.21.8.240:45974
distributed.worker - INFO - Stopping worker at tcp://172.21.8.245:41529
distributed.worker - INFO - Stopping worker at tcp://172.21.8.245:38108
distributed.worker - INFO - Stopping worker at tcp://172.21.9.34:40062
distributed.worker - INFO - Stopping worker at tcp://172.21.9.34:36075
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57795 parent=57699 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=3289 parent=3192 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57796 parent=57698 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=3290 parent=3191 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83137 parent=83039 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13934 parent=13838 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83136 parent=83038 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59659 parent=59562 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50443 parent=50345 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59658 parent=59561 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50442 parent=50346 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17475 parent=17377 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70386 parent=70288 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16857 parent=16760 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13935 parent=13837 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17474 parent=17376 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72348 parent=72250 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16858 parent=16759 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16766 parent=16670 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19156 parent=19058 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72347 parent=72251 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28315 parent=28216 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70385 parent=70289 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19155 parent=19059 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28316 parent=28215 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16767 parent=16669 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86430 parent=86333 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86431 parent=86332 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10633 parent=10535 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10632 parent=10534 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57956 parent=57859 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57957 parent=57858 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1feb4c0)

Current thread 0x00002b361a72bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c3a4c0)

Current thread 0x00002b06a91bdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fe14c0)

Current thread 0x00002b37ab0b2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe4e4c0)

Current thread 0x00002b8e7582cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19724c0)

Current thread 0x00002b02265c7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10bb4c0)

Current thread 0x00002add6a18cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9df4c0)

Current thread 0x00002aba21e15b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xa444c0)

Current thread 0x00002ba409f77b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f334c0)

Current thread 0x00002b5d6b14ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x130e4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd3e4c0)

Current thread 0x00002b6319a6bb80 (most recent call first):
<no Python frame>
Current thread 0x00002b1f14c2fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11b04c0)

Current thread 0x00002ba34c16cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e0a4c0)

Current thread 0x00002ad1ec549b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b7f4c0)

Current thread 0x00002af565265b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9b74c0)

Current thread 0x00002b8fe7bb7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xad84c0)

Current thread 0x00002ac9c1c3db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14564c0)

Current thread 0x00002b0229ea3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15314c0)

Current thread 0x00002ae3ab2f7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9754c0)

Current thread 0x00002b01b1fb7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbc44c0)

Current thread 0x00002b16bf41ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f474c0)

Current thread 0x00002b2193129b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5054c0)

Current thread 0x00002af6e544cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc234c0)

Current thread 0x00002b2d8f71bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xeab4c0)

Current thread 0x00002ba2d62bdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x214f4c0)

Current thread 0x00002b291fafab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16344c0)

Current thread 0x00002ad212fb0b80 (most recent call first):
<no Python frame>
srun: error: irene2191: tasks 14-15: Aborted (core dumped)
srun: error: irene2190: tasks 12-13: Aborted (core dumped)
srun: error: irene2217: tasks 18-19: Aborted (core dumped)
srun: error: irene2238: tasks 28-29: Aborted (core dumped)
srun: error: irene2182: tasks 0-1: Aborted (core dumped)
srun: error: irene2188: tasks 8-9: Aborted (core dumped)
srun: error: irene2186: tasks 6-7: Aborted (core dumped)
srun: error: irene2221: tasks 22-23: Aborted (core dumped)
srun: error: irene2276: tasks 30-31: Aborted (core dumped)
srun: error: irene2218: tasks 20-21: Aborted (core dumped)
srun: error: irene2214: task 17: Aborted (core dumped)
srun: error: irene2184: task 4: Aborted (core dumped)
srun: error: irene2231: task 27: Aborted (core dumped)
srun: error: irene2230: task 24: Aborted (core dumped)
srun: error: irene2189: task 11: Aborted (core dumped)
srun: error: irene2183: task 3: Aborted (core dumped)
