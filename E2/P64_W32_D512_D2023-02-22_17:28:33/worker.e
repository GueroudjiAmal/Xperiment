distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.27:41466'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.24:46451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.24:43220'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.180:46875'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.31:41415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.180:45837'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.28:34359'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.29:43690'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.84:39742'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.30:46655'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.30:45145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.84:45290'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.36:36546'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.96:46096'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.245:35095'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.96:41246'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.181:41564'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:36549'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.181:34980'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.177:40428'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.93:35330'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.93:44756'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.140:45681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.27:38586'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.31:35626'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.28:40863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.29:35811'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.36:33337'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:36023'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.245:40569'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.140:46377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.177:38701'
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.30:33096
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.30:43017
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.24:36399
distributed.worker - INFO -          Listening to:    tcp://172.21.1.30:33096
distributed.worker - INFO -          dashboard at:          172.21.1.30:39075
distributed.worker - INFO -          Listening to:    tcp://172.21.1.24:36399
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          dashboard at:          172.21.1.24:46464
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.1.30:43017
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.1.30:39293
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.24:36779
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7016zwvo
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hqoa0y4i
distributed.worker - INFO -          Listening to:    tcp://172.21.1.24:36779
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.1.24:42321
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1thqsfvu
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5m5mdm4s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.96:34418
distributed.worker - INFO -          Listening to:    tcp://172.21.3.96:34418
distributed.worker - INFO -          dashboard at:          172.21.3.96:41273
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4xqpxq_s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.29:41998
distributed.worker - INFO -          Listening to:    tcp://172.21.1.29:41998
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.29:36589
distributed.worker - INFO -          Listening to:    tcp://172.21.1.29:36589
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.96:42189
distributed.worker - INFO -          dashboard at:          172.21.1.29:35465
distributed.worker - INFO -          dashboard at:          172.21.1.29:45279
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.96:42189
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.96:44569
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.93:36572
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fk3x0p4_
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0pvgckrh
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t0t9ul8g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.93:36572
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.93:36944
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nxilrh63
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.93:34673
distributed.worker - INFO -          Listening to:    tcp://172.21.3.93:34673
distributed.worker - INFO -          dashboard at:          172.21.3.93:33890
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-485awiw1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.31:40144
distributed.worker - INFO -          Listening to:    tcp://172.21.1.31:40144
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.36:43276
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.31:40431
distributed.worker - INFO -          Listening to:    tcp://172.21.1.31:40431
distributed.worker - INFO -          dashboard at:          172.21.1.31:43179
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:41129
distributed.worker - INFO -          dashboard at:          172.21.1.31:42003
distributed.worker - INFO -          Listening to:    tcp://172.21.3.36:43276
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:41129
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.28:38102
distributed.worker - INFO -          Listening to:    tcp://172.21.1.28:38102
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          dashboard at:          172.21.3.36:36262
distributed.worker - INFO -          dashboard at:          172.21.3.34:42582
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.28:38212
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          dashboard at:          172.21.1.28:43901
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.1.28:38212
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.36:33761
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.28:35725
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xz3rhpjs
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.84:37712
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.245:43196
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.36:33761
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2z9f93i9
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-575l50k7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kd_5ckpf
distributed.worker - INFO -          dashboard at:          172.21.3.36:36662
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tpdz3yob
distributed.worker - INFO -          Listening to:    tcp://172.21.3.84:37712
distributed.worker - INFO -          Listening to:   tcp://172.21.2.245:43196
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.84:45130
distributed.worker - INFO -          dashboard at:         172.21.2.245:35325
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.84:46847
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.140:33136
distributed.worker - INFO -          Listening to:   tcp://172.21.3.140:33136
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:32798
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ch8l7k07
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.27:39640
distributed.worker - INFO -          dashboard at:         172.21.3.140:41170
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:32798
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.177:40542
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.181:36418
distributed.worker - INFO -          Listening to:    tcp://172.21.3.84:46847
distributed.worker - INFO -          Listening to:    tcp://172.21.1.27:39640
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9wbpyif8
distributed.worker - INFO -          dashboard at:          172.21.3.34:44881
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.177:41017
distributed.worker - INFO -          Listening to:   tcp://172.21.9.181:36418
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.27:35633
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.140:39530
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x6306wsa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.9.177:41017
distributed.worker - INFO -          dashboard at:         172.21.9.181:38523
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.180:45291
distributed.worker - INFO -          dashboard at:          172.21.3.84:46836
distributed.worker - INFO -          Listening to:    tcp://172.21.1.27:35633
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.9.177:35458
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.27:41235
distributed.worker - INFO -          Listening to:   tcp://172.21.3.140:39530
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.245:45602
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.181:41632
distributed.worker - INFO -          Listening to:   tcp://172.21.9.180:45291
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.2.245:45602
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.9.180:36762
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-945xns2f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.3.140:36663
distributed.worker - INFO -          dashboard at:         172.21.2.245:43208
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sl8ggli1
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zmvxuv1y
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.9.181:41632
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-la7d7jvg
distributed.worker - INFO -          dashboard at:         172.21.9.181:45998
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2izuonh8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.9.177:40542
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_rat4g52
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oz6uy9q3
distributed.worker - INFO -          dashboard at:         172.21.9.177:37556
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.1.27:44362
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a8iazah7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wd6i8580
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-df3kez3_
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.9.180:46531
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uxbh_a94
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7687noac
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.9.180:46531
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-czc5jzya
distributed.worker - INFO -          dashboard at:         172.21.9.180:39477
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-11jvax95
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.18:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1117: error: *** STEP 8487737.1 ON irene1117 CANCELLED AT 2023-02-24T02:02:03 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.181:34980'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.180:46875'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.96:41246'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.84:39742'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.93:44756'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.30:46655'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.24:46451'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.27:41466'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.140:46377'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.245:35095'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.31:35626'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.36:36546'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:36023'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.177:40428'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.28:34359'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.29:35811'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.181:41564'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.180:45837'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.96:46096'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.84:45290'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.93:35330'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.30:45145'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.24:43220'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.27:38586'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.140:45681'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.245:40569'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.31:41415'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.36:33337'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:36549'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.177:38701'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.28:40863'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.29:43690'
distributed.worker - INFO - Stopping worker at tcp://172.21.9.181:41632
distributed.worker - INFO - Stopping worker at tcp://172.21.1.30:33096
distributed.worker - INFO - Stopping worker at tcp://172.21.1.24:36779
distributed.worker - INFO - Stopping worker at tcp://172.21.1.28:38102
distributed.worker - INFO - Stopping worker at tcp://172.21.1.29:41998
distributed.worker - INFO - Stopping worker at tcp://172.21.9.181:36418
distributed.worker - INFO - Stopping worker at tcp://172.21.1.30:43017
distributed.worker - INFO - Stopping worker at tcp://172.21.1.24:36399
distributed.worker - INFO - Stopping worker at tcp://172.21.1.28:38212
distributed.worker - INFO - Stopping worker at tcp://172.21.1.29:36589
distributed.worker - INFO - Stopping worker at tcp://172.21.3.36:33761
distributed.worker - INFO - Stopping worker at tcp://172.21.3.36:43276
distributed.worker - INFO - Stopping worker at tcp://172.21.3.140:33136
distributed.worker - INFO - Stopping worker at tcp://172.21.3.140:39530
distributed.worker - INFO - Stopping worker at tcp://172.21.9.180:45291
distributed.worker - INFO - Stopping worker at tcp://172.21.9.180:46531
distributed.worker - INFO - Stopping worker at tcp://172.21.9.177:41017
distributed.worker - INFO - Stopping worker at tcp://172.21.9.177:40542
distributed.worker - INFO - Stopping worker at tcp://172.21.3.93:34673
distributed.worker - INFO - Stopping worker at tcp://172.21.1.31:40431
distributed.worker - INFO - Stopping worker at tcp://172.21.3.93:36572
distributed.worker - INFO - Stopping worker at tcp://172.21.1.31:40144
distributed.worker - INFO - Stopping worker at tcp://172.21.2.245:45602
distributed.worker - INFO - Stopping worker at tcp://172.21.2.245:43196
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:32798
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:41129
distributed.worker - INFO - Stopping worker at tcp://172.21.3.84:37712
distributed.worker - INFO - Stopping worker at tcp://172.21.3.84:46847
distributed.worker - INFO - Stopping worker at tcp://172.21.1.27:39640
distributed.worker - INFO - Stopping worker at tcp://172.21.1.27:35633
distributed.worker - INFO - Stopping worker at tcp://172.21.3.96:42189
distributed.worker - INFO - Stopping worker at tcp://172.21.3.96:34418
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36410 parent=36306 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36409 parent=36305 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37090 parent=36985 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96116 parent=96012 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61562 parent=61462 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97479 parent=97374 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87902 parent=87801 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=37091 parent=36984 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97478 parent=97375 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21429 parent=21325 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87901 parent=87802 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96115 parent=96013 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4058 parent=3951 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42471 parent=42362 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93875 parent=93768 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93874 parent=93769 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61561 parent=61461 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4059 parent=3952 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42470 parent=42361 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21430 parent=21326 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17370 parent=17270 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91663 parent=91563 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57452 parent=57348 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91664 parent=91564 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38162 parent=38057 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93888 parent=93788 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72015 parent=71913 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38161 parent=38056 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93887 parent=93789 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17369 parent=17271 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57453 parent=57347 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72016 parent=71912 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13434c0)

Current thread 0x00002afbdc8f0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x116f4c0)

Current thread 0x00002aec56ef4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x147a4c0)

Current thread 0x00002b8ba0a49b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd784c0)

Current thread 0x00002b6df7801b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d304c0)

Current thread 0x00002ac6ed4c5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ecd4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19b24c0)

Current thread 0x00002b3aba8f1b80 (most recent call first):
<no Python frame>
Current thread 0x00002ac34ece5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14f84c0)

Current thread 0x00002aab36aaab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8e64c0)

Current thread 0x00002b0a22108b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcef4c0)

Current thread 0x00002af164530b80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x133a4c0)

<no Python frame>
Current thread 0x00002b8882e1cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6634c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17044c0)

Current thread 0x00002ad4d9423b80 (most recent call first):
<no Python frame>
Current thread 0x00002aae74bccb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10674c0)

Current thread 0x00002b8756802b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7ff4c0)

Current thread 0x00002adefee22b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5c74c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17034c0)

Current thread 0x00002ac145de3b80 (most recent call first):
Current thread 0x00002ad96f0a8b80 (most recent call first):
<no Python frame>
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x141a4c0)

Current thread 0x00002ab28e0c9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9774c0)

Current thread 0x00002b25e97acb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe604c0)

Current thread 0x00002ad81172fb80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22804c0)
<no Python frame>

Current thread 0x00002b8bf80b6b80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20bf4c0)

<no Python frame>
Current thread 0x00002b107da97b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10e44c0)

Current thread 0x00002b24c498fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7b94c0)

Current thread 0x00002b5793c44b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x189b4c0)

Current thread 0x00002abf71743b80 (most recent call first):
<no Python frame>
srun: error: irene2374: tasks 26-27: Aborted (core dumped)
srun: error: irene1120: tasks 2-3: Aborted (core dumped)
srun: error: irene1122: tasks 6-7: Aborted (core dumped)
srun: error: irene1124: tasks 10-11: Aborted (core dumped)
srun: error: irene1509: tasks 24-25: Aborted (core dumped)
srun: error: irene1465: tasks 22-23: Aborted (core dumped)
srun: error: irene1361: tasks 12-13: Aborted (core dumped)
srun: error: irene1123: tasks 8-9: Aborted (core dumped)
srun: error: irene1117: tasks 0-1: Aborted (core dumped)
srun: error: irene1121: tasks 4-5: Aborted (core dumped)
srun: error: irene2377: task 29: Aborted (core dumped)
srun: error: irene1453: task 18: Aborted (core dumped)
srun: error: irene1405: task 17: Aborted (core dumped)
srun: error: irene1462: task 21: Aborted (core dumped)
srun: error: irene2378: task 30: Aborted (core dumped)
