distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.51:46139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.55:43070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.50:42386'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.39:40042'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.134:45906'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.85:43842'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.144:43422'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.51:41938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.50:41370'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.55:37294'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.39:38345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.144:43379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.134:46673'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.85:33887'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.163:46187'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.163:45092'
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.51:38856
distributed.worker - INFO -          Listening to:    tcp://172.21.5.51:38856
distributed.worker - INFO -          dashboard at:          172.21.5.51:46714
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.51:37686
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.5.51:37686
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.5.51:37095
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o6gs35pr
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jqz0b6fl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.5.144:36445
distributed.worker - INFO -          Listening to:   tcp://172.21.5.144:36445
distributed.worker - INFO -          dashboard at:         172.21.5.144:38744
distributed.worker - INFO -       Start worker at:   tcp://172.21.5.144:45507
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.5.144:45507
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.5.144:33447
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xusjq5a0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.50:39855
distributed.worker - INFO -          Listening to:    tcp://172.21.5.50:39855
distributed.worker - INFO -          dashboard at:          172.21.5.50:45880
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.50:44766
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.5.50:44766
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.5.50:43083
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uc6cdeot
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xsju1__q
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.85:34282
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2whfgvme
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.39:38034
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.5.85:34282
distributed.worker - INFO -          dashboard at:          172.21.5.85:36221
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.39:37396
distributed.worker - INFO -          Listening to:    tcp://172.21.5.39:37396
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.5.39:38034
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fq72rqit
distributed.worker - INFO -          dashboard at:          172.21.5.39:44422
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.5.39:39437
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bephl_f6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_h6gfei1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.55:33869
distributed.worker - INFO -          Listening to:    tcp://172.21.5.55:33869
distributed.worker - INFO -       Start worker at:   tcp://172.21.5.134:34959
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.85:41269
distributed.worker - INFO -          Listening to:   tcp://172.21.5.134:34959
distributed.worker - INFO -       Start worker at:   tcp://172.21.5.134:43877
distributed.worker - INFO -          dashboard at:         172.21.5.134:36087
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.5.85:41269
distributed.worker - INFO -          dashboard at:          172.21.5.55:41031
distributed.worker - INFO -          Listening to:   tcp://172.21.5.134:43877
distributed.worker - INFO -          dashboard at:          172.21.5.85:43559
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.55:44487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.5.55:44487
distributed.worker - INFO -          dashboard at:         172.21.5.134:39444
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.5.55:33835
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v50d3tes
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hxb8jcnu
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pezr4jzo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rnsfru77
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xhzvmr0_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.163:45926
distributed.worker - INFO -          Listening to:   tcp://172.21.1.163:45926
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.163:45816
distributed.worker - INFO -          dashboard at:         172.21.1.163:46832
distributed.worker - INFO -          Listening to:   tcp://172.21.1.163:45816
distributed.worker - INFO -          dashboard at:         172.21.1.163:46175
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pvjqf9dl
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8p3d53jd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.19:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1256: error: *** STEP 8466589.1 ON irene1256 CANCELLED AT 2023-02-22T05:15:11 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.163:45092'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.85:43842'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.144:43422'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.55:43070'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.50:42386'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.39:38345'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.134:45906'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.51:46139'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.163:46187'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.85:33887'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.144:43379'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.55:37294'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.50:41370'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.39:40042'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.134:46673'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.51:41938'
distributed.worker - INFO - Stopping worker at tcp://172.21.5.51:37686
distributed.worker - INFO - Stopping worker at tcp://172.21.5.51:38856
distributed.worker - INFO - Stopping worker at tcp://172.21.5.144:36445
distributed.worker - INFO - Stopping worker at tcp://172.21.5.144:45507
distributed.worker - INFO - Stopping worker at tcp://172.21.5.39:38034
distributed.worker - INFO - Stopping worker at tcp://172.21.5.39:37396
distributed.worker - INFO - Stopping worker at tcp://172.21.5.55:33869
distributed.worker - INFO - Stopping worker at tcp://172.21.5.85:34282
distributed.worker - INFO - Stopping worker at tcp://172.21.5.85:41269
distributed.worker - INFO - Stopping worker at tcp://172.21.5.55:44487
distributed.worker - INFO - Stopping worker at tcp://172.21.1.163:45926
distributed.worker - INFO - Stopping worker at tcp://172.21.1.163:45816
distributed.worker - INFO - Stopping worker at tcp://172.21.5.50:39855
distributed.worker - INFO - Stopping worker at tcp://172.21.5.50:44766
distributed.worker - INFO - Stopping worker at tcp://172.21.5.134:34959
distributed.worker - INFO - Stopping worker at tcp://172.21.5.134:43877
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90524 parent=90418 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90523 parent=90419 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21481 parent=21381 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21482 parent=21382 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97514 parent=97412 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9527 parent=9428 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97513 parent=97413 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22736 parent=22637 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22737 parent=22636 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9528 parent=9427 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45683 parent=45583 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7986 parent=7886 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7987 parent=7885 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45684 parent=45582 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25095 parent=24991 started daemon>
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25094 parent=24990 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b9f4c0)

Current thread 0x00002ac9f6d28b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e7b4c0)

Current thread 0x00002ab1c5746b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10924c0)

Current thread 0x00002b2cc8001b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14614c0)

Current thread 0x00002b6dd6118b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15604c0)

Current thread 0x00002ab93143bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xdd14c0)

Current thread 0x00002b0113542b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13ea4c0)

Current thread 0x00002b25c8749b80 (most recent call first):
<no Python frame>
srun: error: irene1696: tasks 6-7: Aborted (core dumped)
srun: error: irene1779: tasks 12-13: Aborted (core dumped)
srun: error: irene1256: task 0: Aborted (core dumped)
srun: error: irene1684: task 2: Aborted (core dumped)
srun: error: irene1700: task 9: Aborted (core dumped)
