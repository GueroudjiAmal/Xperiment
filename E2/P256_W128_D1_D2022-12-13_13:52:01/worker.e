distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.174:36230'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.51:32906'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.57:44535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.174:33677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.57:45505'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.101:33709'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.51:36272'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.93:42718'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.64:35938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.101:41753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.93:35999'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.64:36938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.178:43523'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.178:43750'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.45:43694'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.45:36070'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.185:35979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.185:39938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.175:45819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.82:39491'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.175:35916'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.82:33145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.176:41227'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.56:35374'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.56:42010'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.47:35461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.176:41184'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.164:39037'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.47:41199'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.164:33461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.65:43300'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.65:36636'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.183:41398'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.181:38165'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.183:39468'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.181:35839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.182:38433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.46:44460'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.182:44602'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.46:36216'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.39:40783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.74:38899'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.167:42055'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.39:38613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.53:37088'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.74:40341'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.53:45978'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.167:42613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.76:37675'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.76:42852'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.99:39512'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.177:42149'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.99:39968'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.49:40583'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.177:43656'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.49:44919'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.173:44409'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.58:33348'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.173:35460'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.165:36711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.97:36047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.97:35451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.165:34991'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.58:41605'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.186:37060'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.186:33961'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.96:38276'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.81:40940'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.81:45575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.96:33783'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.163:35393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.163:44862'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.41:40883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.40:41714'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.41:46132'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.50:38379'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.50:44282'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.40:33554'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.184:43285'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.184:42257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.55:42282'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.168:43460'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.55:36932'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.168:41354'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.180:42355'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.180:38915'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.169:34470'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.85:44347'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.85:45994'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.169:35096'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.63:46426'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.100:45502'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.100:40613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.179:45938'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.179:46641'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.80:33135'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.63:40391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.80:35925'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.59:44064'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.59:41649'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.171:34124'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.171:36943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.162:41793'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.54:43577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.162:39383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.83:38760'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.83:44238'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.98:38701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.54:41613'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.166:40531'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.166:36921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.91:39403'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.91:37175'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.98:45765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.72:42573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.62:33008'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.62:39872'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.52:40131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.170:44537'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.170:38714'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.52:43129'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.44:42750'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.44:41397'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.61:44015'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.61:35156'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.60:36667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.60:34545'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.72:37242'
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.85:36641
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.85:46409
distributed.worker - INFO -          Listening to:    tcp://172.21.3.85:46409
distributed.worker - INFO -          Listening to:    tcp://172.21.3.85:36641
distributed.worker - INFO -          dashboard at:          172.21.3.85:43626
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.85:39183
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dhb8ktsr
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4rzbwrc0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.184:43626
distributed.worker - INFO -          Listening to:   tcp://172.21.2.184:43626
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.101:40066
distributed.worker - INFO -          Listening to:   tcp://172.21.3.101:40066
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.51:36973
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.83:46798
distributed.worker - INFO -          Listening to:    tcp://172.21.3.83:46798
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.64:35431
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.57:36123
distributed.worker - INFO -          Listening to:    tcp://172.21.3.57:36123
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.80:35561
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.74:38752
distributed.worker - INFO -          Listening to:    tcp://172.21.3.74:38752
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.45:46107
distributed.worker - INFO -          Listening to:    tcp://172.21.3.45:46107
distributed.worker - INFO -          dashboard at:          172.21.3.45:46114
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.174:42653
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.183:42108
distributed.worker - INFO -          Listening to:   tcp://172.21.2.183:42108
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.100:46317
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.60:33048
distributed.worker - INFO -          Listening to:    tcp://172.21.3.60:33048
distributed.worker - INFO -          dashboard at:          172.21.3.60:43402
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.40:44489
distributed.worker - INFO -          Listening to:    tcp://172.21.3.40:44489
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.61:39201
distributed.worker - INFO -          Listening to:    tcp://172.21.3.61:39201
distributed.worker - INFO -          dashboard at:         172.21.2.184:44904
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.3.101:33969
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.63:38563
distributed.worker - INFO -          Listening to:    tcp://172.21.3.63:38563
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.97:36406
distributed.worker - INFO -          Listening to:    tcp://172.21.3.97:36406
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.185:41161
distributed.worker - INFO -          Listening to:    tcp://172.21.3.51:36973
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.176:34412
distributed.worker - INFO -          Listening to:   tcp://172.21.2.176:34412
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.178:35183
distributed.worker - INFO -          Listening to:   tcp://172.21.2.178:35183
distributed.worker - INFO -          dashboard at:         172.21.2.178:32830
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.175:35549
distributed.worker - INFO -          dashboard at:          172.21.3.83:41627
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.52:33242
distributed.worker - INFO -          Listening to:    tcp://172.21.3.52:33242
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.168:42171
distributed.worker - INFO -          Listening to:   tcp://172.21.2.168:42171
distributed.worker - INFO -          dashboard at:         172.21.2.168:42412
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.186:44736
distributed.worker - INFO -          Listening to:   tcp://172.21.2.186:44736
distributed.worker - INFO -          dashboard at:         172.21.2.186:34306
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.55:38865
distributed.worker - INFO -          Listening to:    tcp://172.21.3.55:38865
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.58:37527
distributed.worker - INFO -          Listening to:    tcp://172.21.3.58:37527
distributed.worker - INFO -          dashboard at:          172.21.3.58:40780
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.99:43321
distributed.worker - INFO -          Listening to:    tcp://172.21.3.99:43321
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.171:44540
distributed.worker - INFO -          Listening to:   tcp://172.21.2.171:44540
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.41:38136
distributed.worker - INFO -          Listening to:    tcp://172.21.3.41:38136
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.76:44745
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.46:39476
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.56:40904
distributed.worker - INFO -          Listening to:    tcp://172.21.3.56:40904
distributed.worker - INFO -          dashboard at:          172.21.3.56:42513
distributed.worker - INFO -          Listening to:    tcp://172.21.3.64:35431
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.72:42303
distributed.worker - INFO -          Listening to:    tcp://172.21.3.72:42303
distributed.worker - INFO -          Listening to:    tcp://172.21.3.80:35561
distributed.worker - INFO -          dashboard at:          172.21.3.74:45054
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.174:33313
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.170:44451
distributed.worker - INFO -          dashboard at:         172.21.2.183:38847
distributed.worker - INFO -          Listening to:   tcp://172.21.3.100:46317
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.180:39198
distributed.worker - INFO -          Listening to:   tcp://172.21.2.180:39198
distributed.worker - INFO -          dashboard at:          172.21.3.40:38008
distributed.worker - INFO -          dashboard at:          172.21.3.61:39712
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.63:35516
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.97:34604
distributed.worker - INFO -          Listening to:   tcp://172.21.2.185:41161
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.167:41694
distributed.worker - INFO -          dashboard at:          172.21.3.51:44170
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.182:41950
distributed.worker - INFO -          Listening to:   tcp://172.21.2.182:41950
distributed.worker - INFO -          dashboard at:         172.21.2.176:34091
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.178:36872
distributed.worker - INFO -          Listening to:   tcp://172.21.2.178:36872
distributed.worker - INFO -          dashboard at:         172.21.2.178:42147
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.2.175:35549
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.52:42506
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.55:41679
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.91:34703
distributed.worker - INFO -          Listening to:    tcp://172.21.3.91:34703
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.99:33148
distributed.worker - INFO -          dashboard at:         172.21.2.171:45915
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.177:37653
distributed.worker - INFO -          Listening to:   tcp://172.21.2.177:37653
distributed.worker - INFO -          dashboard at:          172.21.3.41:36238
distributed.worker - INFO -          Listening to:    tcp://172.21.3.76:44745
distributed.worker - INFO -          Listening to:    tcp://172.21.3.46:39476
distributed.worker - INFO -          dashboard at:          172.21.3.64:34823
distributed.worker - INFO -          dashboard at:          172.21.3.72:38955
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.173:44664
distributed.worker - INFO -          dashboard at:          172.21.3.80:35169
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.174:33313
distributed.worker - INFO -          Listening to:   tcp://172.21.2.170:44451
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.98:37486
distributed.worker - INFO -          Listening to:    tcp://172.21.3.98:37486
distributed.worker - INFO -          dashboard at:         172.21.3.100:37105
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.63:33749
distributed.worker - INFO -          Listening to:    tcp://172.21.3.97:34604
distributed.worker - INFO -          dashboard at:         172.21.2.185:46663
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.175:44096
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.181:40256
distributed.worker - INFO -          Listening to:   tcp://172.21.2.181:40256
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.168:39276
distributed.worker - INFO -          Listening to:   tcp://172.21.2.168:39276
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.91:38200
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.171:42797
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.76:42995
distributed.worker - INFO -          dashboard at:          172.21.3.46:42650
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.57:33113
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.2.174:42653
distributed.worker - INFO -          dashboard at:         172.21.2.174:44980
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.170:43881
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.166:38422
distributed.worker - INFO -          Listening to:   tcp://172.21.2.166:38422
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.101:33534
distributed.worker - INFO -          Listening to:   tcp://172.21.3.101:33534
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.97:40259
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.185:40382
distributed.worker - INFO -          Listening to:   tcp://172.21.2.167:41694
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.82:40669
distributed.worker - INFO -          Listening to:    tcp://172.21.3.82:40669
distributed.worker - INFO -          dashboard at:          172.21.3.82:35775
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.44:37992
distributed.worker - INFO -          Listening to:    tcp://172.21.3.44:37992
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.164:41149
distributed.worker - INFO -          Listening to:   tcp://172.21.2.164:41149
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.81:37631
distributed.worker - INFO -          Listening to:    tcp://172.21.3.81:37631
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.91:36597
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.99:36206
distributed.worker - INFO -          Listening to:   tcp://172.21.2.171:42797
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.46:44345
distributed.worker - INFO -          Listening to:    tcp://172.21.3.46:44345
distributed.worker - INFO -          dashboard at:          172.21.3.46:37878
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.56:35254
distributed.worker - INFO -          Listening to:    tcp://172.21.3.56:35254
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.64:42985
distributed.worker - INFO -          Listening to:    tcp://172.21.3.64:42985
distributed.worker - INFO -          dashboard at:          172.21.3.64:36762
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.179:36071
distributed.worker - INFO -          Listening to:   tcp://172.21.2.179:36071
distributed.worker - INFO -          dashboard at:         172.21.2.179:45152
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.49:33017
distributed.worker - INFO -          Listening to:    tcp://172.21.3.49:33017
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.2.174:43220
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.98:34762
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.2.180:39789
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8d05qm57
distributed.worker - INFO -          dashboard at:         172.21.3.101:46299
distributed.worker - INFO -          Listening to:    tcp://172.21.3.63:33749
distributed.worker - INFO -          dashboard at:          172.21.3.97:42171
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.65:37046
distributed.worker - INFO -          dashboard at:         172.21.2.167:33519
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.51:44364
distributed.worker - INFO -          dashboard at:         172.21.2.182:33790
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.93:34827
distributed.worker - INFO -          Listening to:    tcp://172.21.3.93:34827
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-isrli280
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.186:39935
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.91:36597
distributed.worker - INFO -          dashboard at:          172.21.3.91:36192
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.171:38608
distributed.worker - INFO -          dashboard at:         172.21.2.177:35712
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3wagviwp
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.162:38686
distributed.worker - INFO -          Listening to:   tcp://172.21.2.162:38686
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.2.173:44664
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.62:46692
distributed.worker - INFO -          Listening to:    tcp://172.21.3.62:46692
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9dr6e2dl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.47:38756
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.166:36108
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.54:39999
distributed.worker - INFO -          Listening to:    tcp://172.21.3.54:39999
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.98:38827
distributed.worker - INFO -       Start worker at:   tcp://172.21.3.100:41317
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.60:33992
distributed.worker - INFO -          Listening to:    tcp://172.21.3.60:33992
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hyb687mf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3cttlaeh
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.165:44163
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.50:41817
distributed.worker - INFO -          Listening to:    tcp://172.21.3.50:41817
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.176:36747
distributed.worker - INFO -          Listening to:   tcp://172.21.2.176:36747
distributed.worker - INFO -          dashboard at:         172.21.2.176:44092
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.163:45345
distributed.worker - INFO -          Listening to:   tcp://172.21.2.163:45345
distributed.worker - INFO -          dashboard at:         172.21.2.163:37572
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.44:34492
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r55wnwqf
distributed.worker - INFO -          dashboard at:         172.21.2.181:41387
distributed.worker - INFO -          dashboard at:         172.21.2.168:43162
distributed.worker - INFO -          dashboard at:         172.21.2.164:43727
distributed.worker - INFO -          dashboard at:          172.21.3.81:45872
distributed.worker - INFO -          Listening to:   tcp://172.21.2.186:39935
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aoaqseog
distributed.worker - INFO -          Listening to:    tcp://172.21.3.99:36206
distributed.worker - INFO -          dashboard at:          172.21.3.99:46789
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j53psv0c
distributed.worker - INFO -          dashboard at:         172.21.2.173:40401
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.59:41561
distributed.worker - INFO -          Listening to:    tcp://172.21.3.59:41561
distributed.worker - INFO -          dashboard at:          172.21.3.59:46298
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gg65os0o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ebjszvxi
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eyx79q1v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-etf3nbu3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.184:33374
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cik6oadv
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.167:37243
distributed.worker - INFO -          Listening to:    tcp://172.21.3.51:44364
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.96:34611
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.83:45151
distributed.worker - INFO -          Listening to:    tcp://172.21.3.83:45151
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_4fnb1v8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a9occfm7
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.169:36465
distributed.worker - INFO -          dashboard at:          172.21.3.56:46221
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9z7y1vds
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.179:44529
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kd_md16i
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.57:39016
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2mqcq980
distributed.worker - INFO -          dashboard at:          172.21.3.49:38924
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.45:40872
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.47:35678
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.98:38827
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-37ojisfo
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.39:42145
distributed.worker - INFO -          Listening to:    tcp://172.21.3.39:42145
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.40:44581
distributed.worker - INFO -          Listening to:    tcp://172.21.3.40:44581
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.61:43383
distributed.worker - INFO -          Listening to:    tcp://172.21.3.61:43383
distributed.worker - INFO -          dashboard at:          172.21.3.61:44382
distributed.worker - INFO -          Listening to:   tcp://172.21.2.184:33374
distributed.worker - INFO -          dashboard at:         172.21.2.184:40669
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zhad7yvh
distributed.worker - INFO -          dashboard at:          172.21.3.63:38749
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.65:37046
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.93:40600
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.53:33499
distributed.worker - INFO -          Listening to:    tcp://172.21.3.53:33499
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cis_r3j1
distributed.worker - INFO -          dashboard at:          172.21.3.83:33126
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.52:33723
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.181:44221
distributed.worker - INFO -          Listening to:   tcp://172.21.2.181:44221
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.186:37292
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.55:37884
distributed.worker - INFO -          Listening to:    tcp://172.21.3.55:37884
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.58:33704
distributed.worker - INFO -          Listening to:    tcp://172.21.3.58:33704
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x8cnisd7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.162:46679
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.57:39016
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.62:37268
distributed.worker - INFO -          Listening to:    tcp://172.21.3.62:37268
distributed.worker - INFO -          dashboard at:          172.21.3.62:39970
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.74:42272
distributed.worker - INFO -          Listening to:    tcp://172.21.3.45:40872
distributed.worker - INFO -          dashboard at:          172.21.3.45:37455
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aib312cc
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gp4bhqv1
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.166:43669
distributed.worker - INFO -          dashboard at:          172.21.3.54:38080
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.183:36172
distributed.worker - INFO -          Listening to:   tcp://172.21.2.183:36172
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.3.100:41317
distributed.worker - INFO -          dashboard at:         172.21.3.100:45637
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.60:39563
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.165:44163
distributed.worker - INFO -          Listening to:   tcp://172.21.2.185:40382
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.50:41837
distributed.worker - INFO -          Listening to:    tcp://172.21.3.50:41837
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.65:39173
distributed.worker - INFO -          Listening to:    tcp://172.21.3.65:39173
distributed.worker - INFO -          dashboard at:          172.21.3.65:43954
distributed.worker - INFO -          Listening to:   tcp://172.21.2.167:37243
distributed.worker - INFO -          dashboard at:          172.21.3.51:35178
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.82:40865
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fyfxb_ek
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.163:34477
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-acu7xgkl
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.175:45350
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.52:33723
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-46aobq3c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.58:34174
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.41:43248
distributed.worker - INFO -          Listening to:    tcp://172.21.3.41:43248
distributed.worker - INFO -          dashboard at:          172.21.3.41:33687
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.2.169:36465
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.2.179:44529
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.72:44937
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.173:34071
distributed.worker - INFO -          Listening to:   tcp://172.21.2.173:34071
distributed.worker - INFO -          dashboard at:         172.21.2.173:44944
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.80:39878
distributed.worker - INFO -          Listening to:    tcp://172.21.3.80:39878
distributed.worker - INFO -          dashboard at:          172.21.3.80:41565
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.62:46039
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.74:42272
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6riogqb0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.170:36424
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.183:37641
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-56it_pxj
distributed.worker - INFO -          dashboard at:          172.21.3.40:46539
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.165:42478
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nxcbx66s
distributed.worker - INFO -          dashboard at:          172.21.3.50:35233
distributed.worker - INFO -          dashboard at:          172.21.3.65:41808
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qfkquul6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t8ccvsse
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ayb88v9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aaidzx_9
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.44:45364
distributed.worker - INFO -          Listening to:    tcp://172.21.3.44:45364
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.96:39839
distributed.worker - INFO -          Listening to:    tcp://172.21.3.96:39839
distributed.worker - INFO -          dashboard at:          172.21.3.96:41149
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.52:34865
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z55gslpw
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.164:44400
distributed.worker - INFO -          Listening to:   tcp://172.21.2.164:44400
distributed.worker - INFO -          dashboard at:         172.21.2.164:44460
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.55:36437
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xs1fvtd_
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-11duth9e
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.76:35750
distributed.worker - INFO -          Listening to:    tcp://172.21.3.76:35750
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c1w1ji6c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.169:36188
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.72:44937
distributed.worker - INFO -          dashboard at:          172.21.3.57:40097
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m289za2w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.74:35192
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.47:35678
distributed.worker - INFO -          dashboard at:          172.21.3.47:41855
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4pdm20e7
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:          172.21.3.98:33603
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.39:43917
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.180:37646
distributed.worker - INFO -          Listening to:   tcp://172.21.2.180:37646
distributed.worker - INFO -          dashboard at:         172.21.2.180:41527
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8pv5i0_i
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.185:39246
distributed.worker - INFO -          dashboard at:          172.21.3.50:44343
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.167:45315
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.82:40865
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.2.163:34477
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.53:38757
distributed.worker - INFO -          Listening to:    tcp://172.21.3.53:38757
distributed.worker - INFO -          dashboard at:          172.21.3.53:43962
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ejceqm0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.175:45350
distributed.worker - INFO -          Listening to:    tcp://172.21.3.96:34611
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.181:33530
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qp4i0nm6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lkyzd5k9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8rlfqtfe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aj0eq3dr
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.177:46637
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.76:35171
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.169:43482
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u6ih_eu6
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.2.179:41248
distributed.worker - INFO -          dashboard at:          172.21.3.72:38101
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.47:38756
distributed.worker - INFO -          dashboard at:          172.21.3.47:41442
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.2.170:36424
distributed.worker - INFO -          dashboard at:         172.21.2.170:37027
distributed.worker - INFO -          Listening to:   tcp://172.21.2.166:43669
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mjpsyxpp
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v_qwc6ir
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hcgr_j8y
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.165:43109
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.182:37861
distributed.worker - INFO -          Listening to:   tcp://172.21.2.182:37861
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.82:38320
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xry1aoot
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.53:38508
distributed.worker - INFO -          dashboard at:          172.21.3.44:38823
distributed.worker - INFO -          dashboard at:         172.21.2.175:36238
distributed.worker - INFO -          dashboard at:          172.21.3.96:42050
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1ixc3m5u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yqrrqfn7
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-21bvc4t_
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6txu2o55
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fhz6nsey
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          dashboard at:         172.21.2.166:46456
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lstg9t71
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tl1_eceb
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u8rfmspy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-46mdm69g
distributed.worker - INFO -          dashboard at:         172.21.2.182:33939
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.93:37941
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.2.163:33031
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-io81l_89
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ewphwxa9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.81:43384
distributed.worker - INFO -          Listening to:    tcp://172.21.3.81:43384
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-61w6ha2i
distributed.worker - INFO -          Listening to:   tcp://172.21.2.177:46637
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fsdd3k4d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.162:44948
distributed.worker - INFO -          Listening to:   tcp://172.21.2.162:44948
distributed.worker - INFO -          dashboard at:         172.21.2.162:37699
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.2.169:43482
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x4_b65yo
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zk05_d_h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xai_q5i6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.93:37941
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wszd9j4n
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.81:41026
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vz_7_rr9
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3wusnnxv
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.2.177:46315
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l41izmz3
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.49:35983
distributed.worker - INFO -          Listening to:    tcp://172.21.3.49:35983
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cfx1l35f
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.54:46545
distributed.worker - INFO -          Listening to:    tcp://172.21.3.54:46545
distributed.worker - INFO -          dashboard at:          172.21.3.54:37896
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yvmksr_k
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qs2sesxx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2v1f1o7g
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bp6913ma
distributed.worker - INFO -          Listening to:   tcp://172.21.2.165:43109
distributed.worker - INFO -          dashboard at:         172.21.2.165:37168
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.93:35862
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6mjqqml1
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5tk7ujf1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n50vp2s9
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q3ytp8sk
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jtiebwjc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tn6e2zce
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_37xy4km
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6nb2ohyn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5cukx1e_
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.59:39442
distributed.worker - INFO -          Listening to:    tcp://172.21.3.59:39442
distributed.worker - INFO -          dashboard at:          172.21.3.59:43867
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.49:37402
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zm1iimqy
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1nuhevv1
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f6lcy1y1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vjellj9m
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.169:41329
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8y82jnmw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3jqy6s6z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-73p46_d5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gwe4xw50
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.39:37588
distributed.worker - INFO -          Listening to:    tcp://172.21.3.39:37588
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v6lz_bn0
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ejreatdk
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fz41rq3x
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bjjyx0lg
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6909rkag
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tabl3kpp
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dr_pn1z1
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7ia9z6al
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qdur1ai2
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0p4camui
distributed.worker - INFO -          dashboard at:          172.21.3.39:41893
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lkr0fd6k
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-njghld10
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p8054yyo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-996h87zz
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wneh8050
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p1dza0xj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gmxsx935
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vps_yqiq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3vfuqyfp
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nb2favoq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dth99lx_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nmnzjugr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cu_gm4tw
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-smosrq4o
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s7wxesa2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5nh84h21
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ygryyz2d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-taqkziim
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r20g256o
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hcvgv3kh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_th3oawi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-er2gzz0i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 16.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 15.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 9.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.70s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.utils_perf - INFO - full garbage collection released 127.93 MiB from 418 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 2.21 GiB from 323 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.69 GiB from 415 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 3.38 GiB from 327 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 237 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 6.63 GiB from 252 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 197 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 22.06 MiB from 426 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 486 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 65.63 MiB from 280 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.10 GiB from 354 reference cycles (threshold: 9.54 MiB)
distributed.utils_perf - INFO - full garbage collection released 1.13 GiB from 926 reference cycles (threshold: 9.54 MiB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.54s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.92s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - WARNING - Compute Failed
Function:  execute_task
args:      ((subgraph_callable-9f325df5-27f9-4898-bf89-84dbac4f0f6f, (subgraph_callable-659a06c9-cf7b-425b-9fa9-a4c66f00b1fb, (<function concatenate_axes at 0x2b348ea88ca0>, [array([[ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ],
       ...,
       [ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ],
       [ 0.        , 19.3568307 , 27.28745393, ...,  0.        ,
         0.        ,  0.        ]]), array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., 
kwargs:    {}
Exception: "MemoryError((16386, 131104), dtype('float64'))"

distributed.comm.tcp - INFO - Connection from tcp://172.21.2.168:45746 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.2.168:60090 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.3.44:50332 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.3.44:56466 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.3.44:40432 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1278: error: *** STEP 7588887.1 ON irene1278 CANCELLED AT 2022-12-13T23:37:35 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.163:44862'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.163:35393'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.46:44460'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.175:35916'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.46:36216'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.175:45819'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.93:35999'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.93:42718'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.61:35156'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.61:44015'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.97:35451'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.97:36047'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.58:33348'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.58:41605'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.165:34991'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.165:36711'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.41:40883'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.54:41613'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.185:35979'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.65:36636'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.181:38165'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.41:46132'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.64:35938'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.74:40341'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.54:43577'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.183:41398'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.100:45502'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.39:40783'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.167:42613'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.51:36272'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.178:43523'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.53:45978'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.76:37675'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.56:42010'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.179:46641'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.74:38899'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.183:39468'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.100:40613'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.39:38613'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.63:46426'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.185:39938'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.167:42055'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.51:32906'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.178:43750'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.53:37088'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.181:35839'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.76:42852'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.56:35374'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.64:36938'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.179:45938'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.63:40391'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.180:38915'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.180:42355'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.96:33783'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.96:38276'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.186:33961'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.186:37060'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.60:36667'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.171:36943'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.60:34545'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.171:34124'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.40:33554'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.47:35461'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.47:41199'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.40:41714'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.49:40583'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.57:44535'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.49:44919'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.57:45505'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.98:38701'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.52:40131'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.72:42573'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.98:45765'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.52:43129'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.173:44409'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.173:35460'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.65:43300'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.72:37242'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.101:33709'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.101:41753'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.166:36921'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.55:42282'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.55:36932'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.166:40531'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.83:38760'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.83:44238'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.99:39512'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.99:39968'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.62:33008'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.62:39872'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.170:44537'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.170:38714'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.162:41793'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.184:43285'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.162:39383'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.184:42257'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.82:39491'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.82:33145'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.91:39403'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.91:37175'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.168:41354'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.168:43460'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.85:44347'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.85:45994'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.44:42750'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.44:41397'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.174:36230'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.174:33677'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.50:38379'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.50:44282'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.63:33749
distributed.worker - INFO - Stopping worker at tcp://172.21.3.63:38563
distributed.worker - INFO - Stopping worker at tcp://172.21.2.162:38686
distributed.worker - INFO - Stopping worker at tcp://172.21.2.162:44948
distributed.worker - INFO - Stopping worker at tcp://172.21.3.100:41317
distributed.worker - INFO - Stopping worker at tcp://172.21.3.100:46317
distributed.worker - INFO - Stopping worker at tcp://172.21.2.175:45350
distributed.worker - INFO - Stopping worker at tcp://172.21.2.175:35549
distributed.worker - INFO - Stopping worker at tcp://172.21.2.163:45345
distributed.worker - INFO - Stopping worker at tcp://172.21.2.163:34477
distributed.worker - INFO - Stopping worker at tcp://172.21.2.179:44529
distributed.worker - INFO - Stopping worker at tcp://172.21.2.179:36071
distributed.worker - INFO - Stopping worker at tcp://172.21.2.165:44163
distributed.worker - INFO - Stopping worker at tcp://172.21.2.165:43109
distributed.worker - INFO - Stopping worker at tcp://172.21.2.174:42653
distributed.worker - INFO - Stopping worker at tcp://172.21.2.174:33313
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.3.74:42272
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.176:41227'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.176:41184'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.74:38752
distributed.worker - INFO - Stopping worker at tcp://172.21.3.57:39016
distributed.worker - INFO - Stopping worker at tcp://172.21.3.57:36123
distributed.worker - INFO - Stopping worker at tcp://172.21.2.166:43669
distributed.worker - INFO - Stopping worker at tcp://172.21.2.166:38422
distributed.worker - INFO - Stopping worker at tcp://172.21.3.58:37527
distributed.worker - INFO - Stopping worker at tcp://172.21.3.91:34703
distributed.worker - INFO - Stopping worker at tcp://172.21.3.58:33704
distributed.worker - INFO - Stopping worker at tcp://172.21.3.91:36597
distributed.worker - INFO - Stopping worker at tcp://172.21.3.76:35750
distributed.worker - INFO - Stopping worker at tcp://172.21.3.76:44745
distributed.worker - INFO - Stopping worker at tcp://172.21.3.60:33992
distributed.worker - INFO - Stopping worker at tcp://172.21.2.181:40256
distributed.worker - INFO - Stopping worker at tcp://172.21.2.171:44540
distributed.worker - INFO - Stopping worker at tcp://172.21.2.181:44221
distributed.worker - INFO - Stopping worker at tcp://172.21.2.171:42797
distributed.worker - INFO - Stopping worker at tcp://172.21.3.40:44581
distributed.worker - INFO - Stopping worker at tcp://172.21.3.60:33048
distributed.worker - INFO - Stopping worker at tcp://172.21.3.40:44489
distributed.worker - INFO - Stopping worker at tcp://172.21.3.64:35431
distributed.worker - INFO - Stopping worker at tcp://172.21.3.64:42985
distributed.worker - INFO - Stopping worker at tcp://172.21.3.50:41817
distributed.worker - INFO - Stopping worker at tcp://172.21.3.50:41837
distributed.worker - INFO - Stopping worker at tcp://172.21.2.184:43626
distributed.worker - INFO - Stopping worker at tcp://172.21.2.184:33374
distributed.worker - INFO - Stopping worker at tcp://172.21.2.170:44451
distributed.worker - INFO - Stopping worker at tcp://172.21.2.170:36424
distributed.worker - INFO - Stopping worker at tcp://172.21.2.186:39935
distributed.worker - INFO - Stopping worker at tcp://172.21.3.41:38136
distributed.worker - INFO - Stopping worker at tcp://172.21.2.186:44736
distributed.worker - INFO - Stopping worker at tcp://172.21.3.41:43248
distributed.worker - INFO - Stopping worker at tcp://172.21.3.97:34604
distributed.worker - INFO - Stopping worker at tcp://172.21.3.97:36406
distributed.worker - INFO - Stopping worker at tcp://172.21.2.183:36172
distributed.worker - INFO - Stopping worker at tcp://172.21.2.183:42108
distributed.worker - INFO - Stopping worker at tcp://172.21.2.185:40382
distributed.worker - INFO - Stopping worker at tcp://172.21.2.185:41161
distributed.worker - INFO - Stopping worker at tcp://172.21.3.82:40669
distributed.worker - INFO - Stopping worker at tcp://172.21.3.82:40865
distributed.worker - INFO - Stopping worker at tcp://172.21.3.98:37486
distributed.worker - INFO - Stopping worker at tcp://172.21.2.167:37243
distributed.worker - INFO - Stopping worker at tcp://172.21.3.98:38827
distributed.worker - INFO - Stopping worker at tcp://172.21.2.167:41694
distributed.worker - INFO - Stopping worker at tcp://172.21.3.44:45364
distributed.worker - INFO - Stopping worker at tcp://172.21.3.61:43383
distributed.worker - INFO - Stopping worker at tcp://172.21.3.44:37992
distributed.worker - INFO - Stopping worker at tcp://172.21.3.61:39201
distributed.worker - INFO - Stopping worker at tcp://172.21.3.56:40904
distributed.worker - INFO - Stopping worker at tcp://172.21.3.56:35254
distributed.worker - INFO - Stopping worker at tcp://172.21.3.62:46692
distributed.worker - INFO - Stopping worker at tcp://172.21.3.62:37268
distributed.worker - INFO - Stopping worker at tcp://172.21.3.96:34611
distributed.worker - INFO - Stopping worker at tcp://172.21.3.46:44345
distributed.worker - INFO - Stopping worker at tcp://172.21.2.180:37646
distributed.worker - INFO - Stopping worker at tcp://172.21.3.46:39476
distributed.worker - INFO - Stopping worker at tcp://172.21.2.180:39198
distributed.worker - INFO - Stopping worker at tcp://172.21.3.96:39839
distributed.worker - INFO - Stopping worker at tcp://172.21.3.47:35678
distributed.worker - INFO - Stopping worker at tcp://172.21.3.47:38756
distributed.worker - INFO - Stopping worker at tcp://172.21.2.176:36747
distributed.worker - INFO - Stopping worker at tcp://172.21.2.176:34412
distributed.worker - INFO - Stopping worker at tcp://172.21.2.178:35183
distributed.worker - INFO - Stopping worker at tcp://172.21.2.178:36872
distributed.worker - INFO - Stopping worker at tcp://172.21.3.55:38865
distributed.worker - INFO - Stopping worker at tcp://172.21.3.93:37941
distributed.worker - INFO - Stopping worker at tcp://172.21.3.55:37884
distributed.worker - INFO - Stopping worker at tcp://172.21.3.93:34827
distributed.worker - INFO - Stopping worker at tcp://172.21.3.83:45151
distributed.worker - INFO - Stopping worker at tcp://172.21.3.83:46798
distributed.worker - INFO - Stopping worker at tcp://172.21.3.49:35983
distributed.worker - INFO - Stopping worker at tcp://172.21.3.49:33017
distributed.worker - INFO - Stopping worker at tcp://172.21.3.72:44937
distributed.worker - INFO - Stopping worker at tcp://172.21.3.72:42303
distributed.worker - INFO - Stopping worker at tcp://172.21.3.39:37588
distributed.worker - INFO - Stopping worker at tcp://172.21.2.173:44664
distributed.worker - INFO - Stopping worker at tcp://172.21.3.39:42145
distributed.worker - INFO - Stopping worker at tcp://172.21.3.51:44364
distributed.worker - INFO - Stopping worker at tcp://172.21.2.173:34071
distributed.worker - INFO - Stopping worker at tcp://172.21.3.51:36973
distributed.worker - INFO - Stopping worker at tcp://172.21.3.54:39999
distributed.worker - INFO - Stopping worker at tcp://172.21.3.54:46545
distributed.worker - INFO - Stopping worker at tcp://172.21.3.52:33242
distributed.worker - INFO - Stopping worker at tcp://172.21.3.99:43321
distributed.worker - INFO - Stopping worker at tcp://172.21.3.52:33723
distributed.worker - INFO - Stopping worker at tcp://172.21.3.99:36206
distributed.worker - INFO - Stopping worker at tcp://172.21.3.53:38757
distributed.worker - INFO - Stopping worker at tcp://172.21.3.101:40066
distributed.worker - INFO - Stopping worker at tcp://172.21.3.101:33534
distributed.worker - INFO - Stopping worker at tcp://172.21.3.65:39173
distributed.worker - INFO - Stopping worker at tcp://172.21.2.168:42171
distributed.worker - INFO - Stopping worker at tcp://172.21.3.65:37046
distributed.worker - INFO - Stopping worker at tcp://172.21.2.168:39276
distributed.worker - INFO - Stopping worker at tcp://172.21.3.53:33499
distributed.worker - INFO - Stopping worker at tcp://172.21.3.85:46409
distributed.worker - INFO - Stopping worker at tcp://172.21.3.85:36641
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55108 parent=54904 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55107 parent=54905 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49167 parent=48962 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10310 parent=10106 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49166 parent=48961 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74409 parent=74204 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35083 parent=34881 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35082 parent=34882 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5317 parent=5115 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10311 parent=10105 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5316 parent=5116 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81195 parent=80975 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81196 parent=80976 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=837 parent=537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12301 parent=12094 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12302 parent=12095 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=836 parent=538 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20466 parent=20263 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76850 parent=76647 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62837 parent=62636 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46878 parent=46674 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46877 parent=46673 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24868 parent=24664 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24867 parent=24665 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82747 parent=82545 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7733 parent=7525 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62536 parent=62332 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8547 parent=8342 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62838 parent=62635 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62538 parent=62331 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81601 parent=81384 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79636 parent=79433 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94369 parent=94170 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52868 parent=52662 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94370 parent=94169 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82748 parent=82544 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86394 parent=86189 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=52869 parent=52663 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69027 parent=68822 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8546 parent=8343 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79637 parent=79434 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7732 parent=7526 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81600 parent=81383 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45166 parent=44962 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55467 parent=55261 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41341 parent=41139 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=86393 parent=86190 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69028 parent=68823 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41343 parent=41138 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45165 parent=44961 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55468 parent=55262 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48006 parent=47801 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5796 parent=5594 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5797 parent=5595 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48005 parent=47802 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93996 parent=93792 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93995 parent=93793 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25787 parent=25583 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25788 parent=25582 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=78626 parent=78421 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72868 parent=72661 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=78625 parent=78422 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61810 parent=61606 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19589 parent=19386 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84006 parent=83802 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19588 parent=19387 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84005 parent=83803 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19037 parent=18822 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19036 parent=18823 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56103 parent=55902 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56104 parent=55903 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72867 parent=72662 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6991 parent=6788 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6992 parent=6789 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61396 parent=61190 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61394 parent=61191 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72161 parent=71957 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25836 parent=25624 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25837 parent=25623 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61809 parent=61605 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70608 parent=70402 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44066 parent=43862 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70609 parent=70403 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45833 parent=45625 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45834 parent=45626 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38723 parent=38519 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83266 parent=83062 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83267 parent=83063 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72160 parent=71958 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51219 parent=51014 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=51218 parent=51013 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31557 parent=31349 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31556 parent=31350 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74408 parent=74203 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38722 parent=38518 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44697 parent=44492 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44696 parent=44493 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54767 parent=54562 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77218 parent=77015 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68433 parent=68231 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68434 parent=68230 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2711 parent=2497 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30172 parent=29961 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30171 parent=29960 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20467 parent=20264 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76849 parent=76648 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44065 parent=43863 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56120 parent=55919 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=56121 parent=55920 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.177:43656'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.177:42149'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.182:38433'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.81:40940'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.164:33461'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.182:44602'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.81:45575'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.164:39037'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.45:43694'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.45:36070'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.169:34470'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.169:35096'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.80:33135'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.80:35925'
distributed.worker - INFO - Stopping worker at tcp://172.21.2.164:44400
distributed.worker - INFO - Stopping worker at tcp://172.21.2.177:46637
distributed.worker - INFO - Stopping worker at tcp://172.21.2.164:41149
distributed.worker - INFO - Stopping worker at tcp://172.21.2.177:37653
distributed.worker - INFO - Stopping worker at tcp://172.21.3.81:37631
distributed.worker - INFO - Stopping worker at tcp://172.21.3.45:40872
distributed.worker - INFO - Stopping worker at tcp://172.21.3.81:43384
distributed.worker - INFO - Stopping worker at tcp://172.21.3.45:46107
distributed.worker - INFO - Stopping worker at tcp://172.21.2.182:37861
distributed.worker - INFO - Stopping worker at tcp://172.21.2.182:41950
distributed.worker - INFO - Stopping worker at tcp://172.21.2.169:36465
distributed.worker - INFO - Stopping worker at tcp://172.21.2.169:43482
distributed.worker - INFO - Stopping worker at tcp://172.21.3.80:39878
distributed.worker - INFO - Stopping worker at tcp://172.21.3.80:35561
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.59:44064'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.59:41649'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.59:41561
distributed.worker - INFO - Stopping worker at tcp://172.21.3.59:39442
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8074c0)

Current thread 0x00002b7517201b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5884c0)

Current thread 0x00002ab805c2cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd0b4c0)

Current thread 0x00002ad9a573db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e5b4c0)

Current thread 0x00002b6798ad1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d624c0)

Current thread 0x00002b41fc0ebb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xeef4c0)

Current thread 0x00002ac2a6e9cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x47f4c0)

Current thread 0x00002afbac994b80 (most recent call first):
<no Python frame>
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.81:57648 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.81:57670 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.45:39624 remote=tcp://172.21.2.160:8786>: Stream is closed
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.182:42692 remote=tcp://172.21.2.160:8786>: Stream is closed
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.45:39582 remote=tcp://172.21.2.160:8786>: Stream is closed
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.182:42690 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.169:51474 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.169:51452 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.177:43668 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.177:43714 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.80:59200 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.80:59346 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.59:32892 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.59:60914 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.164:37054 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.2.164:37104 remote=tcp://172.21.2.160:8786>: Stream is closed
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73650 parent=73449 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10399 parent=10091 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44442 parent=44235 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44444 parent=44234 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96883 parent=96677 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10398 parent=10090 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96882 parent=96676 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44187 parent=43984 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36764 parent=36563 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73651 parent=73448 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44188 parent=43985 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97911 parent=97694 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97912 parent=97693 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36765 parent=36562 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28835 parent=28621 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28839 parent=28620 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a0f4c0)

Current thread 0x00002ba252fa2b80 (most recent call first):
<no Python frame>
srun: error: irene1433: task 92: Aborted (core dumped)
srun: error: irene1299: task 40: Aborted (core dumped)
srun: error: irene1292: task 26: Aborted (core dumped)
srun: error: irene1454: task 111: Aborted (core dumped)
srun: error: irene1298: task 39: Aborted (core dumped)
srun: error: irene1419: task 65: Aborted (core dumped)
srun: error: irene1424: task 75: Aborted (core dumped)
srun: error: irene1290: task 23: Aborted (core dumped)
