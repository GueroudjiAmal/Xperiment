distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.20:33979'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.20:33634'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.168:45715'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.168:41659'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.174:44698'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.174:44250'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.184:46585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.184:44864'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.175:36643'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.175:39331'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.88:40375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.88:33905'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.87:42021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.87:39322'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.170:42744'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.181:41218'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.170:40022'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.181:41371'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.169:44326'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.0.169:41923'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.174:42477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.174:46239'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.200:39585'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.200:42470'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.175:43843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.175:41246'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.177:37775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.177:45393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:38493'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:33829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.171:45081'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.171:39678'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.191:32917'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.191:44013'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.180:39499'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.180:34708'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.184:35681'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.184:39853'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.192:38430'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.192:34140'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.198:37856'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.198:43092'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.193:35484'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.193:44448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.199:34658'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.196:35125'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.196:46116'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.199:39445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.197:33766'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.197:46634'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.187:39353'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.187:36524'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.179:42601'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.179:45922'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.194:37062'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.194:37277'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.166:38101'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.166:39110'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.201:45576'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.201:46543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.170:46240'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.170:46069'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.168:40789'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.168:37438'
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.168:37953
distributed.worker - INFO -          Listening to:   tcp://172.21.0.168:37953
distributed.worker - INFO -          dashboard at:         172.21.0.168:39647
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x3i62ij9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.168:44807
distributed.worker - INFO -          Listening to:   tcp://172.21.0.168:44807
distributed.worker - INFO -          dashboard at:         172.21.0.168:36064
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qv4oowa5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.20:45424
distributed.worker - INFO -          Listening to:    tcp://172.21.1.20:45424
distributed.worker - INFO -          dashboard at:          172.21.1.20:39425
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4o6tbquc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.20:42499
distributed.worker - INFO -          Listening to:    tcp://172.21.1.20:42499
distributed.worker - INFO -          dashboard at:          172.21.1.20:35823
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4m8eu3ob
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.184:46715
distributed.worker - INFO -          Listening to:   tcp://172.21.0.184:46715
distributed.worker - INFO -          dashboard at:         172.21.0.184:35722
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.184:40323
distributed.worker - INFO -          Listening to:   tcp://172.21.0.184:40323
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.0.184:40472
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nafia5fk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mqi3yqut
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.169:45435
distributed.worker - INFO -          Listening to:   tcp://172.21.0.169:45435
distributed.worker - INFO -          dashboard at:         172.21.0.169:35514
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9r04ojbt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.169:35337
distributed.worker - INFO -          Listening to:   tcp://172.21.0.169:35337
distributed.worker - INFO -          dashboard at:         172.21.0.169:41930
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ii85hvdz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:42648
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:42648
distributed.worker - INFO -          dashboard at:          172.21.1.88:45297
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-da9catzl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.88:33837
distributed.worker - INFO -          Listening to:    tcp://172.21.1.88:33837
distributed.worker - INFO -          dashboard at:          172.21.1.88:41314
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4a6iot7d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.174:40200
distributed.worker - INFO -          Listening to:   tcp://172.21.0.174:40200
distributed.worker - INFO -          dashboard at:         172.21.0.174:46346
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p0k7w32i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.170:46678
distributed.worker - INFO -          Listening to:   tcp://172.21.0.170:46678
distributed.worker - INFO -          dashboard at:         172.21.0.170:44009
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-10ja3lbg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.170:39150
distributed.worker - INFO -          Listening to:   tcp://172.21.0.170:39150
distributed.worker - INFO -          dashboard at:         172.21.0.170:41544
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b8dvq1tx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.174:37616
distributed.worker - INFO -          Listening to:   tcp://172.21.0.174:37616
distributed.worker - INFO -          dashboard at:         172.21.0.174:40267
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gky94e7y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.87:39547
distributed.worker - INFO -          Listening to:    tcp://172.21.1.87:39547
distributed.worker - INFO -          dashboard at:          172.21.1.87:33567
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t5avq6t3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.1.87:36384
distributed.worker - INFO -          Listening to:    tcp://172.21.1.87:36384
distributed.worker - INFO -          dashboard at:          172.21.1.87:40773
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1j3o3ax1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.181:44791
distributed.worker - INFO -          Listening to:   tcp://172.21.0.181:44791
distributed.worker - INFO -          dashboard at:         172.21.0.181:43595
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hhjmrru6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.181:38782
distributed.worker - INFO -          Listening to:   tcp://172.21.0.181:38782
distributed.worker - INFO -          dashboard at:         172.21.0.181:44346
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t8d6ut8s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.175:38602
distributed.worker - INFO -          Listening to:   tcp://172.21.0.175:38602
distributed.worker - INFO -          dashboard at:         172.21.0.175:45411
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ft81mfgg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.0.175:39182
distributed.worker - INFO -          Listening to:   tcp://172.21.0.175:39182
distributed.worker - INFO -          dashboard at:         172.21.0.175:34410
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1rakl9bi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.180:32909
distributed.worker - INFO -          Listening to:   tcp://172.21.8.180:32909
distributed.worker - INFO -          dashboard at:         172.21.8.180:36512
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m2k8cyoi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.180:37240
distributed.worker - INFO -          Listening to:   tcp://172.21.8.180:37240
distributed.worker - INFO -          dashboard at:         172.21.8.180:46193
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-calid__s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.175:32809
distributed.worker - INFO -          Listening to:   tcp://172.21.8.175:32809
distributed.worker - INFO -          dashboard at:         172.21.8.175:40242
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lqsp468t
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.175:39141
distributed.worker - INFO -          Listening to:   tcp://172.21.8.175:39141
distributed.worker - INFO -          dashboard at:         172.21.8.175:45005
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u4ql9dpq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.171:37020
distributed.worker - INFO -          Listening to:   tcp://172.21.8.171:37020
distributed.worker - INFO -          dashboard at:         172.21.8.171:40455
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-djesixpe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.171:35018
distributed.worker - INFO -          Listening to:   tcp://172.21.8.171:35018
distributed.worker - INFO -          dashboard at:         172.21.8.171:36005
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vntlvtl9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.191:43769
distributed.worker - INFO -          Listening to:   tcp://172.21.8.191:43769
distributed.worker - INFO -          dashboard at:         172.21.8.191:43497
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g4l7hzpu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.191:38781
distributed.worker - INFO -          Listening to:   tcp://172.21.8.191:38781
distributed.worker - INFO -          dashboard at:         172.21.8.191:34830
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-28m1m3fa
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.177:43845
distributed.worker - INFO -          Listening to:   tcp://172.21.8.177:43845
distributed.worker - INFO -          dashboard at:         172.21.8.177:39347
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9oy1fdv1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.177:34571
distributed.worker - INFO -          Listening to:   tcp://172.21.8.177:34571
distributed.worker - INFO -          dashboard at:         172.21.8.177:41304
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tf8etl5n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.200:44958
distributed.worker - INFO -          Listening to:   tcp://172.21.8.200:44958
distributed.worker - INFO -          dashboard at:         172.21.8.200:34786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wv14tq80
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.200:32938
distributed.worker - INFO -          Listening to:   tcp://172.21.8.200:32938
distributed.worker - INFO -          dashboard at:         172.21.8.200:43875
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ml3ebb2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:35964
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:35964
distributed.worker - INFO -          dashboard at:         172.21.8.242:46839
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g5wg23ks
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:36551
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:36551
distributed.worker - INFO -          dashboard at:         172.21.8.242:39889
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mdtbfuno
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.174:44642
distributed.worker - INFO -          Listening to:   tcp://172.21.8.174:44642
distributed.worker - INFO -          dashboard at:         172.21.8.174:35396
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-odlwbiqd
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.174:43704
distributed.worker - INFO -          Listening to:   tcp://172.21.8.174:43704
distributed.worker - INFO -          dashboard at:         172.21.8.174:37663
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yrdzdu7x
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.193:46800
distributed.worker - INFO -          Listening to:   tcp://172.21.8.193:46800
distributed.worker - INFO -          dashboard at:         172.21.8.193:42730
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_2yiynsm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.201:43541
distributed.worker - INFO -          Listening to:   tcp://172.21.8.201:43541
distributed.worker - INFO -          dashboard at:         172.21.8.201:43232
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nfh4xxsl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.201:35824
distributed.worker - INFO -          Listening to:   tcp://172.21.8.201:35824
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.193:43214
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.166:39154
distributed.worker - INFO -          Listening to:   tcp://172.21.8.166:39154
distributed.worker - INFO -          dashboard at:         172.21.8.201:40050
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.8.166:45594
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.8.193:43214
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.193:37903
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yim_v5gy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7acjir7y
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rdmn1txi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.166:43780
distributed.worker - INFO -          Listening to:   tcp://172.21.8.166:43780
distributed.worker - INFO -          dashboard at:         172.21.8.166:36068
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kq9aatum
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.194:35500
distributed.worker - INFO -          Listening to:   tcp://172.21.8.194:35500
distributed.worker - INFO -          dashboard at:         172.21.8.194:40535
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4zuc0jat
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.194:35392
distributed.worker - INFO -          Listening to:   tcp://172.21.8.194:35392
distributed.worker - INFO -          dashboard at:         172.21.8.194:34386
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w_k23qrh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.196:44320
distributed.worker - INFO -          Listening to:   tcp://172.21.8.196:44320
distributed.worker - INFO -          dashboard at:         172.21.8.196:45238
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.199:39324
distributed.worker - INFO -          Listening to:   tcp://172.21.8.199:39324
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gi4dofll
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.184:35440
distributed.worker - INFO -          Listening to:   tcp://172.21.8.184:35440
distributed.worker - INFO -          dashboard at:         172.21.8.199:38886
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.184:33614
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ek0364x3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9k4a8tqu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.199:39568
distributed.worker - INFO -          Listening to:   tcp://172.21.8.199:39568
distributed.worker - INFO -          dashboard at:         172.21.8.199:44444
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-st2pdeeb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.168:41142
distributed.worker - INFO -          Listening to:   tcp://172.21.8.168:41142
distributed.worker - INFO -          dashboard at:         172.21.8.168:34555
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.184:42283
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.196:43552
distributed.worker - INFO -          Listening to:   tcp://172.21.8.196:43552
distributed.worker - INFO -          Listening to:   tcp://172.21.8.184:42283
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO -          dashboard at:         172.21.8.184:44131
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.168:44406
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.8.196:35705
distributed.worker - INFO -          Listening to:   tcp://172.21.8.168:44406
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.168:45071
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s35p1lie
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wqepg57z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yuhfplka
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.198:33076
distributed.worker - INFO -          Listening to:   tcp://172.21.8.198:33076
distributed.worker - INFO -          dashboard at:         172.21.8.198:35207
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sm3lyoun
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z3mvopf7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.192:44275
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.198:39066
distributed.worker - INFO -          Listening to:   tcp://172.21.8.198:39066
distributed.worker - INFO -          dashboard at:         172.21.8.198:36378
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.192:37765
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iqc7d2dq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.192:44275
distributed.worker - INFO -          dashboard at:         172.21.8.192:43342
distributed.worker - INFO -          Listening to:   tcp://172.21.8.192:37765
distributed.worker - INFO -          dashboard at:         172.21.8.192:45967
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_l7jcfcy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-44bt06yj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.197:33504
distributed.worker - INFO -          Listening to:   tcp://172.21.8.197:33504
distributed.worker - INFO -          dashboard at:         172.21.8.197:39861
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.197:43215
distributed.worker - INFO -          Listening to:   tcp://172.21.8.197:43215
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.197:36755
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yip3o59c
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.170:44224
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ky0ruyd2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.170:44224
distributed.worker - INFO -          dashboard at:         172.21.8.170:41601
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.170:44739
distributed.worker - INFO -          Listening to:   tcp://172.21.8.170:44739
distributed.worker - INFO -          dashboard at:         172.21.8.170:40657
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-muaz17g0
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vvprgydi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.187:33653
distributed.worker - INFO -          Listening to:   tcp://172.21.8.187:33653
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.187:39579
distributed.worker - INFO -          dashboard at:         172.21.8.187:34584
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.8.187:39579
distributed.worker - INFO -          dashboard at:         172.21.8.187:41340
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-26flbvuq
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_j80ood2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.179:39512
distributed.worker - INFO -          Listening to:   tcp://172.21.8.179:39512
distributed.worker - INFO -          dashboard at:         172.21.8.179:43029
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-usyqzd5p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.179:46340
distributed.worker - INFO -          Listening to:   tcp://172.21.8.179:46340
distributed.worker - INFO -          dashboard at:         172.21.8.179:36250
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rlec5ivz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.165:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.89s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.57s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.27s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.15s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.26s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1008: error: *** STEP 7588874.2 ON irene1008 CANCELLED AT 2022-12-13T22:26:51 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.166:39110'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.166:38101'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.169:41923'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.169:44326'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.174:44250'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.184:44864'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.174:44698'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.184:46585'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.175:39331'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.181:41218'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.175:43843'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.170:40022'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.175:36643'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.170:42744'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.181:41371'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.20:33634'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.175:41246'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.174:42477'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.20:33979'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.168:41659'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.0.168:45715'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.191:44013'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.191:32917'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.198:37856'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.196:35125'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.171:39678'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.184:39853'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.198:43092'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.196:46116'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.194:37277'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.168:37438'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.168:40789'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.171:45081'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.184:35681'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.194:37062'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.187:36524'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.87:42021'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.177:37775'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.187:39353'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.174:46239'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.177:45393'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.199:39445'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.179:42601'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.199:34658'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.179:45922'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.200:39585'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.88:40375'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.200:42470'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.87:39322'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.180:39499'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.88:33905'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.180:34708'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.192:34140'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.170:46240'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:38493'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.170:46069'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:33829'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.201:46543'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.197:46634'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.192:38430'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.193:44448'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.201:45576'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.197:33766'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.193:35484'
distributed.worker - INFO - Stopping worker at tcp://172.21.8.166:43780
distributed.worker - INFO - Stopping worker at tcp://172.21.8.199:39568
distributed.worker - INFO - Stopping worker at tcp://172.21.8.199:39324
distributed.worker - INFO - Stopping worker at tcp://172.21.0.174:40200
distributed.worker - INFO - Stopping worker at tcp://172.21.0.169:45435
distributed.worker - INFO - Stopping worker at tcp://172.21.0.174:37616
distributed.worker - INFO - Stopping worker at tcp://172.21.0.169:35337
distributed.worker - INFO - Stopping worker at tcp://172.21.8.175:39141
distributed.worker - INFO - Stopping worker at tcp://172.21.8.175:32809
distributed.worker - INFO - Stopping worker at tcp://172.21.0.181:44791
distributed.worker - INFO - Stopping worker at tcp://172.21.8.171:37020
distributed.worker - INFO - Stopping worker at tcp://172.21.8.191:38781
distributed.worker - INFO - Stopping worker at tcp://172.21.0.175:39182
distributed.worker - INFO - Stopping worker at tcp://172.21.0.175:38602
distributed.worker - INFO - Stopping worker at tcp://172.21.8.171:35018
distributed.worker - INFO - Stopping worker at tcp://172.21.8.198:39066
distributed.worker - INFO - Stopping worker at tcp://172.21.8.174:43704
distributed.worker - INFO - Stopping worker at tcp://172.21.0.170:46678
distributed.worker - INFO - Stopping worker at tcp://172.21.1.20:42499
distributed.worker - INFO - Stopping worker at tcp://172.21.1.20:45424
distributed.worker - INFO - Stopping worker at tcp://172.21.8.179:39512
distributed.worker - INFO - Stopping worker at tcp://172.21.8.194:35392
distributed.worker - INFO - Stopping worker at tcp://172.21.8.180:37240
distributed.worker - INFO - Stopping worker at tcp://172.21.8.196:43552
distributed.worker - INFO - Stopping worker at tcp://172.21.8.201:43541
distributed.worker - INFO - Stopping worker at tcp://172.21.8.200:44958
distributed.worker - INFO - Stopping worker at tcp://172.21.8.192:37765
distributed.worker - INFO - Stopping worker at tcp://172.21.8.197:43215
distributed.worker - INFO - Stopping worker at tcp://172.21.1.87:39547
distributed.worker - INFO - Stopping worker at tcp://172.21.8.180:32909
distributed.worker - INFO - Stopping worker at tcp://172.21.8.184:42283
distributed.worker - INFO - Stopping worker at tcp://172.21.8.192:44275
distributed.worker - INFO - Stopping worker at tcp://172.21.8.196:44320
distributed.worker - INFO - Stopping worker at tcp://172.21.8.197:33504
distributed.worker - INFO - Stopping worker at tcp://172.21.8.170:44224
distributed.worker - INFO - Stopping worker at tcp://172.21.8.179:46340
distributed.worker - INFO - Stopping worker at tcp://172.21.8.193:43214
distributed.worker - INFO - Stopping worker at tcp://172.21.8.170:44739
distributed.worker - INFO - Stopping worker at tcp://172.21.8.166:39154
distributed.worker - INFO - Stopping worker at tcp://172.21.0.184:40323
distributed.worker - INFO - Stopping worker at tcp://172.21.0.184:46715
distributed.worker - INFO - Stopping worker at tcp://172.21.8.198:33076
distributed.worker - INFO - Stopping worker at tcp://172.21.8.174:44642
distributed.worker - INFO - Stopping worker at tcp://172.21.0.168:37953
distributed.worker - INFO - Stopping worker at tcp://172.21.0.181:38782
distributed.worker - INFO - Stopping worker at tcp://172.21.8.168:41142
distributed.worker - INFO - Stopping worker at tcp://172.21.0.168:44807
distributed.worker - INFO - Stopping worker at tcp://172.21.8.168:44406
distributed.worker - INFO - Stopping worker at tcp://172.21.8.184:35440
distributed.worker - INFO - Stopping worker at tcp://172.21.0.170:39150
distributed.worker - INFO - Stopping worker at tcp://172.21.1.88:42648
distributed.worker - INFO - Stopping worker at tcp://172.21.8.191:43769
distributed.worker - INFO - Stopping worker at tcp://172.21.8.187:33653
distributed.worker - INFO - Stopping worker at tcp://172.21.8.177:34571
distributed.worker - INFO - Stopping worker at tcp://172.21.8.187:39579
distributed.worker - INFO - Stopping worker at tcp://172.21.1.88:33837
distributed.worker - INFO - Stopping worker at tcp://172.21.8.200:32938
distributed.worker - INFO - Stopping worker at tcp://172.21.8.201:35824
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:36551
distributed.worker - INFO - Stopping worker at tcp://172.21.8.193:46800
distributed.worker - INFO - Stopping worker at tcp://172.21.8.194:35500
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:35964
distributed.worker - INFO - Stopping worker at tcp://172.21.1.87:36384
distributed.worker - INFO - Stopping worker at tcp://172.21.8.177:43845
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65427 parent=63503 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60206 parent=58365 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57036 parent=55187 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62726 parent=60880 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65428 parent=63504 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46144 parent=44312 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16166 parent=14349 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62725 parent=60881 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59069 parent=57200 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46142 parent=44313 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24763 parent=22925 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57037 parent=55188 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68749 parent=66904 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=11077 parent=9232 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62589 parent=60765 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=11078 parent=9231 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83255 parent=81427 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39753 parent=37904 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44228 parent=42318 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43797 parent=41970 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=767 parent=96638 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39752 parent=37905 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40916 parent=39091 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91919 parent=90069 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40917 parent=39092 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48282 parent=46450 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76366 parent=74538 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40502 parent=38678 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74309 parent=72484 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62588 parent=60766 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16639 parent=14804 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90619 parent=88795 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44229 parent=42317 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91920 parent=90070 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68989 parent=67167 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90618 parent=88796 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25580 parent=23744 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59068 parent=57199 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60207 parent=58366 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21308 parent=19470 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29536 parent=27696 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76369 parent=74537 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=765 parent=96639 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48279 parent=46451 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68992 parent=67168 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25581 parent=23745 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47305 parent=45402 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93218 parent=91368 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47303 parent=45403 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93216 parent=91367 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16164 parent=14348 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21310 parent=19469 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24761 parent=22924 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40504 parent=38679 started daemon>
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18249 parent=16416 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29537 parent=27697 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68748 parent=66905 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=16640 parent=14805 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66112 parent=64291 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74310 parent=72485 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=66113 parent=64290 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43796 parent=41971 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18248 parent=16417 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83256 parent=81428 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11fa4c0)

Current thread 0x00002b58e5228b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbb14c0)

Current thread 0x00002af1081a3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd6d4c0)

Current thread 0x00002ada719b8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf784c0)

Current thread 0x00002b40b0bf3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf154c0)

Current thread 0x00002b02264abb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b0f4c0)

Current thread 0x00002afb07a09b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xdb64c0)

Current thread 0x00002aff01d06b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x179e4c0)

Current thread 0x00002b88f21ecb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16354c0)

Current thread 0x00002b8777c18b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12d64c0)

Current thread 0x00002b0f9e840b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4674c0)

Current thread 0x00002b9915c1cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fb94c0)

Current thread 0x00002b7dc0b98b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19ad4c0)

Current thread 0x00002b98d7ca8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc234c0)

Current thread 0x00002b210a264b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23694c0)

Current thread 0x00002abf5d9cab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8924c0)

Current thread 0x00002b260437db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fdd4c0)

Current thread 0x00002b4f961f3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9594c0)

Current thread 0x00002b3f41d65b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc0e4c0)

Current thread 0x00002b8602518b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf7a4c0)

Current thread 0x00002ac6d14b7b80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1bc44c0)

Current thread 0x00002af835654b80 (most recent call first):
<no Python frame>
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5834c0)

Current thread 0x00002b05a7ac9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c904c0)

Current thread 0x00002ab71c0f4b80 (most recent call first):
<no Python frame>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
srun: error: irene2115: task 26: Aborted (core dumped)
srun: error: irene1021: task 11: Aborted (core dumped)
srun: error: irene1010: task 5: Aborted (core dumped)
srun: error: irene1014: task 7: Aborted (core dumped)
srun: error: irene2140: task 51: Aborted (core dumped)
srun: error: irene2119: task 30: Aborted (core dumped)
srun: error: irene2124: task 36: Aborted (core dumped)
srun: error: irene1008: task 0: Aborted (core dumped)
srun: error: irene2136: task 45: Aborted (core dumped)
srun: error: irene2128: task 38: Aborted (core dumped)
srun: error: irene1009: task 2: Aborted (core dumped)
srun: error: irene1113: task 14: Aborted (core dumped)
srun: error: irene2112: task 23: Aborted (core dumped)
srun: error: irene2114: task 25: Aborted (core dumped)
srun: error: irene2131: task 40: Aborted (core dumped)
srun: error: irene2186: task 62: Aborted (core dumped)
srun: error: irene2135: task 42: Aborted (core dumped)
srun: error: irene2142: task 54: Aborted (core dumped)
srun: error: irene2118: task 29: Aborted (core dumped)
srun: error: irene2121: task 32: Aborted (core dumped)
srun: error: irene2138: task 48: Aborted (core dumped)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8cf4c0)

Current thread 0x00002b5539ec2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb2c4c0)

Current thread 0x00002b24f2dbab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17d84c0)

Current thread 0x00002b03f577ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x184f4c0)

Current thread 0x00002b0686bb4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1eb34c0)

Current thread 0x00002b919b1c1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9564c0)

Current thread 0x00002ac6767ecb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9434c0)

Current thread 0x00002adf19160b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11b64c0)

Current thread 0x00002b080cf93b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22a44c0)

Current thread 0x00002ad9eefb9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8ea4c0)

Current thread 0x00002af49fdc8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x55a4c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc464c0)

Current thread 0x00002b409d1f8b80 (most recent call first):
<no Python frame>

Current thread 0x00002ba2f9fafb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20ee4c0)

Current thread 0x00002ab3163e3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd8a4c0)

Current thread 0x00002b8507010b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cb54c0)

Current thread 0x00002b733a80db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc2d4c0)

Current thread 0x00002b40ee99fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11644c0)

Current thread 0x00002b66f665db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b944c0)

Current thread 0x00002abc302f2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x232c4c0)

Current thread 0x00002b94f2a18b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10d24c0)

Current thread 0x00002adbc66d0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9594c0)

Current thread 0x00002b626743ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x220f4c0)

Current thread 0x00002b52dda83b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x56b4c0)

Current thread 0x00002b9dc175ab80 (most recent call first):
<no Python frame>
srun: error: irene2110: task 20: Aborted (core dumped)
srun: error: irene2144: task 58: Aborted (core dumped)
srun: error: irene1009: task 3: Aborted (core dumped)
srun: error: irene2140: task 50: Aborted (core dumped)
srun: error: irene2112: task 22: Aborted (core dumped)
srun: error: irene1181: task 18: Aborted (core dumped)
srun: error: irene2143: task 56: Aborted (core dumped)
srun: error: irene2136: task 44: Aborted (core dumped)
srun: error: irene1014: task 6: Aborted (core dumped)
srun: error: irene1113: task 15: Aborted (core dumped)
srun: error: irene2135: task 43: Aborted (core dumped)
srun: error: irene2114: task 24: Aborted (core dumped)
srun: error: irene2128: task 39: Aborted (core dumped)
srun: error: irene1008: task 1: Aborted (core dumped)
srun: error: irene2115: task 27: Aborted (core dumped)
srun: error: irene2121: task 33: Aborted (core dumped)
srun: error: irene2118: task 28: Aborted (core dumped)
srun: error: irene2186: task 63: Aborted (core dumped)
srun: error: irene2131: task 41: Aborted (core dumped)
srun: error: irene2124: task 37: Aborted (core dumped)
srun: error: irene1180: task 17: Aborted (core dumped)
srun: error: irene2145: task 60: Aborted (core dumped)
srun: error: irene2123: task 34: Aborted (core dumped)
srun: error: irene1024: task 13: Aborted (core dumped)
srun: error: irene2141: task 52: Aborted (core dumped)
