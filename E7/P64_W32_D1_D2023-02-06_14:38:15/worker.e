distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.248:36827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:35893'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.248:44315'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:46819'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:36498'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:46469'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:42405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:41785'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:45782'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.7:34115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:36445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.7:43500'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.39:36908'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.39:38677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:36758'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:45906'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:37732'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.253:45543'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.6:43408'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:41340'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:35554'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.13:33532'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:42864'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:35622'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.35:34262'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.35:45049'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:39873'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:39963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.253:34041'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.6:42303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.13:42303'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:38005'
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:40007
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:39626
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:40007
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:39626
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:42177
distributed.worker - INFO -          dashboard at:          172.21.3.15:35062
distributed.worker - INFO -          dashboard at:          172.21.3.18:34530
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:42177
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.18:34588
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:39826
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.248:43345
distributed.worker - INFO -          Listening to:   tcp://172.21.2.248:43345
distributed.worker - INFO -          dashboard at:         172.21.2.248:32786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:39826
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.15:33074
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xpqwv59t
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l7uohklz
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.248:44274
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ru922iaq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bm_j6u8s
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:43829
distributed.worker - INFO -          Listening to:   tcp://172.21.2.248:44274
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ne79j1jb
distributed.worker - INFO -          dashboard at:         172.21.2.248:36606
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:33421
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:43829
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           172.21.3.5:40806
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:33421
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ml4pzoj_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           172.21.3.5:33237
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-df94jb1l
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yx_brat3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.7:46883
distributed.worker - INFO -          Listening to:     tcp://172.21.3.7:46883
distributed.worker - INFO -          dashboard at:           172.21.3.7:38130
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.7:33291
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -          Listening to:     tcp://172.21.3.7:33291
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:           172.21.3.7:33110
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z1mlr4do
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tklw434v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.6:38524
distributed.worker - INFO -          Listening to:     tcp://172.21.3.6:38524
distributed.worker - INFO -          dashboard at:           172.21.3.6:34055
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:33095
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:33095
distributed.worker - INFO -          dashboard at:          172.21.3.33:35782
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:41614
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:40794
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:41614
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.38:35351
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.6:41717
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:40794
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1j4fpilq
distributed.worker - INFO -          dashboard at:          172.21.3.33:41677
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:34464
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:     tcp://172.21.3.6:41717
distributed.worker - INFO -          dashboard at:           172.21.3.6:39278
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ybrj6dpq
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:34464
distributed.worker - INFO -          dashboard at:          172.21.3.38:39023
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ei88bcw6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fjer6gor
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ngm3r7u
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nkwiso7e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.39:36190
distributed.worker - INFO -          Listening to:    tcp://172.21.3.39:36190
distributed.worker - INFO -          dashboard at:          172.21.3.39:44605
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:37687
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.39:46622
distributed.worker - INFO -          Listening to:    tcp://172.21.3.39:46622
distributed.worker - INFO -          dashboard at:          172.21.3.39:40209
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ops7ptjs
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:37687
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.17:34731
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-je4d4b6s
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:45832
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:45832
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.17:43792
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jr00953e
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9lg81ec8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:39597
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:39597
distributed.worker - INFO -          dashboard at:          172.21.3.16:33917
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e5x2n3vf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:43158
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:43158
distributed.worker - INFO -          dashboard at:          172.21.3.16:33586
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ldpkvhjw
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.253:40510
distributed.worker - INFO -          Listening to:   tcp://172.21.2.253:40510
distributed.worker - INFO -          dashboard at:         172.21.2.253:44501
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q56sp7cl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.35:45864
distributed.worker - INFO -          Listening to:    tcp://172.21.3.35:45864
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.253:36775
distributed.worker - INFO -          Listening to:   tcp://172.21.2.253:36775
distributed.worker - INFO -          dashboard at:         172.21.2.253:36914
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.35:37178
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p7o5qjl5
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dg_2yy4j
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.35:44873
distributed.worker - INFO -          Listening to:    tcp://172.21.3.35:44873
distributed.worker - INFO -          dashboard at:          172.21.3.35:40900
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iqnf529n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.13:35609
distributed.worker - INFO -          Listening to:    tcp://172.21.3.13:35609
distributed.worker - INFO -          dashboard at:          172.21.3.13:33924
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-juqcg8si
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:37767
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:38193
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:42956
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:42956
distributed.worker - INFO -          dashboard at:          172.21.3.19:38142
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.13:39761
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9p_rvhhq
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:46635
distributed.worker - INFO -          Listening to:    tcp://172.21.3.13:39761
distributed.worker - INFO -          dashboard at:          172.21.3.13:34807
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:37767
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:46635
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.34:43608
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          dashboard at:          172.21.3.19:44580
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xxsk4gxw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:38193
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.3.34:46243
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a63amfxw
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s0cuq1od
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6pzmca4j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO - Worker process 7130 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 26307 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.utils_perf - INFO - full garbage collection released 7.00 GiB from 443 reference cycles (threshold: 9.54 MiB)
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:43097
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:43097
distributed.worker - INFO -          dashboard at:          172.21.3.15:40841
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x3j3itit
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:46037
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:46037
distributed.worker - INFO -          dashboard at:          172.21.3.34:43335
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6y1vooyz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.0.229:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1364: error: *** STEP 8263177.1 ON irene1364 CANCELLED AT 2023-02-06T20:04:13 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:36498'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.13:33532'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:42405'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.13:42303'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.248:44315'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.248:36827'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:45906'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:37732'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:46819'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:35893'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:41785'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:46469'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.7:34115'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.7:43500'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.35:45049'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.35:34262'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:39963'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:36758'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:35622'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:42864'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:46635
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:42956
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:36445'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:45782'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.39:36908'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.39:38677'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:34464
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:41614
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.6:42303'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.6:43408'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.13:39761
distributed.worker - INFO - Stopping worker at tcp://172.21.3.13:35609
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:39826
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:43097
distributed.worker - INFO - Stopping worker at tcp://172.21.3.35:45864
distributed.worker - INFO - Stopping worker at tcp://172.21.3.35:44873
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:35554'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:41340'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.253:34041'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:43829
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.253:45543'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:33421
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:40007
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:42177
distributed.worker - INFO - Stopping worker at tcp://172.21.3.39:46622
distributed.worker - INFO - Stopping worker at tcp://172.21.3.39:36190
distributed.worker - INFO - Stopping worker at tcp://172.21.2.248:44274
distributed.worker - INFO - Stopping worker at tcp://172.21.2.248:43345
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:45832
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:37687
distributed.worker - INFO - Stopping worker at tcp://172.21.3.6:38524
distributed.worker - INFO - Stopping worker at tcp://172.21.3.7:33291
distributed.worker - INFO - Stopping worker at tcp://172.21.3.6:41717
distributed.worker - INFO - Stopping worker at tcp://172.21.3.7:46883
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:39597
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:43158
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:40794
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:33095
distributed.worker - INFO - Stopping worker at tcp://172.21.2.253:40510
distributed.worker - INFO - Stopping worker at tcp://172.21.2.253:36775
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:39873'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:38005'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:37767
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:46037
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.15:33076 remote=tcp://172.21.0.229:8786>: Stream is closed
distributed.worker - WARNING - Heartbeat to scheduler failed
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 205, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 1235, in heartbeat
    response = await retry_operation(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 896, in send_recv_from_rpc
    result = await send_recv(comm=comm, op=key, **kwargs)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://172.21.3.34:60602 remote=tcp://172.21.0.229:8786>: Stream is closed
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22308 parent=22202 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87479 parent=87374 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7129 parent=7025 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38330 parent=38223 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=7443 parent=7026 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22307 parent=22203 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47110 parent=47003 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47109 parent=47002 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74302 parent=74198 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38329 parent=38224 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4039 parent=3930 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74303 parent=74197 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42515 parent=42410 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32799 parent=32694 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38319 parent=38215 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38320 parent=38214 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42514 parent=42411 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=87478 parent=87375 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4038 parent=3931 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32798 parent=32693 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10913 parent=10807 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10914 parent=10806 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9249 parent=9143 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50944 parent=50840 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9250 parent=9144 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50945 parent=50839 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74405 parent=74297 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74404 parent=74298 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47052 parent=46946 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47051 parent=46947 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26306 parent=26200 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26642 parent=26201 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22bf4c0)

Current thread 0x00002af6188cab80 (most recent call first):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11f04c0)

Current thread 0x00002b01f10b2b80 (most recent call first):
<no Python frame>
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1aa64c0)

Current thread 0x00002ba1e47f6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf6d4c0)

Current thread 0x00002b8be497bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4db4c0)

Current thread 0x00002b7f62124b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c274c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8f64c0)

Current thread 0x00002b14bcd33b80 (most recent call first):
<no Python frame>
Current thread 0x00002b8b38af6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14bc4c0)

Current thread 0x00002b038c605b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13fa4c0)

Current thread 0x00002b5929946b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23b44c0)

Current thread 0x00002aeb53650b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c8c4c0)

Current thread 0x00002b51d38a3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfd64c0)

Current thread 0x00002b510340cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x103b4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15134c0)

Current thread 0x00002ae14608bb80 (most recent call first):
<no Python frame>
Current thread 0x00002b3170275b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16b14c0)

Current thread 0x00002b3ef42b8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4f94c0)

Current thread 0x00002b9673fc6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9e64c0)

Current thread 0x00002b4383941b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23414c0)

Current thread 0x00002b333476fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12c64c0)

Current thread 0x00002b5fa9307b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf8a4c0)

Current thread 0x00002aad10a33b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xee84c0)

Current thread 0x00002b15e8b6bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8354c0)

Current thread 0x00002b9e88c03b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b644c0)

Current thread 0x00002b69940f0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd444c0)

Current thread 0x00002b019904ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22d34c0)

Current thread 0x00002b873a2c8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd064c0)

Current thread 0x00002afa09e31b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13214c0)

Current thread 0x00002b9ca1bf8b80 (most recent call first):
<no Python frame>
slurmstepd-irene1403: error: Detected 1 oom-kill event(s) in StepId=8263177.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1403: task 24: Out Of Memory
slurmstepd-irene1384: error: Detected 1 oom-kill event(s) in StepId=8263177.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1408: task 30: Aborted (core dumped)
srun: error: irene1404: task 27: Aborted (core dumped)
srun: error: irene1387: task 18: Aborted (core dumped)
srun: error: irene1388: task 20: Aborted (core dumped)
srun: error: irene1385: task 14: Aborted (core dumped)
srun: error: irene1369: task 3: Aborted (core dumped)
srun: error: irene1382: task 10: Aborted (core dumped)
srun: error: irene1374: task 4: Aborted (core dumped)
srun: error: irene1402: task 22: Aborted (core dumped)
srun: error: irene1374: task 5: Aborted (core dumped)
srun: error: irene1407: task 28: Aborted (core dumped)
srun: error: irene1404: task 26: Aborted (core dumped)
srun: error: irene1364: tasks 0-1: Aborted (core dumped)
srun: error: irene1385: task 15: Aborted (core dumped)
srun: error: irene1402: task 23: Aborted (core dumped)
srun: error: irene1369: task 2: Aborted (core dumped)
srun: error: irene1386: task 16: Aborted (core dumped)
srun: error: irene1387: task 19: Aborted (core dumped)
srun: error: irene1375: task 6: Aborted (core dumped)
srun: error: irene1382: task 11: Aborted (core dumped)
srun: error: irene1388: task 21: Aborted (core dumped)
srun: error: irene1376: task 9: Aborted (core dumped)
srun: error: irene1408: task 31: Aborted (core dumped)
