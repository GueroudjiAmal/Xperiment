distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.7:33687'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.7:35672'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:44831'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.183:37854'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.20:45612'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:38405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.22:45867'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.244:42800'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.38:36345'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.183:34734'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.20:35004'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.16:45596'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.22:39640'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.244:42555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.24:37647'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.181:46205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.181:44026'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.241:35833'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.241:32940'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.242:37509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.242:39252'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.24:35770'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.13:40060'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.13:42047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:42267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.33:45092'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:45568'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.17:38504'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:35298'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.5:42987'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.248:36575'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.248:42974'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.8:40992'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.8:39683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.26:34044'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.247:44364'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.26:34876'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.247:35844'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:41594'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.19:39747'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.6:34509'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.6:39535'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.227:40894'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.239:37267'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.227:46050'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.239:35829'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.23:45733'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.23:37953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:39772'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.15:37929'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.35:45393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.35:37437'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.253:40802'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:42411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.34:34947'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.253:38326'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.180:38958'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.180:35321'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:39137'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.18:43628'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.189:39385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.2.189:37492'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.21:38124'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.3.21:40112'
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.244:37894
distributed.worker - INFO -          Listening to:   tcp://172.21.2.244:37894
distributed.worker - INFO -          dashboard at:         172.21.2.244:41790
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h2v34ij4
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.244:33864
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.244:33864
distributed.worker - INFO -          dashboard at:         172.21.2.244:37284
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u2ytnrwy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.22:40870
distributed.worker - INFO -          Listening to:    tcp://172.21.3.22:40870
distributed.worker - INFO -          dashboard at:          172.21.3.22:42793
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ln884qa
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.24:44404
distributed.worker - INFO -          Listening to:    tcp://172.21.3.24:44404
distributed.worker - INFO -          dashboard at:          172.21.3.24:44544
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gylu_f6h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.22:43118
distributed.worker - INFO -          Listening to:    tcp://172.21.3.22:43118
distributed.worker - INFO -          dashboard at:          172.21.3.22:39504
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zt3g0x7f
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.24:38473
distributed.worker - INFO -          Listening to:    tcp://172.21.3.24:38473
distributed.worker - INFO -          dashboard at:          172.21.3.24:35266
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-26t9xqua
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.183:40799
distributed.worker - INFO -          Listening to:   tcp://172.21.2.183:40799
distributed.worker - INFO -          dashboard at:         172.21.2.183:35371
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hbul76sz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.183:45477
distributed.worker - INFO -          Listening to:   tcp://172.21.2.183:45477
distributed.worker - INFO -          dashboard at:         172.21.2.183:43508
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-odwmj2ny
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.181:35305
distributed.worker - INFO -          Listening to:   tcp://172.21.2.181:35305
distributed.worker - INFO -          dashboard at:         172.21.2.181:33930
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.181:45345
distributed.worker - INFO -          Listening to:   tcp://172.21.2.181:45345
distributed.worker - INFO -          dashboard at:         172.21.2.181:36322
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n2hjuibd
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gmrsnebl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:39117
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:39117
distributed.worker - INFO -          dashboard at:          172.21.3.38:33202
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_vq9ka3i
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.38:42241
distributed.worker - INFO -          Listening to:    tcp://172.21.3.38:42241
distributed.worker - INFO -          dashboard at:          172.21.3.38:44607
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l9nw547j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.7:33915
distributed.worker - INFO -          Listening to:     tcp://172.21.3.7:33915
distributed.worker - INFO -          dashboard at:           172.21.3.7:41581
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hsvqcj0a
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.20:40085
distributed.worker - INFO -          Listening to:    tcp://172.21.3.20:40085
distributed.worker - INFO -          dashboard at:          172.21.3.20:43330
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4lv17j9x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.7:41394
distributed.worker - INFO -          Listening to:     tcp://172.21.3.7:41394
distributed.worker - INFO -          dashboard at:           172.21.3.7:44172
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jaiubukf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.20:34969
distributed.worker - INFO -          Listening to:    tcp://172.21.3.20:34969
distributed.worker - INFO -          dashboard at:          172.21.3.20:46168
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qf7km_c2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.241:45306
distributed.worker - INFO -          Listening to:   tcp://172.21.2.241:45306
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.241:33114
distributed.worker - INFO -          dashboard at:         172.21.2.241:34928
distributed.worker - INFO -          Listening to:   tcp://172.21.2.241:33114
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -          dashboard at:         172.21.2.241:39299
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:37851
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:37851
distributed.worker - INFO -          dashboard at:          172.21.3.16:40862
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cvrs4yxo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vmgi_3vt
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.16:45121
distributed.worker - INFO -          Listening to:    tcp://172.21.3.16:45121
distributed.worker - INFO -          dashboard at:          172.21.3.16:42389
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-muri70b4
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mkgf6yga
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.239:44171
distributed.worker - INFO -          Listening to:   tcp://172.21.2.239:44171
distributed.worker - INFO -          dashboard at:         172.21.2.239:34021
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.239:44487
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.2.239:44487
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.2.239:41817
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2dn2_4pc
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1pezafuz
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.6:38757
distributed.worker - INFO -          Listening to:     tcp://172.21.3.6:38757
distributed.worker - INFO -          dashboard at:           172.21.3.6:46450
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.6:44223
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:     tcp://172.21.3.6:44223
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rtd10zrw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.247:40781
distributed.worker - INFO -          dashboard at:           172.21.3.6:42426
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.247:40781
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-945xxbr8
distributed.worker - INFO -          dashboard at:         172.21.2.247:39382
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.247:33380
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.247:33380
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.247:46146
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lwvugyad
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-at_fs0px
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:43222
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:43222
distributed.worker - INFO -          dashboard at:          172.21.3.19:41095
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:41629
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-plsm7hqe
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:41629
distributed.worker - INFO -          dashboard at:          172.21.3.19:34368
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_w3yo7cx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:40066
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:40066
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.33:44716
distributed.worker - INFO -          dashboard at:          172.21.3.33:44871
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.33:44716
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.33:37422
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-op141ja1
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yyps9ckz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:38619
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:38619
distributed.worker - INFO -          dashboard at:          172.21.3.17:33410
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:33012
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:33012
distributed.worker - INFO -          dashboard at:          172.21.3.17:43691
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c290e_jv
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hqu0o2_4
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.242:43321
distributed.worker - INFO -          Listening to:   tcp://172.21.2.242:43321
distributed.worker - INFO -          dashboard at:         172.21.2.242:38259
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.242:42069
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -          Listening to:   tcp://172.21.2.242:42069
distributed.worker - INFO -          dashboard at:         172.21.2.242:45852
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4f99vd1d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ny8d95n2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.35:43198
distributed.worker - INFO -          Listening to:    tcp://172.21.3.35:43198
distributed.worker - INFO -          dashboard at:          172.21.3.35:36649
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.35:36859
distributed.worker - INFO -          Listening to:    tcp://172.21.3.35:36859
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.35:37425
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9gmyqt6r
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1kbnek4e
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.8:41772
distributed.worker - INFO -          Listening to:     tcp://172.21.3.8:41772
distributed.worker - INFO -          dashboard at:           172.21.3.8:33664
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.8:41946
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:     tcp://172.21.3.8:41946
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d8_i6wft
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:           172.21.3.8:46083
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8eim9gdn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:32978
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:32978
distributed.worker - INFO -          dashboard at:          172.21.3.15:40696
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:33948
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:33948
distributed.worker - INFO -          dashboard at:          172.21.3.15:40211
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vsl1fs1x
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qa_b1qdv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:39750
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:39750
distributed.worker - INFO -          dashboard at:          172.21.3.18:35490
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.18:33164
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rtvont6p
distributed.worker - INFO -          Listening to:    tcp://172.21.3.18:33164
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.3.18:33208
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u1nczp5z
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:40387
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:40387
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.26:44231
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:39855
distributed.worker - INFO -          dashboard at:          172.21.3.34:46636
distributed.worker - INFO -          Listening to:    tcp://172.21.3.26:44231
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:39855
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -          dashboard at:          172.21.3.26:33964
distributed.worker - INFO -          dashboard at:           172.21.3.5:45865
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v19sx203
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-11dl2gmp
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.26:34518
distributed.worker - INFO -          Listening to:    tcp://172.21.3.26:34518
distributed.worker - INFO -          dashboard at:          172.21.3.26:42944
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-70pgas3b
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:35974
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:35974
distributed.worker - INFO -          dashboard at:          172.21.3.34:39634
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:     tcp://172.21.3.5:46271
distributed.worker - INFO -          Listening to:     tcp://172.21.3.5:46271
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:           172.21.3.5:45712
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n1cg3m3c
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-poisgqyj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-34v0edir
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.248:40526
distributed.worker - INFO -          Listening to:   tcp://172.21.2.248:40526
distributed.worker - INFO -          dashboard at:         172.21.2.248:44900
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7j1vbzbf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.248:46073
distributed.worker - INFO -          Listening to:   tcp://172.21.2.248:46073
distributed.worker - INFO -          dashboard at:         172.21.2.248:35552
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fpo0uoe3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.23:39914
distributed.worker - INFO -          Listening to:    tcp://172.21.3.23:39914
distributed.worker - INFO -          dashboard at:          172.21.3.23:38194
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.180:40760
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.180:35927
distributed.worker - INFO -          Listening to:   tcp://172.21.2.180:40760
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.23:45529
distributed.worker - INFO -          Listening to:    tcp://172.21.3.23:45529
distributed.worker - INFO -          Listening to:   tcp://172.21.2.180:35927
distributed.worker - INFO -          dashboard at:         172.21.2.180:40340
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.180:37991
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -          dashboard at:          172.21.3.23:36174
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iciif2yl
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5m9e65kr
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aj3fzh2m
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bw4v3rnp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.253:42368
distributed.worker - INFO -          Listening to:   tcp://172.21.2.253:42368
distributed.worker - INFO -          dashboard at:         172.21.2.253:41062
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.253:36861
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.227:44704
distributed.worker - INFO -          Listening to:   tcp://172.21.2.227:44704
distributed.worker - INFO -          dashboard at:         172.21.2.227:42042
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hfe61t24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.189:42772
distributed.worker - INFO -          Listening to:   tcp://172.21.2.253:36861
distributed.worker - INFO -          dashboard at:         172.21.2.253:40526
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r2ms_n4n
distributed.worker - INFO -          Listening to:   tcp://172.21.2.189:42772
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.227:40917
distributed.worker - INFO -          dashboard at:         172.21.2.189:39886
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.189:45456
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4d0i3kb1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.2.189:45456
distributed.worker - INFO -          Listening to:   tcp://172.21.2.227:40917
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.2.227:46828
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hfnfh1__
distributed.worker - INFO -          dashboard at:         172.21.2.189:37607
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z9chq7sr
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ynj4l04z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.13:39525
distributed.worker - INFO -          Listening to:    tcp://172.21.3.13:39525
distributed.worker - INFO -          dashboard at:          172.21.3.13:36602
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.13:39704
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.3.13:39704
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.3.13:33094
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xvuf6nbi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1hv31une
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.21:34162
distributed.worker - INFO -          Listening to:    tcp://172.21.3.21:34162
distributed.worker - INFO -          dashboard at:          172.21.3.21:37051
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.21:42955
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:    tcp://172.21.3.21:42955
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_38nxk86
distributed.worker - INFO -          dashboard at:          172.21.3.21:34765
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qpq4fsa6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.66s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 59145 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 62199 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 93507 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 13138 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.nanny - INFO - Worker process 5262 was killed by signal 9
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.17:43482
distributed.worker - INFO -          Listening to:    tcp://172.21.3.17:43482
distributed.worker - INFO -          dashboard at:          172.21.3.17:34031
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nagpumy7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.15:45204
distributed.worker - INFO -          Listening to:    tcp://172.21.3.15:45204
distributed.worker - INFO -          dashboard at:          172.21.3.15:41285
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ie58e4ok
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.19:33970
distributed.worker - INFO -          Listening to:    tcp://172.21.3.19:33970
distributed.worker - INFO -          dashboard at:          172.21.3.19:33600
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-892syxr8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.24:38729
distributed.worker - INFO -          Listening to:    tcp://172.21.3.24:38729
distributed.worker - INFO -          dashboard at:          172.21.3.24:40208
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u_tuo6qj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - INFO - Worker process 83917 was killed by signal 9
distributed.worker - INFO -       Start worker at:   tcp://172.21.2.253:36891
distributed.worker - INFO -          Listening to:   tcp://172.21.2.253:36891
distributed.worker - INFO -          dashboard at:         172.21.2.253:36077
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3321blvr
distributed.worker - INFO - -------------------------------------------------
distributed.nanny - WARNING - Restarting worker
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.3.34:39411
distributed.worker - INFO -          Listening to:    tcp://172.21.3.34:39411
distributed.worker - INFO -          dashboard at:          172.21.3.34:39744
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g7pxta09
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.2.163:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1296: error: *** STEP 8263198.1 ON irene1296 CANCELLED AT 2023-02-07T00:56:18 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.22:39640'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.22:45867'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.241:35833'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.241:32940'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:43628'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.227:46050'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.244:42555'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.248:36575'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.244:42800'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.248:42974'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.7:35672'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.227:40894'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.18:39137'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.7:33687'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.239:35829'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.239:37267'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.181:46205'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.181:44026'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.26:34876'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.26:34044'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.21:38124'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.21:40112'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.23:45733'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.23:37953'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:44831'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.38:36345'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.35:45393'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.35:37437'
distributed.worker - INFO - Stopping worker at tcp://172.21.2.181:35305
distributed.worker - INFO - Stopping worker at tcp://172.21.2.181:45345
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.20:45612'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.20:35004'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:45596'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.16:38405'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.242:39252'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.242:37509'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.183:37854'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.183:34734'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.8:40992'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.8:39683'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.180:35321'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.180:38958'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:34947'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.34:42411'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.189:39385'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.189:37492'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.247:44364'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.247:35844'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:39411
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:37929'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.253:40802'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.34:35974
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.15:39772'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.2.253:38326'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.24:35770'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.24:37647'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:32978
distributed.worker - INFO - Stopping worker at tcp://172.21.3.15:45204
distributed.worker - INFO - Stopping worker at tcp://172.21.2.253:36891
distributed.worker - INFO - Stopping worker at tcp://172.21.2.253:36861
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:42987'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.5:35298'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.6:34509'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.6:39535'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.13:40060'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.13:42047'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:45092'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.33:42267'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:39747'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.19:41594'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:38504'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.3.17:45568'
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:44716
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:41629
distributed.worker - INFO - Stopping worker at tcp://172.21.3.33:40066
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:33012
distributed.worker - INFO - Stopping worker at tcp://172.21.3.17:43482
distributed.worker - INFO - Stopping worker at tcp://172.21.3.19:33970
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:39750
distributed.worker - INFO - Stopping worker at tcp://172.21.3.18:33164
distributed.worker - INFO - Stopping worker at tcp://172.21.2.244:37894
distributed.worker - INFO - Stopping worker at tcp://172.21.2.244:33864
distributed.worker - INFO - Stopping worker at tcp://172.21.2.183:40799
distributed.worker - INFO - Stopping worker at tcp://172.21.2.183:45477
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:45121
distributed.worker - INFO - Stopping worker at tcp://172.21.3.16:37851
distributed.worker - INFO - Stopping worker at tcp://172.21.2.239:44487
distributed.worker - INFO - Stopping worker at tcp://172.21.2.239:44171
distributed.worker - INFO - Stopping worker at tcp://172.21.3.21:34162
distributed.worker - INFO - Stopping worker at tcp://172.21.3.21:42955
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:39855
distributed.worker - INFO - Stopping worker at tcp://172.21.3.5:46271
distributed.worker - INFO - Stopping worker at tcp://172.21.3.13:39525
distributed.worker - INFO - Stopping worker at tcp://172.21.3.13:39704
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:42241
distributed.worker - INFO - Stopping worker at tcp://172.21.3.38:39117
distributed.worker - INFO - Stopping worker at tcp://172.21.2.242:42069
distributed.worker - INFO - Stopping worker at tcp://172.21.2.242:43321
distributed.worker - INFO - Stopping worker at tcp://172.21.2.189:45456
distributed.worker - INFO - Stopping worker at tcp://172.21.3.24:38473
distributed.worker - INFO - Stopping worker at tcp://172.21.3.24:38729
distributed.worker - INFO - Stopping worker at tcp://172.21.2.189:42772
distributed.worker - INFO - Stopping worker at tcp://172.21.3.8:41946
distributed.worker - INFO - Stopping worker at tcp://172.21.3.8:41772
distributed.worker - INFO - Stopping worker at tcp://172.21.2.248:46073
distributed.worker - INFO - Stopping worker at tcp://172.21.2.248:40526
distributed.worker - INFO - Stopping worker at tcp://172.21.3.22:43118
distributed.worker - INFO - Stopping worker at tcp://172.21.3.22:40870
distributed.worker - INFO - Stopping worker at tcp://172.21.2.180:35927
distributed.worker - INFO - Stopping worker at tcp://172.21.3.6:44223
distributed.worker - INFO - Stopping worker at tcp://172.21.3.7:41394
distributed.worker - INFO - Stopping worker at tcp://172.21.2.180:40760
distributed.worker - INFO - Stopping worker at tcp://172.21.3.23:39914
distributed.worker - INFO - Stopping worker at tcp://172.21.3.35:43198
distributed.worker - INFO - Stopping worker at tcp://172.21.3.6:38757
distributed.worker - INFO - Stopping worker at tcp://172.21.2.247:40781
distributed.worker - INFO - Stopping worker at tcp://172.21.3.7:33915
distributed.worker - INFO - Stopping worker at tcp://172.21.3.20:34969
distributed.worker - INFO - Stopping worker at tcp://172.21.3.23:45529
distributed.worker - INFO - Stopping worker at tcp://172.21.3.35:36859
distributed.worker - INFO - Stopping worker at tcp://172.21.2.247:33380
distributed.worker - INFO - Stopping worker at tcp://172.21.2.227:44704
distributed.worker - INFO - Stopping worker at tcp://172.21.2.227:40917
distributed.worker - INFO - Stopping worker at tcp://172.21.3.20:40085
distributed.worker - INFO - Stopping worker at tcp://172.21.2.241:45306
distributed.worker - INFO - Stopping worker at tcp://172.21.2.241:33114
distributed.worker - INFO - Stopping worker at tcp://172.21.3.26:44231
distributed.worker - INFO - Stopping worker at tcp://172.21.3.26:34518
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75989 parent=75883 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97025 parent=96920 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93549 parent=93443 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88997 parent=88890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=849 parent=743 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=850 parent=744 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97750 parent=97645 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6084 parent=5978 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45506 parent=45401 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75988 parent=75882 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32109 parent=32003 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67144 parent=67040 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73877 parent=73768 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97751 parent=97646 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95270 parent=95164 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32406 parent=32301 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93548 parent=93444 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97996 parent=97889 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10660 parent=10556 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10659 parent=10555 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8025 parent=7922 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13588 parent=13034 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=67145 parent=67039 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73876 parent=73769 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32108 parent=32004 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83444 parent=83337 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88996 parent=88891 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9469 parent=9366 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13137 parent=13033 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8026 parent=7921 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68894 parent=68786 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83443 parent=83338 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55299 parent=55194 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77466 parent=77360 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45507 parent=45400 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77465 parent=77361 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5091 parent=4985 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97024 parent=96921 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68893 parent=68787 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5090 parent=4984 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=9470 parent=9365 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=97995 parent=97890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95271 parent=95165 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32405 parent=32300 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55300 parent=55193 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6083 parent=5979 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70025 parent=69917 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70024 parent=69916 started daemon>
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46798 parent=46692 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46799 parent=46693 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83916 parent=83813 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5261 parent=5157 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84406 parent=83812 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62200 parent=62092 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5718 parent=5156 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62636 parent=62093 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93944 parent=93402 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35041 parent=34937 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59144 parent=59038 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35042 parent=34938 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59586 parent=59039 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93508 parent=93401 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45529 parent=45425 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45530 parent=45424 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
slurmstepd-irene1384: error: Detected 1 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1386: error: Detected 2437 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1369: error: Detected 1 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1403: error: Detected 2 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x164b4c0)

Current thread 0x00002b248eeb3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17c44c0)

Current thread 0x00002b2eb26f1b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb4c4c0)

Current thread 0x00002af431b02b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b574c0)

Current thread 0x00002abe115ffb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xabd4c0)

Current thread 0x00002af5cf6a0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23114c0)

Current thread 0x00002af64becab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x183d4c0)

Current thread 0x00002b5af875bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x63a4c0)

Current thread 0x00002b387da0cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb184c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe694c0)

Current thread 0x00002b26bc0ecb80 (most recent call first):
<no Python frame>
Current thread 0x00002abc2b03bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a284c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6454c0)

Current thread 0x00002b37bccfdb80 (most recent call first):
<no Python frame>
Current thread 0x00002b343f197b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ae34c0)

Current thread 0x00002af2d8a25b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x66c4c0)

Current thread 0x00002add9a52bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14a54c0)

Current thread 0x00002b0237133b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18b24c0)

Current thread 0x00002af251050b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f594c0)

Current thread 0x00002b14b1dfab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x71d4c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e574c0)


Current thread 0x00002b42739cfb80 (most recent call first):
<no Python frame>
Current thread 0x00002b587abb2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbc84c0)

Current thread 0x00002b57058bbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xdbb4c0)

Current thread 0x00002ba224f33b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd4f4c0)

Current thread 0x00002abba4e23b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11a24c0)

Current thread 0x00002b9088321b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e2b4c0)

Current thread 0x00002b502df5cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5ce4c0)

Current thread 0x00002b388462cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16b84c0)

Current thread 0x00002b2027748b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x155f4c0)

Current thread 0x00002ae6295c2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b794c0)

Current thread 0x00002b8fab7e8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1dfa4c0)

Current thread 0x00002b35d3074b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20524c0)

Current thread 0x00002afae689fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7134c0)

Current thread 0x00002baa8cb35b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cb44c0)

Current thread 0x00002b10703dbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x116d4c0)

Current thread 0x00002b6aa3445b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbe74c0)

Current thread 0x00002b0df4406b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12374c0)

Current thread 0x00002aba25091b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22fd4c0)

Current thread 0x00002adb08f3db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9ba4c0)

Current thread 0x00002baa35e7fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20ce4c0)

Current thread 0x00002afa077d0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23354c0)

Current thread 0x00002b7a2abbcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f944c0)

Current thread 0x00002ab3174ddb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x223f4c0)

Current thread 0x00002b6d1476eb80 (most recent call first):
<no Python frame>
srun: error: irene1384: task 34: Out Of Memory
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a514c0)

Current thread 0x00002b2576119b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8724c0)

Current thread 0x00002ab8d52a6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd324c0)

Current thread 0x00002b4359813b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7694c0)

Current thread 0x00002b0291625b80 (most recent call first):
<no Python frame>
slurmstepd-irene1393: error: Detected 1 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
slurmstepd-irene1388: error: Detected 1 oom-kill event(s) in StepId=8263198.1 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: irene1355: tasks 10-11: Aborted (core dumped)
srun: error: irene1385: tasks 36-37: Aborted (core dumped)
srun: error: irene1343: tasks 8-9: Aborted (core dumped)
srun: error: irene1297: tasks 2-3: Aborted (core dumped)
srun: error: irene1376: tasks 28-29: Aborted (core dumped)
srun: error: irene1389: tasks 44-45: Aborted (core dumped)
srun: error: irene1404: tasks 60-61: Aborted (core dumped)
srun: error: irene1299: tasks 4-5: Aborted (core dumped)
srun: error: irene1382: tasks 32-33: Aborted (core dumped)
srun: error: irene1377: tasks 30-31: Aborted (core dumped)
srun: error: irene1392: tasks 50-51: Aborted (core dumped)
srun: error: irene1390: tasks 46-47: Aborted (core dumped)
srun: error: irene1374: tasks 24-25: Aborted (core dumped)
srun: error: irene1407: task 63: Aborted (core dumped)
srun: error: irene1296: task 0: Aborted (core dumped)
srun: error: irene1358: task 14: Aborted (core dumped)
srun: error: irene1363: task 18: Aborted (core dumped)
srun: error: irene1360: task 16: Aborted (core dumped)
srun: error: irene1305: task 6: Aborted (core dumped)
srun: error: irene1364: task 21: Aborted (core dumped)
srun: error: irene1402: task 57: Aborted (core dumped)
srun: error: irene1387: task 40: Aborted (core dumped)
srun: error: irene1391: task 48: Aborted (core dumped)
srun: error: irene1375: task 27: Aborted (core dumped)
srun: error: irene1395: task 55: Aborted (core dumped)
