distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.192:43163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.218:34405'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:37300'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.180:43292'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.192:41433'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:45954'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:42794'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.180:41607'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.196:33843'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.168:34965'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.247:41495'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.181:35237'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.251:40188'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.177:46824'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:39438'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:36822'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.196:38856'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.168:41136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.247:38927'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.190:46132'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.240:38335'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.184:42960'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.184:36542'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.176:42168'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:46718'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.176:34415'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.171:33678'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:40806'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.190:43047'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.171:32771'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.217:43820'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:43000'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:45029'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.185:34383'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:35967'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.197:36773'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:43232'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.219:35466'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.197:38131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.219:37711'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.163:34436'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.198:35087'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.248:37094'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.198:46342'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.165:46589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.169:43707'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.248:34887'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.165:38994'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.233:42981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.253:44949'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.233:43561'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.253:45201'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.169:39909'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.166:41558'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.166:46557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.167:44073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.167:44953'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.221:42040'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.221:45883'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.244:42744'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.238:45164'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.214:45838'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.214:42667'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.174:36533'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.174:43196'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.238:44490'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.172:42750'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.182:34800'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.170:35869'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.182:39123'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.172:36586'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.234:33880'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.164:38684'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.220:38753'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.164:40884'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.220:35179'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.226:44254'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.202:33870'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.202:40732'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:37461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.224:38461'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.228:42466'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:34375'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.224:39589'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.239:43110'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.242:44304'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.242:38822'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.239:46704'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.243:37318'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.227:35539'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.186:39622'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.227:39478'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.241:40744'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.186:46115'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.241:42220'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.178:44135'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.215:45600'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.215:41257'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.246:35309'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.183:36555'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.246:43745'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.183:35751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.178:38597'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.162:39571'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.162:39156'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.237:37884'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.237:34765'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.191:35946'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.191:45981'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.173:45751'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.173:39506'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.225:46085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.251:43581'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.218:38477'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.177:44719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:38148'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.181:39874'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.217:38475'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.240:35445'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.185:41338'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.163:41139'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.244:45302'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.234:36370'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.170:41749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.226:36430'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.228:36731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.243:34479'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.225:33323'
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.163:41123
distributed.worker - INFO -          Listening to:   tcp://172.21.6.163:41123
distributed.worker - INFO -          dashboard at:         172.21.6.163:44050
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.214:37057
distributed.worker - INFO -          Listening to:   tcp://172.21.6.214:37057
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.163:40586
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:44284
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:44284
distributed.worker - INFO -          dashboard at:         172.21.6.214:35406
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.167:39615
distributed.worker - INFO -          Listening to:   tcp://172.21.6.167:39615
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.213:35540
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.221:34956
distributed.worker - INFO -          Listening to:   tcp://172.21.6.221:34956
distributed.worker - INFO -          dashboard at:         172.21.6.221:39355
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:41472
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:41472
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.197:45574
distributed.worker - INFO -          Listening to:   tcp://172.21.6.197:45574
distributed.worker - INFO -          dashboard at:         172.21.6.167:40315
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.218:35412
distributed.worker - INFO -          Listening to:   tcp://172.21.6.163:40586
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.221:45999
distributed.worker - INFO -          Listening to:   tcp://172.21.6.221:45999
distributed.worker - INFO -          dashboard at:         172.21.6.221:37468
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:46031
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.182:45089
distributed.worker - INFO -          Listening to:   tcp://172.21.6.182:45089
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.197:38914
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.251:37567
distributed.worker - INFO -          Listening to:   tcp://172.21.6.251:37567
distributed.worker - INFO -          Listening to:   tcp://172.21.6.218:35412
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:41262
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:41262
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.238:38320
distributed.worker - INFO -          Listening to:   tcp://172.21.6.238:38320
distributed.worker - INFO -          dashboard at:         172.21.6.238:40130
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.239:41656
distributed.worker - INFO -          Listening to:   tcp://172.21.6.239:41656
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.173:42757
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:45662
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:45662
distributed.worker - INFO -          dashboard at:         172.21.6.208:45538
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.184:40860
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.178:39829
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.227:36573
distributed.worker - INFO -          Listening to:   tcp://172.21.6.227:36573
distributed.worker - INFO -          dashboard at:         172.21.6.209:43412
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.233:42672
distributed.worker - INFO -          Listening to:   tcp://172.21.6.233:42672
distributed.worker - INFO -          dashboard at:         172.21.6.233:38792
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.180:32785
distributed.worker - INFO -          Listening to:   tcp://172.21.6.180:32785
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.183:35492
distributed.worker - INFO -          Listening to:   tcp://172.21.6.183:35492
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.246:44550
distributed.worker - INFO -          Listening to:   tcp://172.21.6.246:44550
distributed.worker - INFO -          dashboard at:         172.21.6.182:33939
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.196:39228
distributed.worker - INFO -          Listening to:   tcp://172.21.6.196:39228
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.172:46218
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.224:44746
distributed.worker - INFO -          Listening to:   tcp://172.21.6.224:44746
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.198:46843
distributed.worker - INFO -          Listening to:   tcp://172.21.6.198:46843
distributed.worker - INFO -          dashboard at:         172.21.6.198:38716
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.191:45408
distributed.worker - INFO -          Listening to:   tcp://172.21.6.191:45408
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.174:34493
distributed.worker - INFO -          Listening to:   tcp://172.21.6.174:34493
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.162:45036
distributed.worker - INFO -          Listening to:   tcp://172.21.6.162:45036
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.247:35366
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.242:42848
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.240:33691
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.237:41570
distributed.worker - INFO -          Listening to:   tcp://172.21.6.237:41570
distributed.worker - INFO -          dashboard at:         172.21.6.237:43641
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:34980
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.219:37031
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.192:44613
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.190:44000
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.165:44713
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.169:43701
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.248:36250
distributed.worker - INFO -          Listening to:   tcp://172.21.6.248:36250
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.181:35465
distributed.worker - INFO -          Listening to:   tcp://172.21.6.181:35465
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.217:33698
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.243:36236
distributed.worker - INFO -          Listening to:   tcp://172.21.6.243:36236
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.185:40513
distributed.worker - INFO -          Listening to:   tcp://172.21.6.185:40513
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.226:45398
distributed.worker - INFO -          Listening to:   tcp://172.21.6.226:45398
distributed.worker - INFO -          dashboard at:         172.21.6.226:39561
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.251:38877
distributed.worker - INFO -          Listening to:   tcp://172.21.6.251:38877
distributed.worker - INFO -          dashboard at:         172.21.6.251:36789
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.228:46150
distributed.worker - INFO -          Listening to:   tcp://172.21.6.228:46150
distributed.worker - INFO -          dashboard at:         172.21.6.218:37237
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.170:33493
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:37773
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:37773
distributed.worker - INFO -          dashboard at:         172.21.6.163:44052
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.225:37366
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:35430
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:33214
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.239:44775
distributed.worker - INFO -          Listening to:   tcp://172.21.6.239:44775
distributed.worker - INFO -          dashboard at:         172.21.6.239:33241
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:46064
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:46064
distributed.worker - INFO -          dashboard at:         172.21.6.208:41026
distributed.worker - INFO -          Listening to:   tcp://172.21.6.184:40860
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:34481
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.178:46385
distributed.worker - INFO -          Listening to:   tcp://172.21.6.178:46385
distributed.worker - INFO -          dashboard at:         172.21.6.178:36361
distributed.worker - INFO -          dashboard at:         172.21.6.227:44178
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:46031
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.176:34681
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.180:42422
distributed.worker - INFO -          dashboard at:         172.21.6.183:34594
distributed.worker - INFO -          dashboard at:         172.21.6.246:33812
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.196:36645
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.167:41434
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.172:40989
distributed.worker - INFO -          Listening to:   tcp://172.21.6.172:40989
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.224:37630
distributed.worker - INFO -          Listening to:   tcp://172.21.6.224:37630
distributed.worker - INFO -          dashboard at:         172.21.6.224:44174
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.191:41889
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.215:44118
distributed.worker - INFO -          dashboard at:         172.21.6.174:36386
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.166:45535
distributed.worker - INFO -          dashboard at:         172.21.6.162:45420
distributed.worker - INFO -          Listening to:   tcp://172.21.6.247:35366
distributed.worker - INFO -          Listening to:   tcp://172.21.6.242:42848
distributed.worker - INFO -          Listening to:   tcp://172.21.6.240:33691
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:33154
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:33154
distributed.worker - INFO -          dashboard at:         172.21.6.206:35756
distributed.worker - INFO -          Listening to:   tcp://172.21.6.219:37031
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.253:38916
distributed.worker - INFO -          Listening to:   tcp://172.21.6.253:38916
distributed.worker - INFO -          dashboard at:         172.21.6.253:41004
distributed.worker - INFO -          Listening to:   tcp://172.21.6.192:44613
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.190:45150
distributed.worker - INFO -          Listening to:   tcp://172.21.6.165:44713
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.169:46320
distributed.worker - INFO -          Listening to:   tcp://172.21.6.169:46320
distributed.worker - INFO -          dashboard at:         172.21.6.169:33385
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.202:34031
distributed.worker - INFO -          dashboard at:         172.21.6.248:44408
distributed.worker - INFO -          dashboard at:         172.21.6.181:43403
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.217:45628
distributed.worker - INFO -          Listening to:   tcp://172.21.6.217:45628
distributed.worker - INFO -          dashboard at:         172.21.6.217:46349
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.243:38421
distributed.worker - INFO -          dashboard at:         172.21.6.185:45648
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.251:41996
distributed.worker - INFO -          dashboard at:         172.21.6.228:40747
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.170:33493
distributed.worker - INFO -          dashboard at:         172.21.6.205:35862
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.225:37366
distributed.worker - INFO -          dashboard at:         172.21.6.207:33428
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:33214
distributed.worker - INFO -          dashboard at:         172.21.6.213:44819
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.238:41554
distributed.worker - INFO -          dashboard at:         172.21.6.239:46136
distributed.worker - INFO -          Listening to:   tcp://172.21.6.173:42757
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.171:34775
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.184:38817
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:44720
distributed.worker - INFO -          Listening to:   tcp://172.21.6.178:39829
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.164:38090
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a3vvzk_x
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.172:46218
distributed.worker - INFO -          dashboard at:         172.21.6.224:39096
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.168:44658
distributed.worker - INFO -          Listening to:   tcp://172.21.6.168:44658
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.220:42532
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.198:35826
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.162:40705
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.247:36861
distributed.worker - INFO -          dashboard at:         172.21.6.242:45153
distributed.worker - INFO -          dashboard at:         172.21.6.240:46254
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:34980
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.241:41731
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.219:41787
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.253:44017
distributed.worker - INFO -          dashboard at:         172.21.6.192:41191
distributed.worker - INFO -          Listening to:   tcp://172.21.6.190:45150
distributed.worker - INFO -          dashboard at:         172.21.6.165:45066
distributed.worker - INFO -          Listening to:   tcp://172.21.6.169:43701
distributed.worker - INFO -          dashboard at:         172.21.6.169:42342
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.217:33698
distributed.worker - INFO -          dashboard at:         172.21.6.243:40262
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.170:36748
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.177:36137
distributed.worker - INFO -          Listening to:   tcp://172.21.6.177:36137
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.225:45863
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:35430
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.173:41581
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:34481
distributed.worker - INFO -          dashboard at:         172.21.6.178:44380
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.209:35444
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.176:34853
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.180:46347
distributed.worker - INFO -          Listening to:   tcp://172.21.6.180:46347
distributed.worker - INFO -          Listening to:   tcp://172.21.6.164:38090
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s4hoyiax
distributed.worker - INFO -          Listening to:   tcp://172.21.6.167:41434
distributed.worker - INFO -          dashboard at:         172.21.6.172:41539
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.215:44118
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.166:34968
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.247:34128
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.186:36749
distributed.worker - INFO -          Listening to:   tcp://172.21.6.186:36749
distributed.worker - INFO -          dashboard at:         172.21.6.186:42565
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.234:40963
distributed.worker - INFO -          Listening to:   tcp://172.21.6.234:40963
distributed.worker - INFO -          dashboard at:         172.21.6.234:43879
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.219:39129
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.190:38403
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.202:34031
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.248:40519
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.185:46875
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.226:37058
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.244:39286
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hjzajauf
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.238:41554
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.171:34775
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.212:37419
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.176:34681
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-skllv6vf
distributed.worker - INFO -          dashboard at:         172.21.6.164:37380
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nq7n_k53
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.214:33304
distributed.worker - INFO -          Listening to:   tcp://172.21.6.214:33304
distributed.worker - INFO -          dashboard at:         172.21.6.214:39550
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.172:45539
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.168:44749
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.220:33037
distributed.worker - INFO -          Listening to:   tcp://172.21.6.198:35826
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.215:39287
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.166:45535
distributed.worker - INFO -          dashboard at:         172.21.6.166:34037
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.206:39273
distributed.worker - INFO -          Listening to:   tcp://172.21.6.241:41731
distributed.worker - INFO -          Listening to:   tcp://172.21.6.219:41787
distributed.worker - INFO -          Listening to:   tcp://172.21.6.253:44017
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.165:37891
distributed.worker - INFO -          Listening to:   tcp://172.21.6.165:37891
distributed.worker - INFO -          dashboard at:         172.21.6.165:41044
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.202:37939
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.243:38421
distributed.worker - INFO -          Listening to:   tcp://172.21.6.185:46875
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.177:37106
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2n72lnj7
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.207:33380
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.238:38072
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6acskdsq
distributed.worker - INFO -          dashboard at:         172.21.6.171:44338
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:44720
distributed.worker - INFO -          dashboard at:         172.21.6.212:38387
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.176:33787
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.233:38419
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bqbsmck6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_dy0cyrc
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.182:34519
distributed.worker - INFO -          Listening to:   tcp://172.21.6.182:34519
distributed.worker - INFO -          dashboard at:         172.21.6.182:35123
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-23_oyqbt
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.197:45508
distributed.worker - INFO -          Listening to:   tcp://172.21.6.197:45508
distributed.worker - INFO -          dashboard at:         172.21.6.167:37508
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.220:33037
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.191:44433
distributed.worker - INFO -          Listening to:   tcp://172.21.6.191:44433
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xo61mqba
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.162:40705
distributed.worker - INFO -          dashboard at:         172.21.6.162:41703
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.186:42421
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rn9vi_yf
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6md0ah2r
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.241:42439
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.248:40519
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.217:39022
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.226:37058
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.244:36313
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_mb1prgy
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s86gdxt0
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.177:36061
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:39035
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:39035
distributed.worker - INFO -          dashboard at:         172.21.6.205:39089
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z7y5_492
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0qffn75m
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-93fdkn_r
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gy75dmpe
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zdqmygzj
distributed.worker - INFO -          dashboard at:         172.21.6.180:44743
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.197:40021
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y3ua5np3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.220:42532
distributed.worker - INFO -          dashboard at:         172.21.6.198:45995
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e7n69rwh
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.215:37628
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.166:34968
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9i2c7220
distributed.worker - INFO -          Listening to:   tcp://172.21.6.247:36861
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.240:40187
distributed.worker - INFO -          Listening to:   tcp://172.21.6.240:40187
distributed.worker - INFO -          dashboard at:         172.21.6.240:34706
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.219:40168
distributed.worker - INFO -          dashboard at:         172.21.6.253:40983
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fjmqcec9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.181:40508
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.243:41352
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.244:39286
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6gkhy1a9
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.228:41664
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.177:37106
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.184:38529
distributed.worker - INFO -          Listening to:   tcp://172.21.6.184:38529
distributed.worker - INFO -          dashboard at:         172.21.6.184:44913
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.233:38419
distributed.worker - INFO -          dashboard at:         172.21.6.233:35762
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.164:43828
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.183:44421
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.246:38191
distributed.worker - INFO -          Listening to:   tcp://172.21.6.246:38191
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.196:39442
distributed.worker - INFO -          Listening to:   tcp://172.21.6.196:39442
distributed.worker - INFO -          dashboard at:         172.21.6.196:35426
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.220:35638
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pb184b22
distributed.worker - INFO -          dashboard at:         172.21.6.191:38285
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.174:34275
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z7lx2_am
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.237:37990
distributed.worker - INFO -          Listening to:   tcp://172.21.6.237:37990
distributed.worker - INFO -          dashboard at:         172.21.6.237:41695
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-37dwz36s
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.248:35806
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tfl8o75_
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -          dashboard at:         172.21.6.185:43170
distributed.worker - INFO -          dashboard at:         172.21.6.226:41402
distributed.worker - INFO -          dashboard at:         172.21.6.244:41584
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-63e_8t25
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.228:41664
distributed.worker - INFO -          dashboard at:         172.21.6.228:38920
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.218:33583
distributed.worker - INFO -          Listening to:   tcp://172.21.6.218:33583
distributed.worker - INFO -          dashboard at:         172.21.6.218:36003
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8t8vjmd4
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5fw9zu_a
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aj4_rd0x
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gy2sb3ma
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.227:35726
distributed.worker - INFO -          Listening to:   tcp://172.21.6.227:35726
distributed.worker - INFO -          dashboard at:         172.21.6.227:34099
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.176:34853
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.183:44421
distributed.worker - INFO -          dashboard at:         172.21.6.246:37622
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.168:45478
distributed.worker - INFO -          dashboard at:         172.21.6.220:43218
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.174:34275
distributed.worker - INFO -          dashboard at:         172.21.6.166:43404
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8agze4z0
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.242:37634
distributed.worker - INFO -          Listening to:   tcp://172.21.6.186:42421
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.234:37483
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bsrujsuc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4_07hlgz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.202:33398
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-evlla7m5
distributed.worker - INFO -          Listening to:   tcp://172.21.6.181:40508
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yhzvdlda
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ye8nu0kw
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-746qm_sx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3945j_tc
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6hch7ju1
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kqablmht
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3r5q2tl7
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.164:43828
distributed.worker - INFO -          dashboard at:         172.21.6.183:42030
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e_xuo822
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-va4hs7sv
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.215:37628
distributed.worker - INFO -          dashboard at:         172.21.6.215:40802
distributed.worker - INFO -          dashboard at:         172.21.6.174:37871
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.247:34392
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.242:37634
distributed.worker - INFO -          dashboard at:         172.21.6.186:35521
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-whsbya8q
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.241:41502
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.192:35190
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u3vjtdh4
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.170:32862
distributed.worker - INFO -          dashboard at:         172.21.6.177:39787
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.225:43635
distributed.worker - INFO -          Listening to:   tcp://172.21.6.225:43635
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_dqsbzs5
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.173:41075
distributed.worker - INFO -          Listening to:   tcp://172.21.6.173:41075
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xnodsazn
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mvr9b7wy
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uascj9ea
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6hjp4qcr
distributed.worker - INFO -          dashboard at:         172.21.6.176:35079
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y3j0xb94
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qcmp2jjj
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.168:45478
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-584aku1h
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.242:37483
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zgpam8g_
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kgn40xvi
distributed.worker - INFO -          Listening to:   tcp://172.21.6.192:35190
distributed.worker - INFO -          Listening to:   tcp://172.21.6.190:44000
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.202:33398
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.181:38680
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cq_8n94w
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.244:36313
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.6.170:32862
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_lfc_vfq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n_i_lt4k
distributed.worker - INFO -          dashboard at:         172.21.6.225:35084
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.173:34858
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6zumaf6h
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2zq1ftuf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2buf39vs
distributed.worker - INFO -          dashboard at:         172.21.6.164:34255
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gmcxewll
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ukagkce
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wjhf3gu2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z_9n6ctf
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bswc4vbm
distributed.worker - INFO -          Listening to:   tcp://172.21.6.234:37483
distributed.worker - INFO -          dashboard at:         172.21.6.234:46496
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uo8pvfg6
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x4ike5oe
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-echoloan
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.192:34345
distributed.worker - INFO -          dashboard at:         172.21.6.190:39284
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8twzcuy3
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rjn3sjda
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7ehrpbxv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c6xirhi8
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zha5xjfn
distributed.worker - INFO -          dashboard at:         172.21.6.170:37102
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pk7jd9ut
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-09o2hl_j
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wjt06kkn
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.171:42740
distributed.worker - INFO -          Listening to:   tcp://172.21.6.171:42740
distributed.worker - INFO -          dashboard at:         172.21.6.171:45827
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8yg8ocso
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dzme1q49
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xi_r2nyh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dw4vzs65
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p5f4wmge
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3hyufhhb
distributed.worker - INFO -          dashboard at:         172.21.6.168:34710
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0y4vdzxy
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-450led4o
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u6xntihf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rp3qk2dg
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-keehlggu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.241:41502
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pzoqoq2e
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fq9qh4tx
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oovvt0e3
distributed.worker - INFO -          dashboard at:         172.21.6.202:41905
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z8d_r2pd
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8t3htq9s
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kgjlojgm
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kmtoin3t
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c0bro1pg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-95jjvaqi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0uwycqja
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dgphf9jt
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dmyitaua
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j_pevivg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nlyjpd9v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rgn8yc6j
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6gb59ovi
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.244:44251
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-akhowj1n
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hou0kq8w
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ajfttiie
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dazkjff_
distributed.worker - INFO -          dashboard at:         172.21.6.241:38135
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-znaxpkj1
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t4t4r7k_
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-26o5shu2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mo3sa793
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mzn93fo1
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1hxawp2t
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-85i8bfsg
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mliv0v2o
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p4mu8kyr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hazs7tjo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bez5f2yf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bsd4big1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-smbct_6p
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xpwoqdzc
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-40em3oyn
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h9onenh6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 6.71s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.98s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.72s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.37s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.25s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.23s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.97s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.59s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.50s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.63s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.94s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.19s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - WARNING - Compute Failed
Function:  execute_task
args:      ((subgraph_callable-7ecd71da-70e7-4184-96c1-29c5dd1bd000, (subgraph_callable-516cc0cc-7bf2-4f0b-8a7c-df57c67ed682, (<function concatenate_axes at 0x2b757750bdc0>, [array([[ 0.00000000e+00,  1.83411016e+01,  2.52643341e+01, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  1.83411016e+01,  2.52643341e+01, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  1.83411016e+01,  2.52643341e+01, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       ...,
       [ 0.00000000e+00, -2.58413385e+05, -4.12286207e+05, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00, -4.55008605e+05, -6.07829193e+05, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00, -9.53949338e+05, -9.08063215e+05, ...,
         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
  
kwargs:    {}
Exception: "MemoryError((16386, 131104), dtype('float64'))"

distributed.core - INFO - Event loop was unresponsive in Worker for 3.69s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.93s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 3.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.74s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.181:56578 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.181:53206 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.248:57632 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 6.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.206:36118 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.215:45030 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.215:40836 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.215:42622 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.215:36656 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.205:44600 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.248:59682 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.181:37926 closed before handshake completed
distributed.core - INFO - Event loop was unresponsive in Worker for 3.73s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.nanny - INFO - Worker process 20245 was killed by signal 9
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.6.198:35826
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4123, in _get_data
    response = await send_recv(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.21.6.205:33852 remote=tcp://172.21.6.198:35826>: Stream is closed
distributed.worker - ERROR - Worker stream died during communication: tcp://172.21.6.198:35826
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 215, in read
    n = await stream.read_into(chunk)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 2878, in gather_dep
    response = await get_data_from_worker(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4143, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 366, in retry_operation
    return await retry(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/utils_comm.py", line 351, in retry
    return await coro()
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/worker.py", line 4123, in _get_data
    response = await send_recv(
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/core.py", line 671, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 221, in read
    convert_stream_closed_error(self, e)
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/view/lib/python3.9/site-packages/distributed/comm/tcp.py", line 128, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Ephemeral Worker->Worker for gather local=tcp://172.21.6.206:58918 remote=tcp://172.21.6.198:35826>: Stream is closed
distributed.nanny - WARNING - Restarting worker
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.226:47714 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.226:59424 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.226:48078 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.183:42714 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.183:43162 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:48806 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:35816 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:49472 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:42618 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:37148 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:47688 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.244:51274 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:49264 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:38470 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:51466 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.238:53084 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.238:53034 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.242:48600 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.242:40520 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.242:39210 closed before handshake completed
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.198:45155
distributed.worker - INFO -          Listening to:   tcp://172.21.6.198:45155
distributed.worker - INFO -          dashboard at:         172.21.6.198:36762
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tek0pay9
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.6.160:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:35168 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:45512 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:35504 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:36396 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.224:33242 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.190:43416 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.190:43840 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.190:47308 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:52462 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:42092 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:55610 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:58816 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:54934 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:56136 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:36828 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:35938 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:48326 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:53300 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:47084 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.239:42906 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.239:46288 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.239:52200 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.212:42434 closed before handshake completed
distributed.comm.tcp - INFO - Connection from tcp://172.21.6.167:41662 closed before handshake completed
slurmstepd-irene1830: error: *** STEP 8263225.2 ON irene1830 CANCELLED AT 2023-02-07T08:30:55 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.243:37318'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.171:33678'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.253:44949'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.192:43163'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.225:33323'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.171:32771'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.253:45201'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.192:41433'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.243:34479'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.225:46085'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.215:45600'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.215:41257'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:43000'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.163:34436'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.180:43292'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.168:41136'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.166:41558'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.181:35237'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.163:41139'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.233:42981'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.180:41607'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.168:34965'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.166:46557'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.234:33880'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:45029'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.241:42220'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.219:35466'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.202:40732'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.248:37094'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.181:39874'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.228:42466'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.218:34405'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.233:43561'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.183:36555'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.197:36773'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.234:36370'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.241:40744'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.219:37711'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.202:33870'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.248:34887'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.228:36731'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.183:35751'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.197:38131'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.218:38477'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.167:44073'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:39438'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.167:44953'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.242:38822'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:38148'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.242:44304'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.172:42750'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.162:39571'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.191:45981'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.162:39156'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.191:35946'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.177:44719'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.172:36586'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.177:46824'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.237:37884'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.237:34765'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.196:33843'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.196:38856'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.176:34415'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.176:42168'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:37461'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.247:38927'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:34375'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.247:41495'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.165:46589'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.184:36542'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.227:39478'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:37300'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.227:35539'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:42794'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.165:38994'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.184:42960'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.165:44713
distributed.worker - INFO - Stopping worker at tcp://172.21.6.165:37891
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.169:39909'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.182:34800'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.169:43707'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.182:39123'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.239:43110'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.186:39622'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.239:46704'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.186:46115'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.224:39589'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.224:38461'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.164:38684'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.164:40884'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.251:43581'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.220:38753'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.251:40188'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.220:35179'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.240:38335'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.240:35445'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.173:45751'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.173:39506'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:34980
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.190:46132'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.190:43047'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:33154
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:36822'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:45954'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:40806'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:46718'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.221:45883'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.221:42040'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.178:44135'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.178:38597'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.185:41338'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.185:34383'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.226:44254'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.226:36430'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.238:45164'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.238:44490'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.198:46342'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.198:35087'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.244:42744'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.244:45302'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.217:38475'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.217:43820'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.198:46843
distributed.worker - INFO - Stopping worker at tcp://172.21.6.198:45155
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.170:35869'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.170:41749'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.177:37106
distributed.worker - INFO - Stopping worker at tcp://172.21.6.197:45574
distributed.worker - INFO - Stopping worker at tcp://172.21.6.177:36137
distributed.worker - INFO - Stopping worker at tcp://172.21.6.197:45508
distributed.worker - INFO - Stopping worker at tcp://172.21.6.241:41502
distributed.worker - INFO - Stopping worker at tcp://172.21.6.241:41731
distributed.worker - INFO - Stopping worker at tcp://172.21.6.240:33691
distributed.worker - INFO - Stopping worker at tcp://172.21.6.240:40187
distributed.worker - INFO - Stopping worker at tcp://172.21.6.227:36573
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:39035
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.246:35309'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.246:43745'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:37773
distributed.worker - INFO - Stopping worker at tcp://172.21.6.227:35726
distributed.worker - INFO - Stopping worker at tcp://172.21.6.173:42757
distributed.worker - INFO - Stopping worker at tcp://172.21.6.173:41075
distributed.worker - INFO - Stopping worker at tcp://172.21.6.221:34956
distributed.worker - INFO - Stopping worker at tcp://172.21.6.242:37634
distributed.worker - INFO - Stopping worker at tcp://172.21.6.221:45999
distributed.worker - INFO - Stopping worker at tcp://172.21.6.242:42848
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:44284
distributed.worker - INFO - Stopping worker at tcp://172.21.6.192:44613
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:33214
distributed.worker - INFO - Stopping worker at tcp://172.21.6.192:35190
distributed.worker - INFO - Stopping worker at tcp://172.21.6.225:43635
distributed.worker - INFO - Stopping worker at tcp://172.21.6.171:34775
distributed.worker - INFO - Stopping worker at tcp://172.21.6.225:37366
distributed.worker - INFO - Stopping worker at tcp://172.21.6.171:42740
distributed.worker - INFO - Stopping worker at tcp://172.21.6.233:38419
distributed.worker - INFO - Stopping worker at tcp://172.21.6.233:42672
distributed.worker - INFO - Stopping worker at tcp://172.21.6.169:43701
distributed.worker - INFO - Stopping worker at tcp://172.21.6.169:46320
distributed.worker - INFO - Stopping worker at tcp://172.21.6.167:39615
distributed.worker - INFO - Stopping worker at tcp://172.21.6.167:41434
distributed.worker - INFO - Stopping worker at tcp://172.21.6.218:33583
distributed.worker - INFO - Stopping worker at tcp://172.21.6.248:36250
distributed.worker - INFO - Stopping worker at tcp://172.21.6.218:35412
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:44720
distributed.worker - INFO - Stopping worker at tcp://172.21.6.248:40519
distributed.worker - INFO - Stopping worker at tcp://172.21.6.184:38529
distributed.worker - INFO - Stopping worker at tcp://172.21.6.190:44000
distributed.worker - INFO - Stopping worker at tcp://172.21.6.184:40860
distributed.worker - INFO - Stopping worker at tcp://172.21.6.190:45150
distributed.worker - INFO - Stopping worker at tcp://172.21.6.196:39442
distributed.worker - INFO - Stopping worker at tcp://172.21.6.196:39228
distributed.worker - INFO - Stopping worker at tcp://172.21.6.191:44433
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:34481
distributed.worker - INFO - Stopping worker at tcp://172.21.6.168:44658
distributed.worker - INFO - Stopping worker at tcp://172.21.6.180:46347
distributed.worker - INFO - Stopping worker at tcp://172.21.6.180:32785
distributed.worker - INFO - Stopping worker at tcp://172.21.6.168:45478
distributed.worker - INFO - Stopping worker at tcp://172.21.6.186:36749
distributed.worker - INFO - Stopping worker at tcp://172.21.6.191:45408
distributed.worker - INFO - Stopping worker at tcp://172.21.6.186:42421
distributed.worker - INFO - Stopping worker at tcp://172.21.6.219:37031
distributed.worker - INFO - Stopping worker at tcp://172.21.6.219:41787
distributed.worker - INFO - Stopping worker at tcp://172.21.6.164:38090
distributed.worker - INFO - Stopping worker at tcp://172.21.6.164:43828
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.214:45838'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.214:42667'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:45662
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:46064
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.174:43196'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.174:36533'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:35967'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:43232'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.202:34031
distributed.worker - INFO - Stopping worker at tcp://172.21.6.202:33398
distributed.worker - INFO - Stopping worker at tcp://172.21.6.247:36861
distributed.worker - INFO - Stopping worker at tcp://172.21.6.166:45535
distributed.worker - INFO - Stopping worker at tcp://172.21.6.162:40705
distributed.worker - INFO - Stopping worker at tcp://172.21.6.166:34968
distributed.worker - INFO - Stopping worker at tcp://172.21.6.176:34681
distributed.worker - INFO - Stopping worker at tcp://172.21.6.247:35366
distributed.worker - INFO - Stopping worker at tcp://172.21.6.183:44421
distributed.worker - INFO - Stopping worker at tcp://172.21.6.182:34519
distributed.worker - INFO - Stopping worker at tcp://172.21.6.176:34853
distributed.worker - INFO - Stopping worker at tcp://172.21.6.182:45089
distributed.worker - INFO - Stopping worker at tcp://172.21.6.162:45036
distributed.worker - INFO - Stopping worker at tcp://172.21.6.226:37058
distributed.worker - INFO - Stopping worker at tcp://172.21.6.239:41656
distributed.worker - INFO - Stopping worker at tcp://172.21.6.226:45398
distributed.worker - INFO - Stopping worker at tcp://172.21.6.239:44775
distributed.worker - INFO - Stopping worker at tcp://172.21.6.183:35492
distributed.worker - INFO - Stopping worker at tcp://172.21.6.224:44746
distributed.worker - INFO - Stopping worker at tcp://172.21.6.185:40513
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:35430
distributed.worker - INFO - Stopping worker at tcp://172.21.6.224:37630
distributed.worker - INFO - Stopping worker at tcp://172.21.6.215:37628
distributed.worker - INFO - Stopping worker at tcp://172.21.6.185:46875
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:41262
distributed.worker - INFO - Stopping worker at tcp://172.21.6.172:40989
distributed.worker - INFO - Stopping worker at tcp://172.21.6.220:33037
distributed.worker - INFO - Stopping worker at tcp://172.21.6.215:44118
distributed.worker - INFO - Stopping worker at tcp://172.21.6.234:37483
distributed.worker - INFO - Stopping worker at tcp://172.21.6.237:41570
distributed.worker - INFO - Stopping worker at tcp://172.21.6.253:44017
distributed.worker - INFO - Stopping worker at tcp://172.21.6.181:35465
distributed.worker - INFO - Stopping worker at tcp://172.21.6.251:38877
distributed.worker - INFO - Stopping worker at tcp://172.21.6.172:46218
distributed.worker - INFO - Stopping worker at tcp://172.21.6.220:42532
distributed.worker - INFO - Stopping worker at tcp://172.21.6.234:40963
distributed.worker - INFO - Stopping worker at tcp://172.21.6.237:37990
distributed.worker - INFO - Stopping worker at tcp://172.21.6.253:38916
distributed.worker - INFO - Stopping worker at tcp://172.21.6.181:40508
distributed.worker - INFO - Stopping worker at tcp://172.21.6.243:36236
distributed.worker - INFO - Stopping worker at tcp://172.21.6.251:37567
distributed.worker - INFO - Stopping worker at tcp://172.21.6.228:41664
distributed.worker - INFO - Stopping worker at tcp://172.21.6.243:38421
distributed.worker - INFO - Stopping worker at tcp://172.21.6.228:46150
distributed.worker - INFO - Stopping worker at tcp://172.21.6.178:39829
distributed.worker - INFO - Stopping worker at tcp://172.21.6.178:46385
distributed.worker - INFO - Stopping worker at tcp://172.21.6.163:40586
distributed.worker - INFO - Stopping worker at tcp://172.21.6.163:41123
distributed.worker - INFO - Stopping worker at tcp://172.21.6.246:44550
distributed.worker - INFO - Stopping worker at tcp://172.21.6.246:38191
distributed.worker - INFO - Stopping worker at tcp://172.21.6.214:37057
distributed.worker - INFO - Stopping worker at tcp://172.21.6.214:33304
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:46031
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:41472
distributed.worker - INFO - Stopping worker at tcp://172.21.6.174:34493
distributed.worker - INFO - Stopping worker at tcp://172.21.6.174:34275
distributed.worker - INFO - Stopping worker at tcp://172.21.6.238:38320
distributed.worker - INFO - Stopping worker at tcp://172.21.6.238:41554
distributed.worker - INFO - Stopping worker at tcp://172.21.6.217:33698
distributed.worker - INFO - Stopping worker at tcp://172.21.6.217:45628
distributed.worker - INFO - Stopping worker at tcp://172.21.6.244:39286
distributed.worker - INFO - Stopping worker at tcp://172.21.6.244:36313
distributed.worker - INFO - Stopping worker at tcp://172.21.6.170:32862
distributed.worker - INFO - Stopping worker at tcp://172.21.6.170:33493
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54953 parent=54767 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54952 parent=54768 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62485 parent=62300 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95678 parent=95496 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40913 parent=40730 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44467 parent=44281 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95677 parent=95497 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34264 parent=34081 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73459 parent=73277 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13515 parent=13334 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62484 parent=62301 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33161 parent=32980 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73460 parent=73278 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96379 parent=96195 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22269 parent=22083 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23074 parent=22888 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59704 parent=59526 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94641 parent=94464 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=23075 parent=22886 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17257 parent=17082 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95043 parent=94855 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47058 parent=46881 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44466 parent=44282 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28535 parent=28360 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53519 parent=53337 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12067 parent=11889 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17258 parent=17083 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71246 parent=71063 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=33160 parent=32981 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43336 parent=43158 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34265 parent=34082 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34460 parent=34285 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35940 parent=35750 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=96378 parent=96194 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93163 parent=92987 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40285 parent=40104 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48674 parent=48490 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19244 parent=19059 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47982 parent=47798 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53520 parent=53338 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40286 parent=40103 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=95042 parent=94856 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22196 parent=22012 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25093 parent=24906 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74872 parent=74689 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19245 parent=19058 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74873 parent=74688 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8795 parent=8608 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68938 parent=68756 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48675 parent=48489 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59703 parent=59527 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72453 parent=72268 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91260 parent=91084 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8375 parent=8192 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79867 parent=79688 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73701 parent=73511 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40914 parent=40729 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20330 parent=20153 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17782 parent=17598 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93162 parent=92988 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=73699 parent=73512 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91259 parent=91083 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28536 parent=28359 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34459 parent=34286 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53746 parent=53570 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94640 parent=94463 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2698 parent=2497 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47988 parent=47804 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12066 parent=11890 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44072 parent=43895 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22195 parent=22013 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=22268 parent=22082 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=35939 parent=35751 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47990 parent=47805 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44073 parent=43896 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68939 parent=68757 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43337 parent=43159 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79866 parent=79687 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47567 parent=47388 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=53747 parent=53569 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76000 parent=75813 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90924 parent=90738 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28859 parent=28678 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90923 parent=90737 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8376 parent=8193 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70827 parent=70641 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28860 parent=28679 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47568 parent=47389 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=70828 parent=70642 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47059 parent=46882 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=72452 parent=72267 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82916 parent=82742 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82917 parent=82741 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8794 parent=8609 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76001 parent=75814 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55722 parent=55536 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17781 parent=17597 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20328 parent=20154 started daemon>
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57403 parent=57216 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=25094 parent=24907 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=55724 parent=55535 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57404 parent=57215 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=71247 parent=71064 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47983 parent=47797 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=2697 parent=2496 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13514 parent=13333 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32218 parent=32035 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32217 parent=32036 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34510 parent=34324 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40819 parent=40635 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34506 parent=34325 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29040 parent=28856 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40818 parent=40636 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17714 parent=17540 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29041 parent=28857 started daemon>
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17715 parent=17539 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31937 parent=31754 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=31936 parent=31753 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21722 parent=20069 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=20244 parent=20070 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88191 parent=88013 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=88192 parent=88012 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65960 parent=65784 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34696 parent=34523 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90538 parent=90354 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=34700 parent=34524 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=65959 parent=65783 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90537 parent=90355 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Traceback (most recent call last):
  File "/ccc/work/cont003/gen2224/gen2224/spack/var/spack/environments/gysela-deisa-irene-skl-draft/.spack-env/._view/whcr3wyhzgorszy3rfxsphyebtvfpqzk/lib/python3.9/threading.py", line 973, in _bootstrap_inner
slurmstepd-irene1866: error: Detected 1 oom-kill event(s) in StepId=8263225.2 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
Exception in thread Exception in thread Exception in thread Exception in thread Exception in thread srun: error: irene1866: task 57: Out Of Memory
Exception in thread Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12cf4c0)

Current thread 0x00002b1697ff4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14f74c0)

Current thread 0x00002b222a09eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xffe4c0)

Current thread 0x00002b8eb62fdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcf34c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe5b4c0)

Current thread 0x00002b40d8d78b80 (most recent call first):
<no Python frame>
Current thread 0x00002b95f5f2db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xec54c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x151d4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18df4c0)

Current thread 0x00002b0480ee6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x137b4c0)

Current thread 0x00002b90d8ebeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23454c0)

Current thread 0x00002b9407f24b80 (most recent call first):
<no Python frame>
Current thread 0x00002af9c21d4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22d94c0)

Current thread 0x00002ac487543b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x49a4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d534c0)

Current thread 0x00002ab4bc48bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16044c0)

Current thread 0x00002b8bf9415b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x208d4c0)

Current thread 0x00002b46bbd44b80 (most recent call first):
<no Python frame>
Current thread 0x00002ac4fc6bcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x134b4c0)

Current thread 0x00002b30d6c0db80 (most recent call first):
Current thread 0x00002b3dc878bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc7b4c0)

Current thread 0x00002b4218a3eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11334c0)

<no Python frame>
Current thread 0x00002b1ed81e2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20284c0)

Current thread 0x00002ac9a5c16b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe884c0)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10974c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xbbe4c0)

Current thread 0x00002b154ed96b80 (most recent call first):
<no Python frame>

Current thread 0x00002b2eabcfdb80 (most recent call first):
<no Python frame>
Current thread 0x00002b14bee7ab80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x220b4c0)

Current thread 0x00002ac13f254b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x17644c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x207b4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x170e4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8be4c0)

Current thread 0x00002b6bf9ae6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7154c0)

Current thread 0x00002b13647f0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22b64c0)

Current thread 0x00002b7842affb80 (most recent call first):
<no Python frame>
Current thread 0x00002ba35be0bb80 (most recent call first):
<no Python frame>
Current thread 0x00002ae928cebb80 (most recent call first):
<no Python frame>
Current thread 0x00002b6c3f457b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a344c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1fc34c0)

Current thread 0x00002b5ba77cdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7d24c0)

Current thread 0x00002add04b9ab80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x5024c0)

Current thread 0x00002b3dc1b11b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13fd4c0)

Current thread 0x00002ab497d7eb80 (most recent call first):
Current thread 0x00002b3ef3c74b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1dc34c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe174c0)

Current thread 0x00002ba903cefb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb544c0)

Current thread 0x00002b2ae2d1fb80 (most recent call first):
<no Python frame>
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc9f4c0)

Current thread 0x00002b3cea9e0b80 (most recent call first):
<no Python frame>
Current thread 0x00002b764ecbbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x115b4c0)

Current thread 0x00002b9603906b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14364c0)

Current thread 0x00002b76f3498b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x208e4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23a14c0)
Current thread 0x00002b95e07c4b80 (most recent call first):
<no Python frame>

Current thread 0x00002aeaafbe4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcb04c0)

Current thread 0x00002b25728e8b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22d84c0)

Current thread 0x00002b31f3bf2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfad4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9604c0)

Current thread 0x00002b37cf6e0b80 (most recent call first):
<no Python frame>
Current thread 0x00002af348dd6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19ae4c0)

Current thread 0x00002aff24a8cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e694c0)

Current thread 0x00002b7898da5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1a664c0)

Current thread 0x00002b9a9e973b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14114c0)

Current thread 0x00002b76a92fbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15244c0)

Current thread 0x00002b5c432dcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1b9b4c0)

Current thread 0x00002ae3efad2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11814c0)

Current thread 0x00002aef12611b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12a14c0)

Current thread 0x00002b861f1dbb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x15d14c0)

Current thread 0x00002b3bb1662b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x8864c0)

Current thread 0x00002abd30bb4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5814c0)

Current thread 0x00002ac05fd51b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c924c0)

Current thread 0x00002b80685c5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12984c0)

Current thread 0x00002af97c434b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20884c0)

Current thread 0x00002ba053639b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6124c0)

Current thread 0x00002ad5fc968b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d1f4c0)

Current thread 0x00002b3e04c7cb80 (most recent call first):
<no Python frame>
Exception in thread srun: error: irene1914: tasks 118-119: Aborted (core dumped)
srun: error: irene1845: tasks 28-29: Aborted (core dumped)
srun: error: irene1916: tasks 122-123: Aborted (core dumped)
srun: error: irene1865: tasks 54-55: Aborted (core dumped)
srun: error: irene1844: tasks 26-27: Aborted (core dumped)
srun: error: irene1915: tasks 120-121: Aborted (core dumped)
srun: error: irene1893: tasks 90-91: Aborted (core dumped)
srun: error: irene1870: tasks 58-59: Aborted (core dumped)
srun: error: irene1901: tasks 98-99: Aborted (core dumped)
srun: error: irene1910: tasks 112-113: Aborted (core dumped)
srun: error: irene1850: tasks 36-37: Aborted (core dumped)
Exception in thread srun: error: irene1851: task 38: Aborted (core dumped)
srun: error: irene1919: task 125: Aborted (core dumped)
srun: error: irene1907: task 106: Aborted (core dumped)
srun: error: irene1888: task 84: Aborted (core dumped)
srun: error: irene1880: task 70: Aborted (core dumped)
srun: error: irene1840: task 20: Aborted (core dumped)
srun: error: irene1895: task 95: Aborted (core dumped)
srun: error: irene1877: task 68: Aborted (core dumped)
srun: error: irene1849: task 35: Aborted (core dumped)
srun: error: irene1874: task 63: Aborted (core dumped)
srun: error: irene1854: task 45: Aborted (core dumped)
srun: error: irene1853: task 42: Aborted (core dumped)
srun: error: irene1892: task 88: Aborted (core dumped)
srun: error: irene1875: task 65: Aborted (core dumped)
srun: error: irene1836: task 12: Aborted (core dumped)
srun: error: irene1912: task 117: Aborted (core dumped)
srun: error: irene1908: task 108: Aborted (core dumped)
srun: error: irene1831: task 3: Aborted (core dumped)
srun: error: irene1858: task 46: Aborted (core dumped)
srun: error: irene1876: task 67: Aborted (core dumped)
srun: error: irene1846: task 30: Aborted (core dumped)
srun: error: irene1882: task 75: Aborted (core dumped)
srun: error: irene1852: task 41: Aborted (core dumped)
srun: error: irene1859: task 48: Aborted (core dumped)
srun: error: irene1834: task 9: Aborted (core dumped)
srun: error: irene1896: task 96: Aborted (core dumped)
srun: error: irene1833: task 6: Aborted (core dumped)
srun: error: irene1864: task 52: Aborted (core dumped)
srun: error: irene1837: task 15: Aborted (core dumped)
srun: error: irene1902: task 101: Aborted (core dumped)
srun: error: irene1887: task 83: Aborted (core dumped)
srun: error: irene1848: task 33: Aborted (core dumped)
srun: error: irene1883: task 77: Aborted (core dumped)
srun: error: irene1881: task 72: Aborted (core dumped)
srun: error: irene1886: task 80: Aborted (core dumped)
srun: error: irene1909: task 111: Aborted (core dumped)
srun: error: irene1832: task 5: Aborted (core dumped)
srun: error: irene1889: task 86: Aborted (core dumped)
srun: error: irene1841: task 23: Aborted (core dumped)
