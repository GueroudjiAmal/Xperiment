distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:45085'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.211:34421'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.204:42425'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.241:45402'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:38863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.190:46784'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.238:40590'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.233:46801'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.211:40851'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.204:45899'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.241:36814'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.205:34311'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:36388'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.239:39145'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.239:39558'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.237:40870'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.235:46557'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:44982'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.209:37621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.202:42731'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.223:38389'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.201:33611'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.238:43736'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.206:37701'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.190:45629'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.233:41281'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.207:32921'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.237:41733'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.240:45639'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.200:39650'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.235:43160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.202:41901'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.223:37824'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.201:43620'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.240:34577'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.200:44028'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.236:38978'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.243:41048'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.199:36772'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.203:36852'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.224:34411'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.227:42104'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.210:33397'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.236:37684'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.243:34346'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.199:45656'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.203:39669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.224:36645'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.227:46240'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.210:46447'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:45152'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.213:36046'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:35317'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.226:40683'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.232:36448'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:42840'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.208:36784'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.212:35369'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.234:46110'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.234:37852'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.226:36983'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.232:46518'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.225:34678'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.6.225:40028'
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.235:38861
distributed.worker - INFO -          Listening to:   tcp://172.21.6.235:38861
distributed.worker - INFO -          dashboard at:         172.21.6.235:45057
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i9iyx9qe
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:43292
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:43292
distributed.worker - INFO -          dashboard at:         172.21.6.205:41229
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.235:34798
distributed.worker - INFO -          Listening to:   tcp://172.21.6.235:34798
distributed.worker - INFO -          dashboard at:         172.21.6.235:37090
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ejae2gn9
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.203:43589
distributed.worker - INFO -          Listening to:   tcp://172.21.6.203:43589
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m5ka0ri8
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:37496
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.203:44425
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:37496
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.205:43324
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.206:41817
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.203:45340
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.236:42123
distributed.worker - INFO -          Listening to:   tcp://172.21.6.236:42123
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:35332
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:35332
distributed.worker - INFO -          Listening to:   tcp://172.21.6.205:43324
distributed.worker - INFO -          dashboard at:         172.21.6.205:36249
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.223:40348
distributed.worker - INFO -          Listening to:   tcp://172.21.6.223:40348
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.238:39377
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.227:40285
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.226:46383
distributed.worker - INFO -          dashboard at:         172.21.6.236:40662
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.200:38232
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.211:46503
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.204:36428
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.239:34402
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:39374
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.241:35676
distributed.worker - INFO -          Listening to:   tcp://172.21.6.241:35676
distributed.worker - INFO -          dashboard at:         172.21.6.241:40707
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:44923
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:44923
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.234:36816
distributed.worker - INFO -          Listening to:   tcp://172.21.6.234:36816
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.209:36425
distributed.worker - INFO -          Listening to:   tcp://172.21.6.209:36425
distributed.worker - INFO -          dashboard at:         172.21.6.209:36375
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.243:33481
distributed.worker - INFO -          Listening to:   tcp://172.21.6.243:33481
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:44914
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:44914
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:34091
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:34091
distributed.worker - INFO -          dashboard at:         172.21.6.207:34547
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.237:36598
distributed.worker - INFO -          dashboard at:         172.21.6.223:35337
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.232:36525
distributed.worker - INFO -          Listening to:   tcp://172.21.6.232:36525
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.190:38861
distributed.worker - INFO -          Listening to:   tcp://172.21.6.190:38861
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.210:36676
distributed.worker - INFO -          Listening to:   tcp://172.21.6.238:39377
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cgw7jv1t
distributed.worker - INFO -          Listening to:   tcp://172.21.6.227:40285
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.240:33550
distributed.worker - INFO -          Listening to:   tcp://172.21.6.226:46383
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.224:44436
distributed.worker - INFO -          Listening to:   tcp://172.21.6.224:44436
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.233:35969
distributed.worker - INFO -          Listening to:   tcp://172.21.6.233:35969
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.202:33926
distributed.worker - INFO -          Listening to:   tcp://172.21.6.202:33926
distributed.worker - INFO -          Listening to:   tcp://172.21.6.200:38232
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.225:41063
distributed.worker - INFO -          Listening to:   tcp://172.21.6.225:41063
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.199:34747
distributed.worker - INFO -          Listening to:   tcp://172.21.6.199:34747
distributed.worker - INFO -          dashboard at:         172.21.6.199:45612
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.201:39966
distributed.worker - INFO -          Listening to:   tcp://172.21.6.201:39966
distributed.worker - INFO -          Listening to:   tcp://172.21.6.211:46503
distributed.worker - INFO -          Listening to:   tcp://172.21.6.204:36428
distributed.worker - INFO -          Listening to:   tcp://172.21.6.239:34402
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:39374
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.208:46081
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.234:33320
distributed.worker - INFO -          dashboard at:         172.21.6.209:41544
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.243:43117
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.212:39568
distributed.worker - INFO -          Listening to:   tcp://172.21.6.212:39568
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.237:36598
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.232:36296
distributed.worker - INFO -          Listening to:   tcp://172.21.6.232:36296
distributed.worker - INFO -          dashboard at:         172.21.6.232:34015
distributed.worker - INFO -          dashboard at:         172.21.6.190:40115
distributed.worker - INFO -          Listening to:   tcp://172.21.6.210:36676
distributed.worker - INFO -          dashboard at:         172.21.6.238:45670
distributed.worker - INFO -          Listening to:   tcp://172.21.6.203:45340
distributed.worker - INFO -          dashboard at:         172.21.6.203:43898
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.227:44069
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.240:33803
distributed.worker - INFO -          dashboard at:         172.21.6.226:37099
distributed.worker - INFO -          dashboard at:         172.21.6.224:34109
distributed.worker - INFO -          dashboard at:         172.21.6.233:40853
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.202:42516
distributed.worker - INFO -          dashboard at:         172.21.6.200:36198
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.225:42682
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.201:32862
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.211:41614
distributed.worker - INFO -          dashboard at:         172.21.6.204:34894
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.239:40552
distributed.worker - INFO -          dashboard at:         172.21.6.213:36908
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.243:39621
distributed.worker - INFO -          dashboard at:         172.21.6.212:41786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.237:40420
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.232:44987
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.210:40789
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.227:46813
distributed.worker - INFO -          Listening to:   tcp://172.21.6.240:33550
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.224:46609
distributed.worker - INFO -          Listening to:   tcp://172.21.6.224:46609
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.204:38338
distributed.worker - INFO -          Listening to:   tcp://172.21.6.204:38338
distributed.worker - INFO -          dashboard at:         172.21.6.204:42164
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w7re9rhr
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          dashboard at:         172.21.6.212:46622
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.207:43685
distributed.worker - INFO -          Listening to:   tcp://172.21.6.207:43685
distributed.worker - INFO -          dashboard at:         172.21.6.237:43893
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.210:44827
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.227:44069
distributed.worker - INFO -          dashboard at:         172.21.6.240:44589
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.226:40478
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kw0y_6u3
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pzns1_c2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.237:40420
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.190:40199
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.240:33803
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.224:46303
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.200:36088
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.199:40352
distributed.worker - INFO -          Listening to:   tcp://172.21.6.199:40352
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mrdvxgkl
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vh7r_tkj
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.243:39621
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wp6svka2
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uzc06_x6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.210:44827
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.227:34984
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.233:41418
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.236:35745
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.206:34447
distributed.worker - INFO -          Listening to:   tcp://172.21.6.206:34447
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4tnjhrqj
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.201:38980
distributed.worker - INFO -          Listening to:   tcp://172.21.6.201:38980
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.213:38636
distributed.worker - INFO -          Listening to:   tcp://172.21.6.213:38636
distributed.worker - INFO -          dashboard at:         172.21.6.213:33975
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v_ilo4p7
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.207:38573
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.237:38266
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.190:40199
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iditlfr9
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-096zxxv4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.240:39558
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.6.226:40478
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_o0wkfsc
distributed.worker - INFO -          Listening to:   tcp://172.21.6.236:35745
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.202:43894
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.206:36451
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c5dq_we9
distributed.worker - INFO -          dashboard at:         172.21.6.199:44379
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v2juf3cc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7vqoh2l7
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gu4lvf_7
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.241:42198
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.208:41933
distributed.worker - INFO -          Listening to:   tcp://172.21.6.208:41933
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.243:41052
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.223:42065
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.226:39502
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tb3bnjo4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.236:42501
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a0u2i3si
distributed.worker - INFO -          Listening to:   tcp://172.21.6.200:36088
distributed.worker - INFO -          dashboard at:         172.21.6.200:39062
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.201:33958
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k4s7cuaf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2mv3omi0
distributed.worker - INFO -          Listening to:   tcp://172.21.6.241:42198
distributed.worker - INFO -          dashboard at:         172.21.6.208:44258
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.234:35534
distributed.worker - INFO -          Listening to:   tcp://172.21.6.234:35534
distributed.worker - INFO -          dashboard at:         172.21.6.234:39841
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4akgwbgy
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.223:42065
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bqou0gdr
distributed.worker - INFO -          dashboard at:         172.21.6.210:44545
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.238:44114
distributed.worker - INFO -          Listening to:   tcp://172.21.6.238:44114
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_ni9ro9e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.233:41418
distributed.worker - INFO -          dashboard at:         172.21.6.233:41205
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.6.202:43894
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.225:39523
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.211:39108
distributed.worker - INFO -          Listening to:   tcp://172.21.6.211:39108
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:   tcp://172.21.6.239:36597
distributed.worker - INFO -          Listening to:   tcp://172.21.6.239:36597
distributed.worker - INFO -          dashboard at:         172.21.6.239:34132
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.241:45541
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s5qeyqan
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rj2np98j
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.223:33415
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wqei5moc
distributed.worker - INFO -          dashboard at:         172.21.6.190:44924
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e2xas4m4
distributed.worker - INFO -          dashboard at:         172.21.6.238:45906
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2u7yn9hh
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.6.225:39523
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.6.211:38967
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zuwnmu6a
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qo6orbsx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t9zxneca
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9jr4i4wx
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-abp0z1xm
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1q9c7y4s
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.6.202:35815
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g6ngyo67
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.6.225:42776
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_p_2osp9
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n2mj46dx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2zpcupaj
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bcrsfmct
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rxkwbou2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-96hiu8be
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mpjk3o95
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1yh2bgij
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-75qto93m
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3kq7zx7t
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pe0nqjo2
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-10rd89kb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-717zm2r2
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ch5oq5p
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rmj3zp6d
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u2jsc_j8
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j32g42ee
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-97g1kgi8
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6g5hz4m5
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_gnq6xdh
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-84dxg5wk
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8evmp1k0
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fxn3wauq
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-agvbgdtu
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m0o46j24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.6.170:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Worker for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.17s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.13s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.38s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 16.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 16.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.58s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.95s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.28s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.91s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.31s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 6.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.40s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.06s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.47s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.45s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.42s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.46s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.49s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 7.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 13.77s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1858: error: *** STEP 7588987.1 ON irene1858 CANCELLED AT 2022-12-14T01:22:10 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.234:37852'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.232:36448'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.234:46110'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.232:46518'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.226:36983'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.226:40683'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.202:41901'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.227:42104'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.202:42731'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.227:46240'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.223:37824'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.237:41733'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.240:45639'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.237:40870'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.239:39558'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.239:39145'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.223:38389'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.200:44028'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.200:39650'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.240:34577'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:38863'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:36784'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.205:34311'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.201:43620'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.201:33611'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:45085'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.211:40851'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.211:34421'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.208:42840'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.206:37701'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.225:34678'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.225:40028'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:37621'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.209:44982'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.243:41048'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.243:34346'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.241:36814'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.236:37684'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.241:45402'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.236:38978'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.190:45629'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.190:46784'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.233:41281'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.233:46801'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.201:38980
distributed.worker - INFO - Stopping worker at tcp://172.21.6.201:39966
distributed.worker - INFO - Stopping worker at tcp://172.21.6.243:39621
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.243:33481
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.238:43736'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.204:45899'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.238:40590'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.204:42425'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:34447
distributed.worker - INFO - Stopping worker at tcp://172.21.6.206:37496
distributed.worker - INFO - Stopping worker at tcp://172.21.6.240:33803
distributed.worker - INFO - Stopping worker at tcp://172.21.6.240:33550
distributed.worker - INFO - Stopping worker at tcp://172.21.6.241:42198
distributed.worker - INFO - Stopping worker at tcp://172.21.6.241:35676
distributed.worker - INFO - Stopping worker at tcp://172.21.6.233:35969
distributed.worker - INFO - Stopping worker at tcp://172.21.6.233:41418
distributed.worker - INFO - Stopping worker at tcp://172.21.6.238:44114
distributed.worker - INFO - Stopping worker at tcp://172.21.6.238:39377
distributed.worker - INFO - Stopping worker at tcp://172.21.6.226:40478
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.226:46383
distributed.dask_worker - INFO - Exiting on signal 15
distributed.worker - INFO - Stopping worker at tcp://172.21.6.202:43894
distributed.worker - INFO - Stopping worker at tcp://172.21.6.202:33926
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.235:43160'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.235:46557'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.227:40285
distributed.worker - INFO - Stopping worker at tcp://172.21.6.227:44069
distributed.worker - INFO - Stopping worker at tcp://172.21.6.236:42123
distributed.worker - INFO - Stopping worker at tcp://172.21.6.236:35745
distributed.worker - INFO - Stopping worker at tcp://172.21.6.232:36525
distributed.worker - INFO - Stopping worker at tcp://172.21.6.232:36296
distributed.worker - INFO - Stopping worker at tcp://172.21.6.211:39108
distributed.worker - INFO - Stopping worker at tcp://172.21.6.237:36598
distributed.worker - INFO - Stopping worker at tcp://172.21.6.211:46503
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:44923
distributed.worker - INFO - Stopping worker at tcp://172.21.6.208:41933
distributed.worker - INFO - Stopping worker at tcp://172.21.6.237:40420
distributed.worker - INFO - Stopping worker at tcp://172.21.6.234:36816
distributed.worker - INFO - Stopping worker at tcp://172.21.6.234:35534
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:43292
distributed.worker - INFO - Stopping worker at tcp://172.21.6.205:43324
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:36046'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.224:34411'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.224:36645'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.213:45152'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.199:36772'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.210:46447'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.199:45656'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.210:33397'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.239:34402
distributed.worker - INFO - Stopping worker at tcp://172.21.6.239:36597
distributed.worker - INFO - Stopping worker at tcp://172.21.6.235:38861
distributed.worker - INFO - Stopping worker at tcp://172.21.6.235:34798
distributed.worker - INFO - Stopping worker at tcp://172.21.6.224:46609
distributed.worker - INFO - Stopping worker at tcp://172.21.6.224:44436
distributed.worker - INFO - Stopping worker at tcp://172.21.6.199:40352
distributed.worker - INFO - Stopping worker at tcp://172.21.6.199:34747
distributed.worker - INFO - Stopping worker at tcp://172.21.6.204:36428
distributed.worker - INFO - Stopping worker at tcp://172.21.6.204:38338
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:35332
distributed.worker - INFO - Stopping worker at tcp://172.21.6.209:36425
distributed.worker - INFO - Stopping worker at tcp://172.21.6.210:44827
distributed.worker - INFO - Stopping worker at tcp://172.21.6.210:36676
distributed.worker - INFO - Stopping worker at tcp://172.21.6.225:39523
distributed.worker - INFO - Stopping worker at tcp://172.21.6.225:41063
distributed.worker - INFO - Stopping worker at tcp://172.21.6.200:38232
distributed.worker - INFO - Stopping worker at tcp://172.21.6.200:36088
distributed.worker - INFO - Stopping worker at tcp://172.21.6.190:40199
distributed.worker - INFO - Stopping worker at tcp://172.21.6.190:38861
distributed.worker - INFO - Stopping worker at tcp://172.21.6.223:42065
distributed.worker - INFO - Stopping worker at tcp://172.21.6.223:40348
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:39374
distributed.worker - INFO - Stopping worker at tcp://172.21.6.213:38636
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:32921'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.207:36388'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:35369'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.203:36852'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.212:35317'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.6.203:39669'
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:34091
distributed.worker - INFO - Stopping worker at tcp://172.21.6.207:43685
distributed.worker - INFO - Stopping worker at tcp://172.21.6.203:43589
distributed.worker - INFO - Stopping worker at tcp://172.21.6.203:45340
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:44914
distributed.worker - INFO - Stopping worker at tcp://172.21.6.212:39568
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5174 parent=5068 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84227 parent=84122 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=84228 parent=84121 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76180 parent=76072 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=5173 parent=5069 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57018 parent=56912 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19501 parent=19391 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79961 parent=79853 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19500 parent=19392 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59375 parent=59269 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=59376 parent=59270 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57019 parent=56913 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38912 parent=38806 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38911 parent=38805 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48663 parent=48558 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=76179 parent=76073 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13279 parent=13173 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24355 parent=24249 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4668 parent=4557 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93590 parent=93483 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60607 parent=60500 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=13278 parent=13172 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=93591 parent=93484 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=24356 parent=24250 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=79960 parent=79852 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69438 parent=69331 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29062 parent=28955 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29061 parent=28956 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=69439 parent=69332 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4666 parent=4558 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=60608 parent=60501 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62825 parent=62718 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62824 parent=62719 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48662 parent=48557 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39678 parent=39572 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44963 parent=44856 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21355 parent=21247 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21354 parent=21248 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32305 parent=32198 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39679 parent=39573 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44964 parent=44857 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=32304 parent=32199 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48751 parent=48647 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=48752 parent=48646 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8464 parent=8357 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90748 parent=90643 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=90749 parent=90644 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=8463 parent=8358 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81530 parent=81421 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.dask_worker - INFO - End worker
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81529 parent=81420 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82938 parent=82830 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82937 parent=82831 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12690 parent=12582 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26216 parent=26107 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=26214 parent=26108 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=12689 parent=12583 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29604 parent=29499 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29605 parent=29498 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49158 parent=49051 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49159 parent=49052 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63278 parent=63067 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6409 parent=6303 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=63277 parent=63066 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=6408 parent=6302 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x70d4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11a04c0)

Current thread 0x00002b6cf7a49b80 (most recent call first):
<no Python frame>
Current thread 0x00002ab9b45fcb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf954c0)

Current thread 0x00002b4282626b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1ce54c0)

Current thread 0x00002adcd68d7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x24034c0)

Current thread 0x00002ba63c8d6b80 (most recent call first):
<no Python frame>
Python runtime state: finalizing (tstate=0x1bd24c0)

Current thread 0x00002b6570ed0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x4af4c0)

Current thread 0x00002b67942edb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16964c0)

Current thread 0x00002afbc53abb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xb234c0)

Current thread 0x00002b5fb4477b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23bc4c0)

Current thread 0x00002b6463252b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5d94c0)

Current thread 0x00002ad93167cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16104c0)

Current thread 0x00002b105730bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23e04c0)

Current thread 0x00002aed694d0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xca84c0)

Current thread 0x00002af0882bab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20154c0)

Current thread 0x00002b77ea357b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x13ee4c0)

Current thread 0x00002b9a8cfedb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd9d4c0)

Current thread 0x00002b49b7104b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x181f4c0)

Python runtime state: finalizing (tstate=0x10674c0)

Current thread 0x00002b8d4a5a7b80 (most recent call first):
<no Python frame>
Current thread 0x00002b8789cc5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22d64c0)

Current thread 0x00002b81cb143b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1cb84c0)

Current thread 0x00002b30093d5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23a14c0)

Current thread 0x00002b8418144b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xd194c0)

Current thread 0x00002ac3f20f5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7b34c0)

Current thread 0x00002ab327a9bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xec04c0)

Current thread 0x00002b69a992eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11664c0)

Current thread 0x00002b122c1d6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10eb4c0)

Current thread 0x00002adfe83edb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18604c0)

Current thread 0x00002abc412e0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc7c4c0)

Current thread 0x00002b84b575db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20d14c0)

Current thread 0x00002ada8b416b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xecd4c0)

Current thread 0x00002b3752a9eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11c54c0)

Current thread 0x00002ae3943c7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10ec4c0)

Current thread 0x00002af1d47a7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23334c0)

Current thread 0x00002b5216d57b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x106c4c0)

Current thread 0x00002b7d761f9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x109a4c0)

Current thread 0x00002ad95e6f7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1bd64c0)

Current thread 0x00002aef36373b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1d574c0)

Current thread 0x00002b38039efb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xc294c0)

Current thread 0x00002b23147f0b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f854c0)

Current thread 0x00002aae2eb4bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfcc4c0)

Current thread 0x00002ba817bceb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x16864c0)

Current thread 0x00002acf2f4c6b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x154c4c0)

Current thread 0x00002ab3cb383b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1e684c0)

Current thread 0x00002ba5b79bfb80 (most recent call first):
<no Python frame>
srun: error: irene1895: tasks 40-41: Aborted (core dumped)
srun: error: irene1900: tasks 42-43: Aborted (core dumped)
srun: error: irene1868: tasks 4-5: Aborted (core dumped)
srun: error: irene1907: tasks 56-57: Aborted (core dumped)
srun: error: irene1879: tasks 26-27: Aborted (core dumped)
srun: error: irene1904: tasks 50-51: Aborted (core dumped)
srun: error: irene1891: tasks 32-33: Aborted (core dumped)
srun: error: irene1908: tasks 58-59: Aborted (core dumped)
srun: error: irene1870: tasks 8-9: Aborted (core dumped)
srun: error: irene1903: tasks 48-49: Aborted (core dumped)
srun: error: irene1892: tasks 34-35: Aborted (core dumped)
srun: error: irene1894: tasks 38-39: Aborted (core dumped)
srun: error: irene1878: tasks 24-25: Aborted (core dumped)
srun: error: irene1867: tasks 2-3: Aborted (core dumped)
srun: error: irene1874: tasks 16-17: Aborted (core dumped)
srun: error: irene1876: tasks 20-21: Aborted (core dumped)
srun: error: irene1901: tasks 44-45: Aborted (core dumped)
srun: error: irene1873: task 15: Aborted (core dumped)
srun: error: irene1893: task 36: Aborted (core dumped)
srun: error: irene1881: task 31: Aborted (core dumped)
srun: error: irene1909: task 60: Aborted (core dumped)
srun: error: irene1905: task 52: Aborted (core dumped)
srun: error: irene1869: task 6: Aborted (core dumped)
srun: error: irene1872: task 12: Aborted (core dumped)
srun: error: irene1858: task 1: Aborted (core dumped)
srun: error: irene1875: task 19: Aborted (core dumped)
srun: error: irene1880: task 28: Aborted (core dumped)
