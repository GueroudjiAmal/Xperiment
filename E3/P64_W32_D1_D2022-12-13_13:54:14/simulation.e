slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_p2p_send: irene1156 [14]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1156: error:  mpi/pmix_v3: _slurm_send: irene1156 [14]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 681, hostlist:
(null)
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1162 [20]: pmixp_coll_ring.c:742: 0x2b368c0402f0: collective timeout seq=0
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_log: irene1162 [20]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:760: 0x2b368c0402f0: COLL_FENCE_RING state seq=0
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:762: my peerid: 20:irene1135
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:769: neighbor id: next 21:irene1136, prev 19:irene1128
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:779: Context ptr=0x2b368c040368, #0, in-use=0
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:779: Context ptr=0x2b368c0403a0, #1, in-use=0
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:779: Context ptr=0x2b368c0403d8, #2, in-use=1
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=5/fwd=6
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128]
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1136-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1162: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1162 [20]: pmixp_coll_ring.c:833: 	 buf (offset/size): 3486/22078
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1177 [24]: pmixp_coll_ring.c:742: 0x2ac94403f710: collective timeout seq=0
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_log: irene1177 [24]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:760: 0x2ac94403f710: COLL_FENCE_RING state seq=0
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:762: my peerid: 24:irene1143
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:769: neighbor id: next 25:irene1148, prev 23:irene1140
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:779: Context ptr=0x2ac94403f788, #0, in-use=0
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:779: Context ptr=0x2ac94403f7c0, #1, in-use=0
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:779: Context ptr=0x2ac94403f7f8, #2, in-use=1
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=9/fwd=10
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140]
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1148,1150-1155]
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1177: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1177 [24]: pmixp_coll_ring.c:833: 	 buf (offset/size): 5810/24402
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1158 [16]: pmixp_coll_ring.c:742: 0x2b5744040250: collective timeout seq=0
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_log: irene1158 [16]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:760: 0x2b5744040250: COLL_FENCE_RING state seq=0
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:762: my peerid: 16:irene1119
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:769: neighbor id: next 17:irene1120, prev 15:irene1118
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:779: Context ptr=0x2b57440402c8, #0, in-use=0
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:779: Context ptr=0x2b5744040300, #1, in-use=0
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:779: Context ptr=0x2b5744040338, #2, in-use=1
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=1/fwd=2
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:825: 		 done contrib: irene1118
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1158 [16]: pmixp_coll_ring.c:833: 	 buf (offset/size): 1162/19754
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1166 [23]: pmixp_coll_ring.c:742: 0x2ac048005cb0: collective timeout seq=0
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_log: irene1166 [23]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:760: 0x2ac048005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:762: my peerid: 23:irene1140
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:769: neighbor id: next 24:irene1143, prev 22:irene1137
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:779: Context ptr=0x2ac048005d28, #0, in-use=0
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:779: Context ptr=0x2ac048005d60, #1, in-use=0
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:779: Context ptr=0x2ac048005d98, #2, in-use=1
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=8/fwd=9
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137]
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1143,1148,1150-1155]
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1166: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1166 [23]: pmixp_coll_ring.c:833: 	 buf (offset/size): 5229/23821
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1194 [25]: pmixp_coll_ring.c:742: 0x2b5eb0040220: collective timeout seq=0
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_log: irene1194 [25]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:760: 0x2b5eb0040220: COLL_FENCE_RING state seq=0
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:762: my peerid: 25:irene1148
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:769: neighbor id: next 26:irene1150, prev 24:irene1143
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:779: Context ptr=0x2b5eb0040298, #0, in-use=0
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:779: Context ptr=0x2b5eb00402d0, #1, in-use=0
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:779: Context ptr=0x2b5eb0040308, #2, in-use=1
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=10/fwd=11
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143]
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1150-1155]
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1194: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1194 [25]: pmixp_coll_ring.c:833: 	 buf (offset/size): 6391/24983
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1135 [2]: pmixp_coll_ring.c:742: 0x2ba590005cb0: collective timeout seq=0
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_log: irene1135 [2]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:760: 0x2ba590005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:762: my peerid: 2:irene1036
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:769: neighbor id: next 3:irene1049, prev 1:irene1023
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:779: Context ptr=0x2ba590005d28, #0, in-use=0
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:779: Context ptr=0x2ba590005d60, #1, in-use=0
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:779: Context ptr=0x2ba590005d98, #2, in-use=1
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=19/fwd=20
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1049,1062-1068,1074,1076,1078,1106]
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1135 [2]: pmixp_coll_ring.c:833: 	 buf (offset/size): 11620/30212
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1154 [12]: pmixp_coll_ring.c:742: 0x2b30ac005cb0: collective timeout seq=0
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_log: irene1154 [12]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:760: 0x2b30ac005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:762: my peerid: 12:irene1076
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:769: neighbor id: next 13:irene1078, prev 11:irene1074
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:779: Context ptr=0x2b30ac005d28, #0, in-use=0
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:779: Context ptr=0x2b30ac005d60, #1, in-use=0
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:779: Context ptr=0x2b30ac005d98, #2, in-use=1
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=29/fwd=30
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1068,1074,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1078,1106]
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1154 [12]: pmixp_coll_ring.c:833: 	 buf (offset/size): 17430/36022
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1123 [0]: pmixp_coll_ring.c:742: 0x2b9e1003f6a0: collective timeout seq=0
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_log: irene1123 [0]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:760: 0x2b9e1003f6a0: COLL_FENCE_RING state seq=0
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:762: my peerid: 0:irene1020
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:769: neighbor id: next 1:irene1023, prev 31:irene1155
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:779: Context ptr=0x2b9e1003f718, #0, in-use=0
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:779: Context ptr=0x2b9e1003f750, #1, in-use=0
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:779: Context ptr=0x2b9e1003f788, #2, in-use=1
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=17/fwd=18
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1023,1036,1049,1062-1068,1074,1076,1078,1106]
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1123 [0]: pmixp_coll_ring.c:833: 	 buf (offset/size): 10458/29050
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1150 [8]: pmixp_coll_ring.c:742: 0x2b51ec03fd50: collective timeout seq=0
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_log: irene1150 [8]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:760: 0x2b51ec03fd50: COLL_FENCE_RING state seq=0
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:762: my peerid: 8:irene1066
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:769: neighbor id: next 9:irene1067, prev 7:irene1065
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:779: Context ptr=0x2b51ec03fdc8, #0, in-use=0
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:779: Context ptr=0x2b51ec03fe00, #1, in-use=0
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:779: Context ptr=0x2b51ec03fe38, #2, in-use=1
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=25/fwd=26
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1065,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1067-1068,1074,1076,1078,1106]
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1150 [8]: pmixp_coll_ring.c:833: 	 buf (offset/size): 15106/33698
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1160 [18]: pmixp_coll_ring.c:742: 0x2b3378040280: collective timeout seq=0
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_log: irene1160 [18]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:760: 0x2b3378040280: COLL_FENCE_RING state seq=0
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:762: my peerid: 18:irene1123
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:769: neighbor id: next 19:irene1128, prev 17:irene1120
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:779: Context ptr=0x2b33780402f8, #0, in-use=0
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:779: Context ptr=0x2b3378040330, #1, in-use=0
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:779: Context ptr=0x2b3378040368, #2, in-use=1
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=3/fwd=4
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120]
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1160: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1160 [18]: pmixp_coll_ring.c:833: 	 buf (offset/size): 2324/20916
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1199 [27]: pmixp_coll_ring.c:742: 0x2b86ac0402b0: collective timeout seq=0
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_log: irene1199 [27]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:760: 0x2b86ac0402b0: COLL_FENCE_RING state seq=0
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:762: my peerid: 27:irene1151
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:769: neighbor id: next 28:irene1152, prev 26:irene1150
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:779: Context ptr=0x2b86ac040328, #0, in-use=0
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:779: Context ptr=0x2b86ac040360, #1, in-use=0
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:779: Context ptr=0x2b86ac040398, #2, in-use=1
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=12/fwd=13
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150]
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1152-1155]
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1199: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1199 [27]: pmixp_coll_ring.c:833: 	 buf (offset/size): 7553/26145
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1155 [13]: pmixp_coll_ring.c:742: 0x2b028c005c70: collective timeout seq=0
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_log: irene1155 [13]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:760: 0x2b028c005c70: COLL_FENCE_RING state seq=0
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:762: my peerid: 13:irene1078
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:769: neighbor id: next 14:irene1106, prev 12:irene1076
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:779: Context ptr=0x2b028c005ce8, #0, in-use=0
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:779: Context ptr=0x2b028c005d20, #1, in-use=0
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:779: Context ptr=0x2b028c005d58, #2, in-use=1
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=30/fwd=31
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:827: 		 wait contrib: irene1106
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1155 [13]: pmixp_coll_ring.c:833: 	 buf (offset/size): 18011/36603
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1143 [6]: pmixp_coll_ring.c:742: 0x2ba02c005cb0: collective timeout seq=0
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_log: irene1143 [6]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:760: 0x2ba02c005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:762: my peerid: 6:irene1064
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:769: neighbor id: next 7:irene1065, prev 5:irene1063
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:779: Context ptr=0x2ba02c005d28, #0, in-use=0
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:779: Context ptr=0x2ba02c005d60, #1, in-use=0
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:779: Context ptr=0x2ba02c005d98, #2, in-use=1
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=23/fwd=24
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1063,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1065-1068,1074,1076,1078,1106]
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1143 [6]: pmixp_coll_ring.c:833: 	 buf (offset/size): 13944/32536
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1200 [28]: pmixp_coll_ring.c:742: 0x2ba4240401c0: collective timeout seq=0
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_log: irene1200 [28]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:760: 0x2ba4240401c0: COLL_FENCE_RING state seq=0
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:762: my peerid: 28:irene1152
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:769: neighbor id: next 29:irene1153, prev 27:irene1151
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:779: Context ptr=0x2ba424040238, #0, in-use=0
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:779: Context ptr=0x2ba424040270, #1, in-use=0
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:779: Context ptr=0x2ba4240402a8, #2, in-use=1
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=13/fwd=14
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1151]
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1153-1155]
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1200: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1200 [28]: pmixp_coll_ring.c:833: 	 buf (offset/size): 8134/26726
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1201 [29]: pmixp_coll_ring.c:742: 0x2b4fc00402b0: collective timeout seq=0
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_log: irene1201 [29]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:760: 0x2b4fc00402b0: COLL_FENCE_RING state seq=0
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:762: my peerid: 29:irene1153
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:769: neighbor id: next 30:irene1154, prev 28:irene1152
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:779: Context ptr=0x2b4fc0040328, #0, in-use=0
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:779: Context ptr=0x2b4fc0040360, #1, in-use=0
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:779: Context ptr=0x2b4fc0040398, #2, in-use=1
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=14/fwd=15
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1152]
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1154-1155]
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1201: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1201 [29]: pmixp_coll_ring.c:833: 	 buf (offset/size): 8715/27307
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1161 [19]: pmixp_coll_ring.c:742: 0x2b752403fdd0: collective timeout seq=0
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_log: irene1161 [19]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:760: 0x2b752403fdd0: COLL_FENCE_RING state seq=0
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:762: my peerid: 19:irene1128
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:769: neighbor id: next 20:irene1135, prev 18:irene1123
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:779: Context ptr=0x2b752403fe48, #0, in-use=0
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:779: Context ptr=0x2b752403fe80, #1, in-use=0
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:779: Context ptr=0x2b752403feb8, #2, in-use=1
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=4/fwd=5
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123]
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1161: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1161 [19]: pmixp_coll_ring.c:833: 	 buf (offset/size): 2905/21497
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1164 [21]: pmixp_coll_ring.c:742: 0x2b4a14005cb0: collective timeout seq=0
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_log: irene1164 [21]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:760: 0x2b4a14005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:762: my peerid: 21:irene1136
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:769: neighbor id: next 22:irene1137, prev 20:irene1135
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:779: Context ptr=0x2b4a14005d28, #0, in-use=0
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:779: Context ptr=0x2b4a14005d60, #1, in-use=0
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:779: Context ptr=0x2b4a14005d98, #2, in-use=1
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=6/fwd=7
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135]
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1137,1140,1143,1148,1150-1155]
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1164: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1164 [21]: pmixp_coll_ring.c:833: 	 buf (offset/size): 4067/22659
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1221 [31]: pmixp_coll_ring.c:742: 0x2ba664040340: collective timeout seq=0
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_log: irene1221 [31]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:760: 0x2ba664040340: COLL_FENCE_RING state seq=0
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:762: my peerid: 31:irene1155
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:769: neighbor id: next 0:irene1020, prev 30:irene1154
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:779: Context ptr=0x2ba6640403b8, #0, in-use=0
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:779: Context ptr=0x2ba6640403f0, #1, in-use=0
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:779: Context ptr=0x2ba664040428, #2, in-use=1
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=16/fwd=17
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1154]
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106]
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1221 [31]: pmixp_coll_ring.c:833: 	 buf (offset/size): 9877/28469
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1165 [22]: pmixp_coll_ring.c:742: 0x2b2548005cb0: collective timeout seq=0
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_log: irene1165 [22]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:760: 0x2b2548005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:762: my peerid: 22:irene1137
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:769: neighbor id: next 23:irene1140, prev 21:irene1136
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:779: Context ptr=0x2b2548005d28, #0, in-use=0
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:779: Context ptr=0x2b2548005d60, #1, in-use=0
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:779: Context ptr=0x2b2548005d98, #2, in-use=1
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=7/fwd=8
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1136]
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1140,1143,1148,1150-1155]
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1165: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1165 [22]: pmixp_coll_ring.c:833: 	 buf (offset/size): 4648/23240
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1157 [15]: pmixp_coll_ring.c:742: 0x2b400c040200: collective timeout seq=0
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1220 [30]: pmixp_coll_ring.c:742: 0x2b332c005c30: collective timeout seq=0
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1152 [10]: pmixp_coll_ring.c:742: 0x2b882c040290: collective timeout seq=0
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_log: irene1220 [30]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_log: irene1152 [10]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:760: 0x2b332c005c30: COLL_FENCE_RING state seq=0
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:760: 0x2b882c040290: COLL_FENCE_RING state seq=0
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:762: my peerid: 30:irene1154
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:762: my peerid: 10:irene1068
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:769: neighbor id: next 31:irene1155, prev 29:irene1153
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:769: neighbor id: next 11:irene1074, prev 9:irene1067
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:779: Context ptr=0x2b332c005ca8, #0, in-use=0
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:779: Context ptr=0x2b882c040308, #0, in-use=0
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:779: Context ptr=0x2b332c005ce0, #1, in-use=0
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:779: Context ptr=0x2b882c040340, #1, in-use=0
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:779: Context ptr=0x2b332c005d18, #2, in-use=1
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:779: Context ptr=0x2b882c040378, #2, in-use=1
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=15/fwd=16
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=27/fwd=28
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1153]
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1067,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1155]
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1074,1076,1078,1106]
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1220: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1220 [30]: pmixp_coll_ring.c:833: 	 buf (offset/size): 9296/27888
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1152 [10]: pmixp_coll_ring.c:833: 	 buf (offset/size): 16268/34860
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_log: irene1157 [15]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:760: 0x2b400c040200: COLL_FENCE_RING state seq=0
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:762: my peerid: 15:irene1118
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:769: neighbor id: next 16:irene1119, prev 14:irene1106
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:779: Context ptr=0x2b400c040278, #0, in-use=0
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:779: Context ptr=0x2b400c0402b0, #1, in-use=0
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:779: Context ptr=0x2b400c0402e8, #2, in-use=1
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=0/fwd=1
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:825: 		 done contrib: -
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1119-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1157 [15]: pmixp_coll_ring.c:833: 	 buf (offset/size): 581/19173
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1137 [4]: pmixp_coll_ring.c:742: 0x2b2a6803ff30: collective timeout seq=0
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_log: irene1137 [4]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:760: 0x2b2a6803ff30: COLL_FENCE_RING state seq=0
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:762: my peerid: 4:irene1062
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:769: neighbor id: next 5:irene1063, prev 3:irene1049
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:779: Context ptr=0x2b2a6803ffa8, #0, in-use=0
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:779: Context ptr=0x2b2a6803ffe0, #1, in-use=0
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:779: Context ptr=0x2b2a68040018, #2, in-use=1
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=21/fwd=22
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1063-1068,1074,1076,1078,1106]
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1137 [4]: pmixp_coll_ring.c:833: 	 buf (offset/size): 12782/31374
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1128 [1]: pmixp_coll_ring.c:742: 0x2b7654040270: collective timeout seq=0
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_log: irene1128 [1]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:760: 0x2b7654040270: COLL_FENCE_RING state seq=0
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:762: my peerid: 1:irene1023
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:769: neighbor id: next 2:irene1036, prev 0:irene1020
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:779: Context ptr=0x2b76540402e8, #0, in-use=0
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:779: Context ptr=0x2b7654040320, #1, in-use=0
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:779: Context ptr=0x2b7654040358, #2, in-use=1
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=18/fwd=19
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1036,1049,1062-1068,1074,1076,1078,1106]
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1128 [1]: pmixp_coll_ring.c:833: 	 buf (offset/size): 11039/29631
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1136 [3]: pmixp_coll_ring.c:742: 0x2b18cc005cb0: collective timeout seq=0
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_log: irene1136 [3]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:760: 0x2b18cc005cb0: COLL_FENCE_RING state seq=0
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:762: my peerid: 3:irene1049
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:769: neighbor id: next 4:irene1062, prev 2:irene1036
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:779: Context ptr=0x2b18cc005d28, #0, in-use=0
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:779: Context ptr=0x2b18cc005d60, #1, in-use=0
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:779: Context ptr=0x2b18cc005d98, #2, in-use=1
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=20/fwd=21
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1062-1068,1074,1076,1078,1106]
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1136 [3]: pmixp_coll_ring.c:833: 	 buf (offset/size): 12201/30793
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1159 [17]: pmixp_coll_ring.c:742: 0x2aeb94005cf0: collective timeout seq=0
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_log: irene1159 [17]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:760: 0x2aeb94005cf0: COLL_FENCE_RING state seq=0
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:762: my peerid: 17:irene1120
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:769: neighbor id: next 18:irene1123, prev 16:irene1119
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:779: Context ptr=0x2aeb94005d68, #0, in-use=0
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:779: Context ptr=0x2aeb94005da0, #1, in-use=0
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:779: Context ptr=0x2aeb94005dd8, #2, in-use=1
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=2/fwd=3
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1119]
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1159: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1159 [17]: pmixp_coll_ring.c:833: 	 buf (offset/size): 1743/20335
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1153 [11]: pmixp_coll_ring.c:742: 0x2b588403f720: collective timeout seq=0
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_log: irene1153 [11]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:760: 0x2b588403f720: COLL_FENCE_RING state seq=0
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:762: my peerid: 11:irene1074
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:769: neighbor id: next 12:irene1076, prev 10:irene1068
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:779: Context ptr=0x2b588403f798, #0, in-use=0
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:779: Context ptr=0x2b588403f7d0, #1, in-use=0
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:779: Context ptr=0x2b588403f808, #2, in-use=1
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=28/fwd=29
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1068,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1076,1078,1106]
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1153 [11]: pmixp_coll_ring.c:833: 	 buf (offset/size): 16849/35441
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1197 [26]: pmixp_coll_ring.c:742: 0x2b3834005c70: collective timeout seq=0
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_log: irene1197 [26]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:760: 0x2b3834005c70: COLL_FENCE_RING state seq=0
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:762: my peerid: 26:irene1150
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:769: neighbor id: next 27:irene1151, prev 25:irene1148
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:779: Context ptr=0x2b3834005ce8, #0, in-use=0
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:779: Context ptr=0x2b3834005d20, #1, in-use=0
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:779: Context ptr=0x2b3834005d58, #2, in-use=1
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=11/fwd=12
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:825: 		 done contrib: irene[1118-1120,1123,1128,1135-1137,1140,1143,1148]
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1106,1151-1155]
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1197: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1197 [26]: pmixp_coll_ring.c:833: 	 buf (offset/size): 6972/25564
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1140 [5]: pmixp_coll_ring.c:742: 0x2b90c8040320: collective timeout seq=0
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_log: irene1140 [5]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:760: 0x2b90c8040320: COLL_FENCE_RING state seq=0
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:762: my peerid: 5:irene1063
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:769: neighbor id: next 6:irene1064, prev 4:irene1062
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:779: Context ptr=0x2b90c8040398, #0, in-use=0
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:779: Context ptr=0x2b90c80403d0, #1, in-use=0
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:779: Context ptr=0x2b90c8040408, #2, in-use=1
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=22/fwd=23
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1064-1068,1074,1076,1078,1106]
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1140 [5]: pmixp_coll_ring.c:833: 	 buf (offset/size): 13363/31955
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1156 [14]: pmixp_coll_ring.c:742: 0x2afc8c005b60: collective timeout seq=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_log: irene1156 [14]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:760: 0x2afc8c005b60: COLL_FENCE_RING state seq=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:762: my peerid: 14:irene1106
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:769: neighbor id: next 15:irene1118, prev 13:irene1078
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:779: Context ptr=0x2afc8c005bd8, #0, in-use=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:779: Context ptr=0x2afc8c005c10, #1, in-use=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:779: Context ptr=0x2afc8c005c48, #2, in-use=1
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=31/fwd=1
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1068,1074,1076,1078,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:827: 		 wait contrib: -
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_FINILIZE
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1156 [14]: pmixp_coll_ring.c:833: 	 buf (offset/size): 0/37184
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_reset_if_to: irene1156 [14]: pmixp_coll_tree.c:1318: 0x2afc88047810: collective timeout seq=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_log: irene1156 [14]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1337: 0x2afc88047810: COLL_FENCE_TREE state seq=0 contribs: loc=1/prnt=0/child=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1339: my peerid: 14:irene1106
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1342: root host: 18:irene1123
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1346: prnt host: 18:irene1123
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1347: prnt contrib:
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1349: 	 [18:irene1123] false
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1356: child contribs [12]:
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1383: 	 done contrib: -
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1385: 	 wait contrib: irene[1157-1162,1164-1166,1177,1194,1197]
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1392: status: coll=COLL_COLLECT upfw=COLL_SND_NONE dfwd=COLL_SND_NONE
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1394: dfwd status: dfwd_cb_cnt=0, dfwd_cb_wait=0
slurmstepd-irene1156: error:  mpi/pmix_v3: pmixp_coll_tree_log: irene1156 [14]: pmixp_coll_tree.c:1397: bufs (offset/size): upfw 87/16415, dfwd 68/16415
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1151 [9]: pmixp_coll_ring.c:742: 0x2ae5e00402c0: collective timeout seq=0
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_log: irene1151 [9]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:760: 0x2ae5e00402c0: COLL_FENCE_RING state seq=0
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:762: my peerid: 9:irene1067
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:769: neighbor id: next 10:irene1068, prev 8:irene1066
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:779: Context ptr=0x2ae5e0040338, #0, in-use=0
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:779: Context ptr=0x2ae5e0040370, #1, in-use=0
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:779: Context ptr=0x2ae5e00403a8, #2, in-use=1
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=26/fwd=27
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1066,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1068,1074,1076,1078,1106]
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1151 [9]: pmixp_coll_ring.c:833: 	 buf (offset/size): 15687/34279
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_reset_if_to: irene1148 [7]: pmixp_coll_ring.c:742: 0x2b259803f5b0: collective timeout seq=0
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_log: irene1148 [7]: pmixp_coll.c:281: Dumping collective state
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:760: 0x2b259803f5b0: COLL_FENCE_RING state seq=0
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:762: my peerid: 7:irene1065
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:769: neighbor id: next 8:irene1066, prev 6:irene1064
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:779: Context ptr=0x2b259803f628, #0, in-use=0
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:779: Context ptr=0x2b259803f660, #1, in-use=0
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:779: Context ptr=0x2b259803f698, #2, in-use=1
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:790: 	 seq=0 contribs: loc=1/prev=24/fwd=25
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:792: 	 neighbor contribs [32]:
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:825: 		 done contrib: irene[1020,1023,1036,1049,1062-1064,1118-1120,1123,1128,1135-1137,1140,1143,1148,1150-1155]
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:827: 		 wait contrib: irene[1066-1068,1074,1076,1078,1106]
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:829: 	 status=PMIXP_COLL_RING_PROGRESS
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_coll_ring_log: irene1148 [7]: pmixp_coll_ring.c:833: 	 buf (offset/size): 14525/33117
slurmstepd-irene1158: error:  mpi/pmix_v3: pmixp_p2p_send: irene1158 [16]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1158: error:  mpi/pmix_v3: _slurm_send: irene1158 [16]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1135: error:  mpi/pmix_v3: pmixp_p2p_send: irene1135 [2]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1135: error:  mpi/pmix_v3: _slurm_send: irene1135 [2]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1154: error:  mpi/pmix_v3: pmixp_p2p_send: irene1154 [12]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1154: error:  mpi/pmix_v3: _slurm_send: irene1154 [12]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1123: error:  mpi/pmix_v3: pmixp_p2p_send: irene1123 [0]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1123: error:  mpi/pmix_v3: _slurm_send: irene1123 [0]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1150: error:  mpi/pmix_v3: pmixp_p2p_send: irene1150 [8]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1150: error:  mpi/pmix_v3: _slurm_send: irene1150 [8]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1155: error:  mpi/pmix_v3: pmixp_p2p_send: irene1155 [13]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1155: error:  mpi/pmix_v3: _slurm_send: irene1155 [13]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1143: error:  mpi/pmix_v3: pmixp_p2p_send: irene1143 [6]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1143: error:  mpi/pmix_v3: _slurm_send: irene1143 [6]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1221: error:  mpi/pmix_v3: pmixp_p2p_send: irene1221 [31]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1221: error:  mpi/pmix_v3: _slurm_send: irene1221 [31]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1157: error:  mpi/pmix_v3: pmixp_p2p_send: irene1157 [15]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1157: error:  mpi/pmix_v3: _slurm_send: irene1157 [15]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1152: error:  mpi/pmix_v3: pmixp_p2p_send: irene1152 [10]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1152: error:  mpi/pmix_v3: _slurm_send: irene1152 [10]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1137: error:  mpi/pmix_v3: pmixp_p2p_send: irene1137 [4]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1137: error:  mpi/pmix_v3: _slurm_send: irene1137 [4]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1128: error:  mpi/pmix_v3: pmixp_p2p_send: irene1128 [1]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1128: error:  mpi/pmix_v3: _slurm_send: irene1128 [1]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1136: error:  mpi/pmix_v3: pmixp_p2p_send: irene1136 [3]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1136: error:  mpi/pmix_v3: _slurm_send: irene1136 [3]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1153: error:  mpi/pmix_v3: pmixp_p2p_send: irene1153 [11]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1153: error:  mpi/pmix_v3: _slurm_send: irene1153 [11]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1140: error:  mpi/pmix_v3: pmixp_p2p_send: irene1140 [5]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1140: error:  mpi/pmix_v3: _slurm_send: irene1140 [5]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1151: error:  mpi/pmix_v3: pmixp_p2p_send: irene1151 [9]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1151: error:  mpi/pmix_v3: _slurm_send: irene1151 [9]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
slurmstepd-irene1148: error:  mpi/pmix_v3: pmixp_p2p_send: irene1148 [7]: pmixp_utils.c:472: send failed, rc=2, exceeded the retry limit
slurmstepd-irene1148: error:  mpi/pmix_v3: _slurm_send: irene1148 [7]: pmixp_server.c:1588: Cannot send message to /var/spool/slurmd/stepd.slurm.pmix.7588972.3, size = 86, hostlist:
(null)
[irene1150:17162] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1150:17161] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1199:75427] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1199:75426] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1158:44635] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1158:44634] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1158:44634] *** An error occurred in MPI_Init
[irene1158:44634] *** reported by process [1760459894,32]
[irene1158:44634] *** on a NULL communicator
[irene1158:44634] *** Unknown error
[irene1158:44634] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1158:44634] ***    and potentially your MPI job)
[irene1150:17162] *** An error occurred in MPI_Init
[irene1150:17162] *** reported by process [1760459894,17]
[irene1150:17162] *** on a NULL communicator
[irene1150:17162] *** Unknown error
[irene1150:17162] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1150:17162] ***    and potentially your MPI job)
[irene1158:44635] *** An error occurred in MPI_Init
[irene1158:44635] *** reported by process [1760459894,33]
[irene1158:44635] *** on a NULL communicator
[irene1158:44635] *** Unknown error
[irene1158:44635] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1158:44635] ***    and potentially your MPI job)
[irene1199:75426] *** An error occurred in MPI_Init
[irene1199:75426] *** reported by process [1760459894,54]
[irene1199:75426] *** on a NULL communicator
[irene1199:75426] *** Unknown error
[irene1199:75426] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1199:75426] ***    and potentially your MPI job)
[irene1150:17161] *** An error occurred in MPI_Init
[irene1150:17161] *** reported by process [1760459894,16]
[irene1150:17161] *** on a NULL communicator
[irene1150:17161] *** Unknown error
[irene1150:17161] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1150:17161] ***    and potentially your MPI job)
[irene1199:75427] *** An error occurred in MPI_Init
[irene1199:75427] *** reported by process [1760459894,55]
[irene1199:75427] *** on a NULL communicator
[irene1199:75427] *** Unknown error
[irene1199:75427] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1199:75427] ***    and potentially your MPI job)
[irene1135:78773] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1135:78772] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1135:78773] *** An error occurred in MPI_Init
[irene1135:78773] *** reported by process [1760459894,5]
[irene1135:78773] *** on a NULL communicator
[irene1135:78773] *** Unknown error
[irene1135:78773] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1135:78773] ***    and potentially your MPI job)
[irene1135:78772] *** An error occurred in MPI_Init
[irene1135:78772] *** reported by process [1760459894,4]
[irene1135:78772] *** on a NULL communicator
[irene1135:78772] *** Unknown error
[irene1135:78772] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1135:78772] ***    and potentially your MPI job)
[irene1154:52925] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1154:52924] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1154:52925] *** An error occurred in MPI_Init
[irene1154:52925] *** reported by process [1760459894,25]
[irene1154:52925] *** on a NULL communicator
[irene1154:52925] *** Unknown error
[irene1154:52925] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1154:52925] ***    and potentially your MPI job)
[irene1154:52924] *** An error occurred in MPI_Init
[irene1154:52924] *** reported by process [1760459894,24]
[irene1154:52924] *** on a NULL communicator
[irene1154:52924] *** Unknown error
[irene1154:52924] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1154:52924] ***    and potentially your MPI job)
[irene1200:93866] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1200:93867] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1200:93866] *** An error occurred in MPI_Init
[irene1200:93866] *** reported by process [1760459894,56]
[irene1200:93866] *** on a NULL communicator
[irene1200:93866] *** Unknown error
[irene1200:93866] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1200:93866] ***    and potentially your MPI job)
[irene1200:93867] *** An error occurred in MPI_Init
[irene1200:93867] *** reported by process [1760459894,57]
[irene1200:93867] *** on a NULL communicator
[irene1200:93867] *** Unknown error
[irene1200:93867] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1200:93867] ***    and potentially your MPI job)
[irene1160:65623] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1160:65624] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1160:65624] *** An error occurred in MPI_Init
[irene1160:65624] *** reported by process [1760459894,37]
[irene1160:65624] *** on a NULL communicator
[irene1160:65624] *** Unknown error
[irene1160:65624] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1160:65624] ***    and potentially your MPI job)
[irene1160:65623] *** An error occurred in MPI_Init
[irene1160:65623] *** reported by process [1760459894,36]
[irene1160:65623] *** on a NULL communicator
[irene1160:65623] *** Unknown error
[irene1160:65623] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1160:65623] ***    and potentially your MPI job)
[irene1155:08420] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1155:08421] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1155:08421] *** An error occurred in MPI_Init
[irene1155:08421] *** reported by process [1760459894,27]
[irene1155:08421] *** on a NULL communicator
[irene1155:08421] *** Unknown error
[irene1155:08421] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1155:08421] ***    and potentially your MPI job)
[irene1155:08420] *** An error occurred in MPI_Init
[irene1155:08420] *** reported by process [1760459894,26]
[irene1155:08420] *** on a NULL communicator
[irene1155:08420] *** Unknown error
[irene1155:08420] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1155:08420] ***    and potentially your MPI job)
[irene1143:91068] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1143:91069] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1143:91068] *** An error occurred in MPI_Init
[irene1143:91068] *** reported by process [1760459894,12]
[irene1143:91068] *** on a NULL communicator
[irene1143:91068] *** Unknown error
[irene1143:91068] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1143:91068] ***    and potentially your MPI job)
[irene1143:91069] *** An error occurred in MPI_Init
[irene1143:91069] *** reported by process [1760459894,13]
[irene1143:91069] *** on a NULL communicator
[irene1143:91069] *** Unknown error
[irene1143:91069] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1143:91069] ***    and potentially your MPI job)
[irene1201:35770] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1201:35771] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1201:35770] *** An error occurred in MPI_Init
[irene1201:35770] *** reported by process [1760459894,58]
[irene1201:35770] *** on a NULL communicator
[irene1201:35770] *** Unknown error
[irene1201:35770] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1201:35770] ***    and potentially your MPI job)
[irene1201:35771] *** An error occurred in MPI_Init
[irene1201:35771] *** reported by process [1760459894,59]
[irene1201:35771] *** on a NULL communicator
[irene1201:35771] *** Unknown error
[irene1201:35771] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1201:35771] ***    and potentially your MPI job)
[irene1165:94985] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1165:94986] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1165:94985] *** An error occurred in MPI_Init
[irene1165:94985] *** reported by process [1760459894,44]
[irene1165:94985] *** on a NULL communicator
[irene1165:94985] *** Unknown error
[irene1165:94985] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1165:94985] ***    and potentially your MPI job)
[irene1165:94986] *** An error occurred in MPI_Init
[irene1165:94986] *** reported by process [1760459894,45]
[irene1165:94986] *** on a NULL communicator
[irene1165:94986] *** Unknown error
[irene1165:94986] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1165:94986] ***    and potentially your MPI job)
[irene1164:25284] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1164:25283] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1164:25284] *** An error occurred in MPI_Init
[irene1164:25284] *** reported by process [1760459894,43]
[irene1164:25284] *** on a NULL communicator
[irene1164:25284] *** Unknown error
[irene1164:25284] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1164:25284] ***    and potentially your MPI job)
[irene1164:25283] *** An error occurred in MPI_Init
[irene1164:25283] *** reported by process [1760459894,42]
[irene1164:25283] *** on a NULL communicator
[irene1164:25283] *** Unknown error
[irene1164:25283] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1164:25283] ***    and potentially your MPI job)
[irene1153:97318] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1153:97317] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1161:29223] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1161:29224] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1153:97317] *** An error occurred in MPI_Init
[irene1153:97317] *** reported by process [1760459894,22]
[irene1153:97317] *** on a NULL communicator
[irene1153:97317] *** Unknown error
[irene1153:97317] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1153:97317] ***    and potentially your MPI job)
[irene1153:97318] *** An error occurred in MPI_Init
[irene1153:97318] *** reported by process [1760459894,23]
[irene1153:97318] *** on a NULL communicator
[irene1153:97318] *** Unknown error
[irene1153:97318] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1153:97318] ***    and potentially your MPI job)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1161:29224] *** An error occurred in MPI_Init
[irene1161:29224] *** reported by process [1760459894,39]
[irene1161:29224] *** on a NULL communicator
[irene1161:29224] *** Unknown error
[irene1161:29224] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1161:29224] ***    and potentially your MPI job)
[irene1161:29223] *** An error occurred in MPI_Init
[irene1161:29223] *** reported by process [1760459894,38]
[irene1161:29223] *** on a NULL communicator
[irene1161:29223] *** Unknown error
[irene1161:29223] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1161:29223] ***    and potentially your MPI job)
[irene1157:29695] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1157:29694] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1157:29694] *** An error occurred in MPI_Init
[irene1157:29694] *** reported by process [1760459894,30]
[irene1157:29694] *** on a NULL communicator
[irene1157:29694] *** Unknown error
[irene1157:29694] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1157:29694] ***    and potentially your MPI job)
[irene1157:29695] *** An error occurred in MPI_Init
[irene1157:29695] *** reported by process [1760459894,31]
[irene1157:29695] *** on a NULL communicator
[irene1157:29695] *** Unknown error
[irene1157:29695] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1157:29695] ***    and potentially your MPI job)
[irene1221:05913] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1221:05912] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1221:05913] *** An error occurred in MPI_Init
[irene1221:05913] *** reported by process [1760459894,63]
[irene1221:05913] *** on a NULL communicator
[irene1221:05913] *** Unknown error
[irene1221:05913] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1221:05913] ***    and potentially your MPI job)
[irene1221:05912] *** An error occurred in MPI_Init
[irene1221:05912] *** reported by process [1760459894,62]
[irene1221:05912] *** on a NULL communicator
[irene1221:05912] *** Unknown error
[irene1221:05912] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1221:05912] ***    and potentially your MPI job)
[irene1220:79610] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1220:79611] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1220:79610] *** An error occurred in MPI_Init
[irene1220:79610] *** reported by process [1760459894,60]
[irene1220:79610] *** on a NULL communicator
[irene1220:79610] *** Unknown error
[irene1220:79610] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1220:79610] ***    and potentially your MPI job)
[irene1220:79611] *** An error occurred in MPI_Init
[irene1220:79611] *** reported by process [1760459894,61]
[irene1220:79611] *** on a NULL communicator
[irene1220:79611] *** Unknown error
[irene1220:79611] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1220:79611] ***    and potentially your MPI job)
[irene1152:02576] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1152:02577] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1152:02576] *** An error occurred in MPI_Init
[irene1152:02576] *** reported by process [1760459894,20]
[irene1152:02576] *** on a NULL communicator
[irene1152:02576] *** Unknown error
[irene1152:02576] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1152:02576] ***    and potentially your MPI job)
[irene1152:02577] *** An error occurred in MPI_Init
[irene1152:02577] *** reported by process [1760459894,21]
[irene1152:02577] *** on a NULL communicator
[irene1152:02577] *** Unknown error
[irene1152:02577] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1152:02577] ***    and potentially your MPI job)
[irene1136:83108] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1136:83109] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1136:83109] *** An error occurred in MPI_Init
[irene1136:83109] *** reported by process [1760459894,7]
[irene1136:83109] *** on a NULL communicator
[irene1136:83109] *** Unknown error
[irene1136:83109] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1136:83109] ***    and potentially your MPI job)
[irene1136:83108] *** An error occurred in MPI_Init
[irene1136:83108] *** reported by process [1760459894,6]
[irene1136:83108] *** on a NULL communicator
[irene1136:83108] *** Unknown error
[irene1136:83108] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1136:83108] ***    and potentially your MPI job)
[irene1159:10083] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1159:10082] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1137:73535] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1137:73536] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1159:10082] *** An error occurred in MPI_Init
[irene1159:10082] *** reported by process [1760459894,34]
[irene1159:10082] *** on a NULL communicator
[irene1159:10082] *** Unknown error
[irene1159:10082] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1159:10082] ***    and potentially your MPI job)
[irene1159:10083] *** An error occurred in MPI_Init
[irene1159:10083] *** reported by process [1760459894,35]
[irene1159:10083] *** on a NULL communicator
[irene1159:10083] *** Unknown error
[irene1159:10083] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1159:10083] ***    and potentially your MPI job)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1128:44565] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1128:44566] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1137:73535] *** An error occurred in MPI_Init
[irene1137:73535] *** reported by process [1760459894,8]
[irene1137:73535] *** on a NULL communicator
[irene1137:73535] *** Unknown error
[irene1137:73535] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1137:73535] ***    and potentially your MPI job)
[irene1137:73536] *** An error occurred in MPI_Init
[irene1137:73536] *** reported by process [1760459894,9]
[irene1137:73536] *** on a NULL communicator
[irene1137:73536] *** Unknown error
[irene1137:73536] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1137:73536] ***    and potentially your MPI job)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1128:44565] *** An error occurred in MPI_Init
[irene1128:44565] *** reported by process [1760459894,2]
[irene1128:44565] *** on a NULL communicator
[irene1128:44565] *** Unknown error
[irene1128:44565] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1128:44565] ***    and potentially your MPI job)
[irene1128:44566] *** An error occurred in MPI_Init
[irene1128:44566] *** reported by process [1760459894,3]
[irene1128:44566] *** on a NULL communicator
[irene1128:44566] *** Unknown error
[irene1128:44566] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1128:44566] ***    and potentially your MPI job)
[irene1140:60428] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1140:60429] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1140:60428] *** An error occurred in MPI_Init
[irene1140:60428] *** reported by process [1760459894,10]
[irene1140:60428] *** on a NULL communicator
[irene1140:60428] *** Unknown error
[irene1140:60428] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1140:60428] ***    and potentially your MPI job)
[irene1140:60429] *** An error occurred in MPI_Init
[irene1140:60429] *** reported by process [1760459894,11]
[irene1140:60429] *** on a NULL communicator
[irene1140:60429] *** Unknown error
[irene1140:60429] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1140:60429] ***    and potentially your MPI job)
[irene1151:78219] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1151:78220] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1151:78220] *** An error occurred in MPI_Init
[irene1151:78220] *** reported by process [1760459894,19]
[irene1151:78220] *** on a NULL communicator
[irene1151:78220] *** Unknown error
[irene1151:78220] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1151:78220] ***    and potentially your MPI job)
[irene1151:78219] *** An error occurred in MPI_Init
[irene1151:78219] *** reported by process [1760459894,18]
[irene1151:78219] *** on a NULL communicator
[irene1151:78219] *** Unknown error
[irene1151:78219] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1151:78219] ***    and potentially your MPI job)
[irene1197:62446] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1197:62447] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1197:62446] *** An error occurred in MPI_Init
[irene1197:62446] *** reported by process [1760459894,52]
[irene1197:62446] *** on a NULL communicator
[irene1197:62446] *** Unknown error
[irene1197:62446] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1197:62446] ***    and potentially your MPI job)
[irene1197:62447] *** An error occurred in MPI_Init
[irene1197:62447] *** reported by process [1760459894,53]
[irene1197:62447] *** on a NULL communicator
[irene1197:62447] *** Unknown error
[irene1197:62447] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1197:62447] ***    and potentially your MPI job)
[irene1162:62503] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1162:62502] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1162:62502] *** An error occurred in MPI_Init
[irene1162:62502] *** reported by process [1760459894,40]
[irene1162:62502] *** on a NULL communicator
[irene1162:62502] *** Unknown error
[irene1162:62502] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1162:62502] ***    and potentially your MPI job)
[irene1162:62503] *** An error occurred in MPI_Init
[irene1162:62503] *** reported by process [1760459894,41]
[irene1162:62503] *** on a NULL communicator
[irene1162:62503] *** Unknown error
[irene1162:62503] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1162:62503] ***    and potentially your MPI job)
[irene1148:81171] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1148:81170] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1148:81171] *** An error occurred in MPI_Init
[irene1148:81171] *** reported by process [1760459894,15]
[irene1148:81171] *** on a NULL communicator
[irene1148:81171] *** Unknown error
[irene1148:81171] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1148:81171] ***    and potentially your MPI job)
[irene1148:81170] *** An error occurred in MPI_Init
[irene1148:81170] *** reported by process [1760459894,14]
[irene1148:81170] *** on a NULL communicator
[irene1148:81170] *** Unknown error
[irene1148:81170] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1148:81170] ***    and potentially your MPI job)
[irene1166:22511] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1166:22512] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1166:22511] *** An error occurred in MPI_Init
[irene1166:22511] *** reported by process [1760459894,46]
[irene1166:22511] *** on a NULL communicator
[irene1166:22511] *** Unknown error
[irene1166:22511] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1166:22511] ***    and potentially your MPI job)
[irene1166:22512] *** An error occurred in MPI_Init
[irene1166:22512] *** reported by process [1760459894,47]
[irene1166:22512] *** on a NULL communicator
[irene1166:22512] *** Unknown error
[irene1166:22512] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1166:22512] ***    and potentially your MPI job)
[irene1177:80121] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1177:80120] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1177:80120] *** An error occurred in MPI_Init
[irene1177:80120] *** reported by process [1760459894,48]
[irene1177:80120] *** on a NULL communicator
[irene1177:80120] *** Unknown error
[irene1177:80120] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1177:80120] ***    and potentially your MPI job)
[irene1177:80121] *** An error occurred in MPI_Init
[irene1177:80121] *** reported by process [1760459894,49]
[irene1177:80121] *** on a NULL communicator
[irene1177:80121] *** Unknown error
[irene1177:80121] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1177:80121] ***    and potentially your MPI job)
[irene1123:32385] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1123:32386] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1123:32385] *** An error occurred in MPI_Init
[irene1123:32385] *** reported by process [1760459894,0]
[irene1123:32385] *** on a NULL communicator
[irene1123:32385] *** Unknown error
[irene1123:32385] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1123:32385] ***    and potentially your MPI job)
[irene1123:32386] *** An error occurred in MPI_Init
[irene1123:32386] *** reported by process [1760459894,1]
[irene1123:32386] *** on a NULL communicator
[irene1123:32386] *** Unknown error
[irene1123:32386] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1123:32386] ***    and potentially your MPI job)
[irene1194:91429] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
[irene1194:91430] pml_ucx.c:175  Error: Failed to receive UCX worker address: Not found (-13)
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
[irene1194:91429] *** An error occurred in MPI_Init
[irene1194:91429] *** reported by process [1760459894,50]
[irene1194:91429] *** on a NULL communicator
[irene1194:91429] *** Unknown error
[irene1194:91429] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1194:91429] ***    and potentially your MPI job)
[irene1194:91430] *** An error occurred in MPI_Init
[irene1194:91430] *** reported by process [1760459894,51]
[irene1194:91430] *** on a NULL communicator
[irene1194:91430] *** Unknown error
[irene1194:91430] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[irene1194:91430] ***    and potentially your MPI job)
slurmstepd-irene1123: error: *** STEP 7588972.3 ON irene1123 CANCELLED AT 2022-12-13T22:53:08 ***
srun: Job step aborted: Waiting up to 302 seconds for job step to finish.
srun: error: irene1158: task 32: Exited with exit code 1
srun: error: irene1150: task 16: Killed
srun: error: irene1194: tasks 50-51: Killed
srun: error: irene1162: tasks 40-41: Killed
srun: error: irene1155: tasks 26-27: Killed
srun: error: irene1128: tasks 2-3: Killed
srun: error: irene1164: tasks 42-43: Killed
srun: error: irene1135: tasks 4-5: Killed
srun: error: irene1165: tasks 44-45: Killed
srun: error: irene1148: tasks 14-15: Killed
srun: error: irene1154: tasks 24-25: Killed
srun: error: irene1197: tasks 52-53: Killed
srun: error: irene1161: tasks 38-39: Killed
srun: error: irene1199: tasks 54-55: Killed
srun: error: irene1123: tasks 0-1: Killed
srun: error: irene1200: tasks 56-57: Killed
srun: error: irene1140: tasks 10-11: Killed
srun: error: irene1152: tasks 20-21: Killed
srun: error: irene1137: tasks 8-9: Killed
srun: error: irene1201: tasks 58-59: Killed
srun: error: irene1159: tasks 34-35: Killed
srun: error: irene1220: tasks 60-61: Killed
srun: error: irene1151: tasks 18-19: Killed
srun: error: irene1166: tasks 46-47: Killed
srun: error: irene1143: tasks 12-13: Killed
srun: error: irene1177: tasks 48-49: Killed
srun: error: irene1221: tasks 62-63: Killed
srun: error: irene1157: tasks 30-31: Killed
srun: error: irene1136: tasks 6-7: Killed
srun: error: irene1153: tasks 22-23: Killed
srun: error: irene1160: tasks 36-37: Killed
srun: error: irene1156: tasks 28-29: Killed
srun: error: irene1158: task 33: Killed
srun: error: irene1150: task 17: Exited with exit code 1
