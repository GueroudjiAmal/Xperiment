distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.99:46163'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.85:35634'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.92:37092'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.95:37920'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.85:35307'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.94:35756'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.94:40000'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.98:43136'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.89:40230'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.88:33847'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.89:43758'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.99:41297'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.95:43268'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.92:43323'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.88:39391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.98:35996'
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.85:35652
distributed.worker - INFO -          Listening to:    tcp://172.21.5.85:35652
distributed.worker - INFO -          dashboard at:          172.21.5.85:36635
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5w836lwk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.85:34454
distributed.worker - INFO -          Listening to:    tcp://172.21.5.85:34454
distributed.worker - INFO -          dashboard at:          172.21.5.85:35371
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g24m1lss
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.89:37983
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.89:46689
distributed.worker - INFO -          Listening to:    tcp://172.21.5.89:37983
distributed.worker - INFO -          Listening to:    tcp://172.21.5.89:46689
distributed.worker - INFO -          dashboard at:          172.21.5.89:32942
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO -          dashboard at:          172.21.5.89:41298
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6nzyd539
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-et7no373
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.88:35744
distributed.worker - INFO -          Listening to:    tcp://172.21.5.88:35744
distributed.worker - INFO -          dashboard at:          172.21.5.88:42545
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.95:35204
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ppkeua5u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.88:33568
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.95:33693
distributed.worker - INFO -          Listening to:    tcp://172.21.5.95:33693
distributed.worker - INFO -          dashboard at:          172.21.5.95:37661
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.5.95:35204
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-g_n4qaye
distributed.worker - INFO -          dashboard at:          172.21.5.95:40290
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:    tcp://172.21.5.88:33568
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2z464pdz
distributed.worker - INFO -          dashboard at:          172.21.5.88:41391
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2qw4lxrr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.99:39788
distributed.worker - INFO -          Listening to:    tcp://172.21.5.99:39788
distributed.worker - INFO -          dashboard at:          172.21.5.99:39053
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_rane68p
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.98:33195
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.99:38719
distributed.worker - INFO -          Listening to:    tcp://172.21.5.99:38719
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.92:45650
distributed.worker - INFO -          Listening to:    tcp://172.21.5.92:45650
distributed.worker - INFO -          dashboard at:          172.21.5.99:40613
distributed.worker - INFO -          dashboard at:          172.21.5.92:46039
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.94:33492
distributed.worker - INFO -          Listening to:    tcp://172.21.5.94:33492
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.92:32769
distributed.worker - INFO -          dashboard at:          172.21.5.94:44188
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.5.92:32769
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:          172.21.5.92:35135
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2_egg1ly
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q446wmvf
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.98:41560
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.5.98:33195
distributed.worker - INFO -          dashboard at:          172.21.5.98:43383
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.94:44805
distributed.worker - INFO -          Listening to:    tcp://172.21.5.94:44805
distributed.worker - INFO -          dashboard at:          172.21.5.94:34637
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6fqht06v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-her76qr4
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w9_k3l19
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ojl_32kv
distributed.worker - INFO -          Listening to:    tcp://172.21.5.98:41560
distributed.worker - INFO -          dashboard at:          172.21.5.98:39279
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j5zz0rtz
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.5.46:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Event loop was unresponsive in Nanny for 39.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 38.53s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 39.76s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 38.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 36.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Nanny for 36.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 1531.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 1531.35s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 1531.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 1531.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 18.68s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 18.29s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 18.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-irene1730: error: *** STEP 7589007.3 ON irene1730 CANCELLED AT 2022-12-13T18:07:12 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.89:43758'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.89:40230'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.92:37092'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.85:35307'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.92:43323'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.85:35634'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.94:35756'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.88:33847'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.88:39391'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.98:43136'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.95:37920'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.98:35996'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.94:40000'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.95:43268'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.99:41297'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.99:46163'
distributed.worker - INFO - Stopping worker at tcp://172.21.5.88:35744
distributed.worker - INFO - Stopping worker at tcp://172.21.5.98:41560
distributed.worker - INFO - Stopping worker at tcp://172.21.5.85:35652
distributed.worker - INFO - Stopping worker at tcp://172.21.5.95:35204
distributed.worker - INFO - Stopping worker at tcp://172.21.5.98:33195
distributed.worker - INFO - Stopping worker at tcp://172.21.5.85:34454
distributed.worker - INFO - Stopping worker at tcp://172.21.5.88:33568
distributed.worker - INFO - Stopping worker at tcp://172.21.5.95:33693
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30827 parent=27056 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18177 parent=14504 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10436 parent=6788 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49055 parent=45407 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10435 parent=6787 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81974 parent=78327 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44287 parent=40632 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18178 parent=14505 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82377 parent=78700 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30828 parent=27057 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17357 parent=13674 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=81975 parent=78326 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=17356 parent=13675 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=82378 parent=78699 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=49056 parent=45406 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44288 parent=40631 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6ac4c0)

Current thread 0x00002b76fbc6bb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19684c0)

Current thread 0x00002b097acaeb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19574c0)

Current thread 0x00002b2abb338b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f3b4c0)

Current thread 0x00002ad1189f2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10cc4c0)

Current thread 0x00002acce52e2b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19374c0)

Current thread 0x00002b98ddeabb80 (most recent call first):
<no Python frame>
srun: error: irene1733: task 2: Aborted (core dumped)
srun: error: irene1734: task 5: Aborted (core dumped)
srun: error: irene1744: task 15: Aborted (core dumped)
srun: error: irene1737: task 6: Aborted (core dumped)
srun: error: irene1743: task 12: Aborted (core dumped)
srun: error: irene1739: task 8: Aborted (core dumped)
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6c74c0)

Current thread 0x00002b59ba5c6b80 (most recent call first):
<no Python frame>
srun: error: irene1739: task 9: Aborted (core dumped)
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x22b04c0)

Current thread 0x00002ac3d1abcb80 (most recent call first):
<no Python frame>
srun: error: irene1744: task 14: Aborted (core dumped)
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23a44c0)

Current thread 0x00002b7773f18b80 (most recent call first):
<no Python frame>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
srun: error: irene1740: task 11: Aborted (core dumped)
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1de84c0)

Current thread 0x00002b3491ff3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x19e54c0)

Current thread 0x00002baa53420b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1c744c0)

Current thread 0x00002b6026dc9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x10164c0)

Current thread 0x00002aeb36d5bb80 (most recent call first):
<no Python frame>
srun: error: irene1730: task 0: Aborted (core dumped)
srun: error: irene1743: task 13: Aborted (core dumped)
srun: error: irene1740: task 10: Aborted (core dumped)
srun: error: irene1733: task 3: Aborted (core dumped)
