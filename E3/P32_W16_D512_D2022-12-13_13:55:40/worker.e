distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.21:41729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.7:37319'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.7:37778'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.249:37160'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.212:43075'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.212:44293'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.21:41943'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.249:36238'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.248:38385'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.4.248:37856'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.20:33594'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.6:37305'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.20:34521'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.5.6:41604'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.175:36424'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.1.175:43981'
distributed.worker - INFO -       Start worker at:     tcp://172.21.5.7:41273
distributed.worker - INFO -          Listening to:     tcp://172.21.5.7:41273
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.249:42815
distributed.worker - INFO -          Listening to:   tcp://172.21.4.249:42815
distributed.worker - INFO -          dashboard at:           172.21.5.7:35323
distributed.worker - INFO -          dashboard at:         172.21.4.249:41376
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.212:38981
distributed.worker - INFO -          Listening to:   tcp://172.21.4.212:38981
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:     tcp://172.21.5.7:36887
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wvji0gn7
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ji5a_g8i
distributed.worker - INFO -          Listening to:     tcp://172.21.5.7:36887
distributed.worker - INFO -          dashboard at:           172.21.5.7:46580
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.4.212:42323
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-td0yiv2p
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.212:40953
distributed.worker - INFO -          Listening to:   tcp://172.21.4.212:40953
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f6bv6ees
distributed.worker - INFO -          dashboard at:         172.21.4.212:33774
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h9pheq20
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.249:35155
distributed.worker - INFO -          Listening to:   tcp://172.21.4.249:35155
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.4.249:37358
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tbou768l
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.248:41923
distributed.worker - INFO -          Listening to:   tcp://172.21.4.248:41923
distributed.worker - INFO -       Start worker at:   tcp://172.21.4.248:32928
distributed.worker - INFO -          dashboard at:         172.21.4.248:43728
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:   tcp://172.21.4.248:32928
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2apkzub3
distributed.worker - INFO -          dashboard at:         172.21.4.248:41703
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-788zwcjo
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.21:43165
distributed.worker - INFO -          Listening to:    tcp://172.21.5.21:43165
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.21:38098
distributed.worker - INFO -          Listening to:    tcp://172.21.5.21:38098
distributed.worker - INFO -          dashboard at:          172.21.5.21:33833
distributed.worker - INFO -          dashboard at:          172.21.5.21:44887
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ws3gmbvl
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c1jh840x
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.20:40627
distributed.worker - INFO -          Listening to:    tcp://172.21.5.20:40627
distributed.worker - INFO -       Start worker at:    tcp://172.21.5.20:37533
distributed.worker - INFO -          Listening to:    tcp://172.21.5.20:37533
distributed.worker - INFO -          dashboard at:          172.21.5.20:46203
distributed.worker - INFO -          dashboard at:          172.21.5.20:39634
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d7umx7a_
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oix1in01
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:     tcp://172.21.5.6:43560
distributed.worker - INFO -          Listening to:     tcp://172.21.5.6:43560
distributed.worker - INFO -          dashboard at:           172.21.5.6:40129
distributed.worker - INFO -       Start worker at:     tcp://172.21.5.6:42853
distributed.worker - INFO -          Listening to:     tcp://172.21.5.6:42853
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:           172.21.5.6:46449
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yw3xozz3
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c46ukqvq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.175:45590
distributed.worker - INFO -          Listening to:   tcp://172.21.1.175:45590
distributed.worker - INFO -          dashboard at:         172.21.1.175:39235
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0ug0klqb
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.1.175:39898
distributed.worker - INFO -          Listening to:   tcp://172.21.1.175:39898
distributed.worker - INFO -          dashboard at:         172.21.1.175:33631
distributed.worker - INFO - Waiting to connect to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3vhdw7c6
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:     tcp://172.21.1.90:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene1268: error: *** STEP 7589009.1 ON irene1268 CANCELLED AT 2022-12-13T18:12:44 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.212:44293'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.212:43075'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.175:36424'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.1.175:43981'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.6:37305'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.6:41604'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.7:37319'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.7:37778'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.21:41729'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.21:41943'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.249:36238'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.249:37160'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.20:33594'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.5.20:34521'
distributed.worker - INFO - Stopping worker at tcp://172.21.4.212:40953
distributed.worker - INFO - Stopping worker at tcp://172.21.4.212:38981
distributed.worker - INFO - Stopping worker at tcp://172.21.1.175:45590
distributed.worker - INFO - Stopping worker at tcp://172.21.1.175:39898
distributed.worker - INFO - Stopping worker at tcp://172.21.5.6:42853
distributed.worker - INFO - Stopping worker at tcp://172.21.5.7:36887
distributed.worker - INFO - Stopping worker at tcp://172.21.5.6:43560
distributed.worker - INFO - Stopping worker at tcp://172.21.5.21:38098
distributed.worker - INFO - Stopping worker at tcp://172.21.5.7:41273
distributed.worker - INFO - Stopping worker at tcp://172.21.5.21:43165
distributed.worker - INFO - Stopping worker at tcp://172.21.5.20:37533
distributed.worker - INFO - Stopping worker at tcp://172.21.4.249:42815
distributed.worker - INFO - Stopping worker at tcp://172.21.5.20:40627
distributed.worker - INFO - Stopping worker at tcp://172.21.4.249:35155
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.248:38385'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.4.248:37856'
distributed.worker - INFO - Stopping worker at tcp://172.21.4.248:41923
distributed.worker - INFO - Stopping worker at tcp://172.21.4.248:32928
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39285 parent=33955 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41705 parent=36342 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91062 parent=85797 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62552 parent=57345 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28110 parent=21590 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=39286 parent=33954 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=62551 parent=57344 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91061 parent=85798 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=28109 parent=21591 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38546 parent=33142 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4314 parent=96607 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=38547 parent=33141 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=4313 parent=96606 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41706 parent=36341 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91690 parent=86505 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=91689 parent=86506 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f5d4c0)

Current thread 0x00002ab4bf5d5b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18894c0)

Current thread 0x00002b2161697b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x21e84c0)

Current thread 0x00002b48ff6cdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f554c0)

Current thread 0x00002b87fca63b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1adb4c0)

Current thread 0x00002acb77d11b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5224c0)

Current thread 0x00002add0dac9b80 (most recent call first):
<no Python frame>
srun: error: irene1641: task 7: Aborted (core dumped)
srun: error: irene1640: task 5: Aborted (core dumped)
srun: error: irene1652: task 11: Aborted (core dumped)
srun: error: irene1665: task 13: Aborted (core dumped)
srun: error: irene1268: task 1: Aborted (core dumped)
srun: error: irene1666: task 15: Aborted (core dumped)
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5df4c0)

Current thread 0x00002ad00320cb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe974c0)

Current thread 0x00002b2345186b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xda74c0)

Current thread 0x00002b05e7193b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xe754c0)

Current thread 0x00002b3d37479b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x21cf4c0)

Current thread 0x00002b60b4cceb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18f74c0)

Current thread 0x00002b71123a9b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf704c0)

Current thread 0x00002b5f89c60b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x7604c0)

Current thread 0x00002ad6cae8eb80 (most recent call first):
<no Python frame>
srun: error: irene1665: task 12: Aborted (core dumped)
srun: error: irene1604: tasks 2-3: Aborted (core dumped)
srun: error: irene1652: task 10: Aborted (core dumped)
srun: error: irene1641: task 6: Aborted (core dumped)
srun: error: irene1268: task 0: Aborted (core dumped)
srun: error: irene1666: task 14: Aborted (core dumped)
srun: error: irene1651: task 8: Aborted (core dumped)
