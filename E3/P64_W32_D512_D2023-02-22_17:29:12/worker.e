distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.22:34572'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.22:39078'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.238:38617'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.246:37728'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.246:42021'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:33640'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.20:32926'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.182:33185'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.247:34195'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.182:34244'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.177:44973'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.177:40719'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.244:45882'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.17:36775'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.240:41284'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.9.17:40674'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.240:37836'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.176:33729'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.238:44937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.247:43726'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.244:40839'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.176:44102'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.75:35824'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.180:34653'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.180:46573'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:33446'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.242:33665'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.245:37669'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.11.75:35748'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.245:44579'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.239:44937'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.21.8.239:35334'
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.244:44995
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.247:37984
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.244:43411
distributed.worker - INFO -          Listening to:   tcp://172.21.8.247:37984
distributed.worker - INFO -          Listening to:   tcp://172.21.8.244:44995
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.247:37114
distributed.worker - INFO -          dashboard at:         172.21.8.244:42616
distributed.worker - INFO -          dashboard at:         172.21.8.247:43674
distributed.worker - INFO -          Listening to:   tcp://172.21.8.244:43411
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:         172.21.8.244:36553
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.8.247:37114
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:         172.21.8.247:35802
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4n8bioep
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rkwt4coj
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0btf6oyp
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e8r_oka5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.22:36570
distributed.worker - INFO -          Listening to:    tcp://172.21.9.22:36570
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.240:40149
distributed.worker - INFO -          dashboard at:          172.21.9.22:33643
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.240:36496
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.8.240:40149
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.240:36496
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.240:33074
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.22:40165
distributed.worker - INFO -          dashboard at:         172.21.8.240:32944
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_lqc9aog
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:    tcp://172.21.9.22:40165
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:          172.21.9.22:40303
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u8r9hnrz
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_jizzij7
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bxlizo7y
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:37895
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:37895
distributed.worker - INFO -          dashboard at:          172.21.9.20:33709
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.20:44333
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.20:44333
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:          172.21.9.20:41659
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vxtbleaw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jb9efk8e
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.17:33931
distributed.worker - INFO -       Start worker at:    tcp://172.21.9.17:45204
distributed.worker - INFO -          Listening to:    tcp://172.21.9.17:45204
distributed.worker - INFO -          dashboard at:          172.21.9.17:34495
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:    tcp://172.21.9.17:33931
distributed.worker - INFO -          dashboard at:          172.21.9.17:35497
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7ednpgsr
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rzk5bo4z
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:42902
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:42902
distributed.worker - INFO -          dashboard at:         172.21.8.242:40223
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.245:41849
distributed.worker - INFO -          Listening to:   tcp://172.21.8.245:41849
distributed.worker - INFO -          dashboard at:         172.21.8.245:33872
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.245:37460
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.242:46331
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          Listening to:   tcp://172.21.8.245:37460
distributed.worker - INFO -          Listening to:   tcp://172.21.8.242:46331
distributed.worker - INFO -          dashboard at:         172.21.8.245:43503
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tf3sqs7_
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.242:43270
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ncvhq9wf
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r_nq7nf4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8vb7bmmv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.246:45016
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.246:46074
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.182:37029
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.176:39865
distributed.worker - INFO -          Listening to:  tcp://172.21.11.176:39865
distributed.worker - INFO -          Listening to:  tcp://172.21.11.182:37029
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.182:36089
distributed.worker - INFO -          dashboard at:        172.21.11.182:42212
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.177:36740
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.176:39580
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Start worker at:   tcp://172.21.11.75:35795
distributed.worker - INFO -          dashboard at:        172.21.11.176:33420
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:  tcp://172.21.11.182:36089
distributed.worker - INFO -          Listening to:   tcp://172.21.8.246:45016
distributed.worker - INFO -          Listening to:  tcp://172.21.11.176:39580
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:         172.21.8.246:40350
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6umk2v9t
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:  tcp://172.21.11.177:36740
distributed.worker - INFO -          dashboard at:        172.21.11.176:46057
distributed.worker - INFO -          dashboard at:        172.21.11.182:46712
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.177:37389
distributed.worker - INFO -          Listening to:   tcp://172.21.11.75:35795
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.8.246:46074
distributed.worker - INFO -          dashboard at:        172.21.11.177:39687
distributed.worker - INFO -          dashboard at:         172.21.11.75:39128
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.11.177:37389
distributed.worker - INFO -       Start worker at:   tcp://172.21.11.75:36387
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.21.8.246:43310
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          Listening to:   tcp://172.21.11.75:36387
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xrhf7ktf
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -          dashboard at:        172.21.11.177:42780
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cdyls92d
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:         172.21.11.75:36512
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iml7b_xp
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oqtyvysk
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t6ds0l43
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ib_4y1k_
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0c4hbcmx
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xqz7dip1
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6y9b4i2v
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.238:36566
distributed.worker - INFO -          Listening to:   tcp://172.21.8.238:36566
distributed.worker - INFO -          dashboard at:         172.21.8.238:44959
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.238:35765
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.21.8.238:35765
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          dashboard at:         172.21.8.238:32984
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z03isy90
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jrs8dmyr
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.180:33419
distributed.worker - INFO -          Listening to:  tcp://172.21.11.180:33419
distributed.worker - INFO -          dashboard at:        172.21.11.180:37180
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:  tcp://172.21.11.180:46218
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -          Listening to:  tcp://172.21.11.180:46218
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-42d6eh5z
distributed.worker - INFO -          dashboard at:        172.21.11.180:40321
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2w48r282
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.239:38825
distributed.worker - INFO -       Start worker at:   tcp://172.21.8.239:41228
distributed.worker - INFO -          Listening to:   tcp://172.21.8.239:41228
distributed.worker - INFO -          Listening to:   tcp://172.21.8.239:38825
distributed.worker - INFO -          dashboard at:         172.21.8.239:38136
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -          dashboard at:         172.21.8.239:43486
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:    tcp://172.21.8.235:8786
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                         24
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -                Memory:                 172.73 GiB
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-366t8795
distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v5hshqm2
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:    tcp://172.21.8.235:8786
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
slurmstepd-irene2182: error: *** STEP 8488978.1 ON irene2182 CANCELLED AT 2023-02-24T06:44:11 ***
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.238:38617'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.247:34195'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.176:44102'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.238:44937'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.247:43726'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.176:33729'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.75:35748'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.75:35824'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.244:45882'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.244:40839'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.239:35334'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.239:44937'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.182:33185'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.182:34244'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.177:44973'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.177:40719'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.240:41284'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.240:37836'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:33640'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.20:32926'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.246:37728'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.246:42021'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.22:39078'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.22:34572'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.180:34653'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.11.180:46573'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.17:40674'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.245:44579'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.245:37669'
distributed.dask_worker - INFO - Exiting on signal 15
distributed.dask_worker - INFO - Exiting on signal 15
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:33446'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.8.242:33665'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.21.9.17:36775'
distributed.worker - INFO - Stopping worker at tcp://172.21.8.245:41849
distributed.worker - INFO - Stopping worker at tcp://172.21.8.245:37460
distributed.worker - INFO - Stopping worker at tcp://172.21.11.176:39580
distributed.worker - INFO - Stopping worker at tcp://172.21.11.176:39865
distributed.worker - INFO - Stopping worker at tcp://172.21.11.177:36740
distributed.worker - INFO - Stopping worker at tcp://172.21.11.177:37389
distributed.worker - INFO - Stopping worker at tcp://172.21.11.180:33419
distributed.worker - INFO - Stopping worker at tcp://172.21.11.180:46218
distributed.worker - INFO - Stopping worker at tcp://172.21.11.182:36089
distributed.worker - INFO - Stopping worker at tcp://172.21.11.182:37029
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:44333
distributed.worker - INFO - Stopping worker at tcp://172.21.9.20:37895
distributed.worker - INFO - Stopping worker at tcp://172.21.8.238:36566
distributed.worker - INFO - Stopping worker at tcp://172.21.8.238:35765
distributed.worker - INFO - Stopping worker at tcp://172.21.8.247:37984
distributed.worker - INFO - Stopping worker at tcp://172.21.8.247:37114
distributed.worker - INFO - Stopping worker at tcp://172.21.11.75:35795
distributed.worker - INFO - Stopping worker at tcp://172.21.11.75:36387
distributed.worker - INFO - Stopping worker at tcp://172.21.8.239:38825
distributed.worker - INFO - Stopping worker at tcp://172.21.8.239:41228
distributed.worker - INFO - Stopping worker at tcp://172.21.8.244:44995
distributed.worker - INFO - Stopping worker at tcp://172.21.8.244:43411
distributed.worker - INFO - Stopping worker at tcp://172.21.8.240:40149
distributed.worker - INFO - Stopping worker at tcp://172.21.8.240:36496
distributed.worker - INFO - Stopping worker at tcp://172.21.8.246:45016
distributed.worker - INFO - Stopping worker at tcp://172.21.8.246:46074
distributed.worker - INFO - Stopping worker at tcp://172.21.9.22:40165
distributed.worker - INFO - Stopping worker at tcp://172.21.9.22:36570
distributed.worker - INFO - Stopping worker at tcp://172.21.9.17:33931
distributed.worker - INFO - Stopping worker at tcp://172.21.9.17:45204
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:46331
distributed.worker - INFO - Stopping worker at tcp://172.21.8.242:42902
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83328 parent=83222 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18867 parent=18764 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=18868 parent=18763 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19907 parent=19803 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=19908 parent=19802 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=83329 parent=83223 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94096 parent=93989 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=94095 parent=93990 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57091 parent=56984 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58368 parent=58262 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74360 parent=74255 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=57090 parent=56983 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=74359 parent=74256 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10692 parent=10584 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30879 parent=30775 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=30880 parent=30776 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=10691 parent=10585 started daemon>
distributed.dask_worker - INFO - End worker
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=58367 parent=58263 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54542 parent=54439 started daemon>
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54543 parent=54438 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68618 parent=68516 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43658 parent=43553 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77138 parent=77034 started daemon>
distributed.dask_worker - INFO - End worker
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=68619 parent=68515 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=77139 parent=77035 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43657 parent=43552 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75034 parent=74927 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=75033 parent=74928 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21293 parent=21189 started daemon>
distributed.dask_worker - INFO - End worker
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85945 parent=85841 started daemon>
distributed.nanny - WARNING - Worker process still alive after 1 seconds, killing
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=85944 parent=85840 started daemon>
distributed.dask_worker - INFO - End worker
distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=21292 parent=21188 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x12274c0)

Current thread 0x00002b637d3a4b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x23b54c0)

Current thread 0x00002b9ceb9bdb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x50c4c0)

Current thread 0x00002ba598c77b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf5a4c0)

Current thread 0x00002b286f68fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x237f4c0)

Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f074c0)

Current thread 0x00002b7b85dbcb80 (most recent call first):
<no Python frame>
Current thread 0x00002b785ce63b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x20774c0)

Current thread 0x00002b641879db80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xdbb4c0)

Current thread 0x00002aef58aafb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xac54c0)

Current thread 0x00002afc4e0feb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5fd4c0)

Current thread 0x00002b42d729ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6524c0)

Current thread 0x00002b96a59b7b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x14ba4c0)

Current thread 0x00002adc3906eb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x52c4c0)

Current thread 0x00002af17cd67b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xf1a4c0)

Current thread 0x00002b5a04d8ab80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5774c0)

Current thread 0x00002b764c994b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x1f524c0)

Current thread 0x00002acffe47fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x11e74c0)

Current thread 0x00002b7f8ef43b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18d64c0)

Current thread 0x00002b089f314b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x9d64c0)

Current thread 0x00002aaf3a951b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x5424c0)

Current thread 0x00002ae0bf943b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xcf24c0)

Current thread 0x00002b5c485ccb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x123b4c0)

Current thread 0x00002b021ac7fb80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x18644c0)

Current thread 0x00002adfd4bd3b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x6324c0)

Current thread 0x00002b8cda623b80 (most recent call first):
<no Python frame>
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0xfb24c0)

Current thread 0x00002b8c2bcf6b80 (most recent call first):
<no Python frame>
srun: error: irene2186: tasks 6-7: Aborted (core dumped)
srun: error: irene2655: tasks 30-31: Aborted (core dumped)
srun: error: irene2191: tasks 14-15: Aborted (core dumped)
srun: error: irene2219: tasks 20-21: Aborted (core dumped)
srun: error: irene2184: tasks 4-5: Aborted (core dumped)
srun: error: irene2649: tasks 24-25: Aborted (core dumped)
srun: error: irene2183: tasks 2-3: Aborted (core dumped)
srun: error: irene2217: tasks 18-19: Aborted (core dumped)
srun: error: irene2653: tasks 28-29: Aborted (core dumped)
srun: error: irene2190: task 13: Aborted (core dumped)
srun: error: irene2214: task 16: Aborted (core dumped)
srun: error: irene2188: task 9: Aborted (core dumped)
srun: error: irene2548: task 22: Aborted (core dumped)
srun: error: irene2182: task 0: Aborted (core dumped)
srun: error: irene2189: task 11: Aborted (core dumped)
srun: error: irene2650: task 27: Aborted (core dumped)
